%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}


\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage[english]{babel}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Sonny]{fncychap}
\usepackage[,numfigreset=2,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}


\usepackage{ctex}
\setmainfont{Source Serif Pro}
\setsansfont{Source Sans Pro}
\setmonofont{Source Code Pro}
\setCJKmainfont[BoldFont=Source Han Serif SC SemiBold]{Source Han Serif SC}
\setCJKsansfont[BoldFont=Source Han Sans SC Medium]{Source Han Sans SC Normal}
\setCJKmonofont{Source Han Sans SC Normal}
\addto\captionsenglish{\renewcommand{\chaptername}{}}
\addto\captionsenglish{\renewcommand{\contentsname}{目录}}
\usepackage[draft]{minted}
\fvset{breaklines=true, breakanywhere=true}
\setlength{\headheight}{13.6pt}
\makeatletter
\fancypagestyle{normal}{
\fancyhf{}
\fancyfoot[LE,RO]{{\py@HeaderFamily\thepage}}
\fancyfoot[LO]{{\py@HeaderFamily\nouppercase{\rightmark}}}
\fancyfoot[RE]{{\py@HeaderFamily\nouppercase{\leftmark}}}
\fancyhead[LE,RO]{{\py@HeaderFamily }}
}
\makeatother
\CJKsetecglue{}
\usepackage{zhnumber}


\title{机器学习系统：设计和实现}
\date{Aug 09, 2022}
\release{1.0.0}
\author{Luo Mai, Hao Dong}
\newcommand{\sphinxlogo}{\sphinxincludegraphics{logo.png}\par}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{前言}
\label{\detokenize{chapter_preface/index:id1}}\label{\detokenize{chapter_preface/index::doc}}

\section{缘起}
\label{\detokenize{chapter_preface/index:id2}}
\sphinxAtStartPar
我在2020年来到了爱丁堡大学信息学院，爱丁堡大学是AI研究的发源地之一，很多学生慕名而来学习机器学习技术。因此，我们拥有许多出色的AI课程（自然语言处理，计算机视觉，计算神经学等），同时也拥有一系列关于计算机系统的基础课程（操作系统，编程语言，编译器，计算机体系架构等）。但是当我在教学的过程中问起学生：机器学习是如何利用计算机系统来做到计算加速和大规模部署的？许多学生都会报来疑惑的眼神。而这也促使我思考在爱丁堡大学乃至于其他世界顶尖大学的教学大纲里，我们是不是缺乏了一门课程来衔接机器学习技术和计算机系统知识。

\sphinxAtStartPar
我第一反应是寻找一门已有的课程来借鉴。当其时，加州伯克利大学的AI
Systems较为知名。这门课描述了机器学习系统的不同研究方向，内容以研读论文为主。可惜的是，许多论文已经无法经受住时间的检验。更重要的是：这门课缺乏对于知识的整体梳理，形成完整的知识体系架构。学习完这个课程，学生并没有明确的思路可以从头搭建起来一个机器学习框架。而将目光投向其他地方，华盛顿大学曾短期开过Deep
Learning
Systems课程，这门课程讲述了机器学习程序的编译过程，其受限于服务TVM的目的，对于机器学习系统缺乏完整的解读。另外，斯坦福大学的Machine
Learning Systems
Design因为课程设计人是数据库背景，因此课程专注数据清洗，数据管理，数据标注等数据专题。

\sphinxAtStartPar
当时觉得比较合适的是微软亚洲研究院的AI
Systems。这门课程在研读论文的同时，一定程度上讲述了AI框架背后的设计理念。但是当我准备将其教授给本科生的时候，我发现这门课对于机器学习系统核心设计理念讲解很浅，同时也要求学生具有大量的背景知识，实际上更适合给博士生授课。抛开内容不谈，上述的全部课程共同的核心问题是：它们给学生的阅读材料都是高深，零散甚至过时的论文，而不是一本全面，注重基础，语言通熟易懂的面向本科生和工程师的教科书，这给机器学习系统相关知识的传播造成了极大的困难。

\sphinxAtStartPar
回首2020年的世界，我们已经拥有了优秀的操作系统，数据库，分布式系统等基础性教材。在机器学习领域，我们也拥有了一系列机器学习算法的教材。然而，无论是英语世界还是中文世界，我竟找不到任何一本系统性讲述机器学习系统的教材。而这本教材的缺乏，让许多公司和高校实验室不得不花费大量的人力和物力从头培养学生和工程师对于机器学习基础架构的认识，这已经制约了高校培养出符合业界，学界和时代发展的人才了！因此，我开始思考：我们学界和业界是不是需要一本机器学习系统的教科书了呢？


\section{开端}
\label{\detokenize{chapter_preface/index:id3}}
\sphinxAtStartPar
带着写书的构想，我开始和身边的朋友沟通。几乎全部人都非常认可这本书的巨大价值，但是现实的情况是：没有人愿意做这么一件吃力不讨好的事情。我当时的博士后导师也劝我：我现在处在助理教授的关键阶段，追求高影响力的学术论文是当务之急，写一本书要耗费3\sphinxhyphen{}4年的精力，最后可能也无法面世。而当我和同行交流时也发现：人们更愿意改进世面上已经有的教科书，做有迹可循的事情，而不是摸着石头过河，做从无到有的事情。特别是对于机器学习系统这个快速发展，依然在试错的领域，能不能写出一本能够经受住时间检验的书也是一个巨大的未知数。

\sphinxAtStartPar
我因此不得不将这个想法暂时藏在了心里了数月，直到一次探亲回国和朋友聊天。这个朋友就是MindSpore的架构师金雪锋。和雪锋的相识是在疫情前的最后一个圣诞节左右，雪锋来伦敦访问，他正在领导MindSpore的开发（当时1.0还没有发布）。而在世界的另一端，我在2018年也和好友一起试图从头搭建一个AI框架，虽然最终资源不足，无疾而终，不过许多的思考成就了我之后发表的多篇AI系统论文。和雪锋聊起来，我们都对AI框架开发之难深有同感。我们共同的感慨就是：找到懂AI框架开发的人太难了。现今的学生们都一心学习机器学习算法，很多学生对于底层的运作原理理解很粗浅。而当他们在真实世界中应用机器学习意识到系统的重要性，想去学习的时候，却没有了在学校中最充沛的学习时间。我因此对雪锋苦笑到：我是准备写一本机器学习系统教材的，但是可能还要等个3\sphinxhyphen{}4年。雪锋这时候说：我也有这个想法啊，你要是写的话，我能帮助到你吗？

\sphinxAtStartPar
雪锋这句话其实点醒了我。传统的书籍写作，往往是依赖于1\sphinxhyphen{}2个教授将学科十余年的发展慢慢总结，整理出书。这种模式类似于传统软件开发的瀑布流方式。可是，科技的世界已经变了！软件的发展从传统的瀑布流进化到如今的开源，敏捷开发。而书籍的写作为什么还要停留在传统方式呢？MXNet团队构建开源社区来编写的专注于深度学习算法的书籍《Deep
Dive into Deep
Learning》就是一个很好的例子啊。我因此马上找到当年一起创立TensorLayer开源社区的小伙伴北京大学的董豪，我们一拍即合，说干就干。雪锋也很高兴我和董豪愿意开始做这件事，也邀请了他的同事干志良进来帮助我们。我们终于开始书籍的写作了！

\sphinxAtStartPar
经过几轮的讨论，我们将书籍的名字定为《机器学习系统：设计和实现》。我们希望这本书能教给学生经受住时间检验的机器学习系统设计原理，同时也提供大量的系统实现经验分享，让他们将来工作，科研中遇到实际问题知道该如何分析和解决。


\section{社区的构建}
\label{\detokenize{chapter_preface/index:id4}}
\sphinxAtStartPar
考虑到机器学习系统本身就是一个依然在发展，试错，并且频繁孕育细分领域的学科。我从一开始就在思考：如何设计一个高度可扩展（Scalable）的社区架构来保证这本书的可持续发展呢？因为我是专注于大规模软件系统的老师，我决定借鉴几个分布式系统的设计要点来构建社区：
\begin{itemize}
\item {} 
\sphinxAtStartPar
预防单点瓶颈：现代分布式系统往往采用控制层和数据层分离的设计来避免单点故障和瓶颈。那么我们在设计高度可扩展的写作社区的时候，也要如此。因此，我们设计了如下分布式机制：编辑（类似于分布式系统的Leader）决定花最大的时间来寻找每个章节最优秀，主动，负责任的章节负责人(Local
leader)。而章节负责人可以进一步寻找其他作者（Follower）共同协作。而章节负责人和章节作者进行密切的沟通，按照给定时间节点，全速异步推进。而编辑和章节负责人设定了每隔1周的讨论来同步（Synchronise）写作的进展，确保并行完成的章节质量能够持续符合编辑和社区的整体预期。

\item {} 
\sphinxAtStartPar
迭代式改进：深度学习的优化算法随机梯度下降本质上是在复杂问题中利用局部梯度进行海量迭代，最终找到优秀的局部最优解。我因此利用了同样的思路来设计书籍质量的迭代提高。我们首先在Overleaf上写作好书籍的初版（类似于初始参数，Initial
Weights）。接下来，我们进一步将书籍的内容做成标准的Git代码仓库（Book
as
code）。建立机制鼓励开源社区和广大读者开启Issue和PR，频繁改进书籍（相当于梯度，Gradients），而我们设置好完善的书籍构建工具，持续集成工具，贡献者讨论会，标准化的Issue和PR合并流程等等，就可以让书籍的质量持续提高实现随机梯度下降（Stochastic
Gradient Descent）一样的最终最优性。

\item {} 
\sphinxAtStartPar
高可用性：我们要有7x24小时在线的平台，让书籍可以在全球任何时区，任何语言平台下都能参与开发，倾听社区的反馈。因此我们将Git仓库放置在GitHub上，并准备之后在Gitee做好镜像。这样，我们就搭建了一套高可用的写作平台了。

\item {} 
\sphinxAtStartPar
内容中立：一个分布式系统要能长久运行，其中的每一个节点我们要同等对待，遇到故障才能用统一的办法来进行故障恢复。考虑到书籍写作中的故障（设计无法经受时间检验，写作人中途不得不退出等等）可以来源于方方面面，我们让不同背景的参与者共同完成每一个章节，确保写出中立，客观，包容各类型观点的书籍内容，并且写作不会因为故障而中断。

\end{itemize}


\section{现状和未来}
\label{\detokenize{chapter_preface/index:id5}}
\sphinxAtStartPar
机制一旦建立好，写作就自动化地跑起来了，同行人也越来越多，我带过的学生袁秀龙、丁子涵、符尧也很用心参与，董豪邀请了鹏城实验室的韩佳容和赖铖，志良邀请了许多MindSpore的小伙伴进来贡献，许多资深的AI框架的设计者也和我们在各个渠道展开讨论，提供了非常多宝贵的写作建议。另外，学界的教授（Peter
Pietzuch老师，陈雷老师等）也持续给我们内容提供详细的反馈。

\sphinxAtStartPar
充分发动了“分布式系统”的力量后，书籍的内容得以持续高质量的合并了进来。当我们开源了书籍以后，书籍的受众快速增长，GitHub上的关注度增长让我们受宠若惊。在社区的推动下，书籍的中文版，英文版，阿拉伯语版都已经开始推进。这么多年来，我第一次意识到我在分布式系统和机器学习里面学习到的知识，在解决现实复杂问题的时候是如此的有用！

\sphinxAtStartPar
很多时候，当我们面对未知而巨大的困难，个人的力量真的渺小。而和朋友，社区一起，就变成了强大的力量，让我们鼓起勇气，走出了最关键的第一步！希望我的一些思考，能给其他复杂问题的求解带来一些小小的启发。

\sphinxAtStartPar
最后，我们非常欢迎新成员的加入来帮助书籍提升质量，扩展内容。感兴趣的读者可以通过书籍的GitHub社区：\sphinxurl{https://github.com/openmlsys/}
联系到我们，我们非常期待和大家一起努力，写出世界上第一本机器学习系统的书籍！

\sphinxAtStartPar
麦络

\sphinxAtStartPar
写于英国爱丁堡

\sphinxAtStartPar
2022年5月4日


\chapter{导论}
\label{\detokenize{chapter_introduction/index:id1}}\label{\detokenize{chapter_introduction/index::doc}}
\sphinxAtStartPar
本章第一部分首先介绍机器学习的概貌，以及其在系统角度的共性问题。
本章第二部分介绍系统设计的需求和目标，让读者从宏观上来了解系统需要满足的内容，对系统设计的主要用例、设计规格有一个清晰的了解。
本章最后部分介绍机器学习系统的组成原理，让读者对系统整体的实现有一个初步的、宏观的理解。


\section{机器学习应用}
\label{\detokenize{chapter_introduction/machine_learning_applications:id1}}\label{\detokenize{chapter_introduction/machine_learning_applications::doc}}
\sphinxAtStartPar
通俗来讲，机器学习是指从数据中学习出有用知识的技术。从学习模式来说，机器学习可以分为监督学习（Supervised
Learning）、无监督学习（Unsupervised Learning）、强化学习（Reinforcement
Learning）等：
\begin{itemize}
\item {} 
\sphinxAtStartPar
监督学习是已知输入输出对应关系情况下的学习，比如：
给定输入图像和它对应的内容标签，学习图像分类（Classification）。

\item {} 
\sphinxAtStartPar
无监督学习是只有输入数据但不知道输出标签情况下的学习，比如：给定一堆猫和狗的图像，自主学会猫和狗的分类，这种无监督分类也称为聚类（Clustering）。

\item {} 
\sphinxAtStartPar
强化学习则是给定一个学习环境和任务目标，算法自主地去不断尝试、改进自己、以实现任务目标
，比如：
AlphaGo围棋就是用强化学习实现的，给定的环境是围棋的规则、而目标则是胜利得分。

\end{itemize}

\sphinxAtStartPar
从应用领域上划分，主要包括计算机视觉、自然语言处理和智能决策这三大部分，而且这三大部分之间也有交集。
狭义上来讲基于图像的应用都可归为计算机视觉方面的应用，典型的应用有人脸识别、物体识别、目标跟踪、人体姿态估计、以及图像的理解、修复、分割与检测等等。
计算机视觉方法广泛应用于自动驾驶、智慧城市、智慧安防等领域。
自然语言处理涉及文本或者语音方面的应用，典型的应用包括语言翻译、文本转语音、语音转文本、以及文本理解、分类、风格变换与纠错等等。
计算机视觉和自然语言处理有很多交集，例如图像的文本描述生成、基于文本的图像生成、基于文本的图像处理等应用都同时涉及到了语言和图像两种数据类型。
智能决策方面，往往通过结合计算机视觉、自然语言处理、强化学习、控制论等技术手段，实现决策类任务，广泛用于机器人、自动驾驶、游戏、推荐系统、智能工厂、智能电网等领域。

\sphinxAtStartPar
经典的机器学习算法有支持向量机（Support Vector
Machine，SVM）、逻辑回归（Logistic Regression）、朴素贝叶斯（Naive
Bayes）
等方法。然而得力于大数据互联网和计算机性能的提升，以深度学习（Deep
Learning）为代表的方法得到了广泛的研究和应用。
虽然机器学习算法很多，但无论是经典算法还是深度学习算法的计算往往以向量、矩阵运算为主体的，因此本书主要通过深度神经网络为例子展开机器学习系统的介绍。下面我们来快速了解一下机器学习系统的设计需求、实现目标以及组成原理。


\section{设计目标}
\label{\detokenize{chapter_introduction/requirements_for_machine_learning_systems:id1}}\label{\detokenize{chapter_introduction/requirements_for_machine_learning_systems::doc}}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{framework_position}.png}
\caption{机器学习框架}\label{\detokenize{chapter_introduction/requirements_for_machine_learning_systems:id2}}\label{\detokenize{chapter_introduction/requirements_for_machine_learning_systems:framework-position}}\end{figure}

\sphinxAtStartPar
开发者需要设计和实现机器学习系统来满足以下目标（如
\hyperref[\detokenize{chapter_introduction/requirements_for_machine_learning_systems:framework-position}]{图\ref{\detokenize{chapter_introduction/requirements_for_machine_learning_systems:framework-position}}}所示）：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{支持多种神经网络：}
深度学习的巨大成功使得神经网络成为了机器学习应用的核心。不同应用需要不同的神经网络，例如，卷积神经网络（Convolutional
Neural Networks），图神经网络（Graph Neural
Networks），自注意力神经网络（Self\sphinxhyphen{}Attention Neural
Networks）等。这些神经网络需要一个共同的系统软件来进行开发和运行。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{支持自动微分：}
为了训练神经网络，我们需要利用数据、标注（Label）和目标损失函数（Loss
Function）来计算梯度（Gradients）。因此，机器学习系统需要有一个通用的方法来\sphinxstylestrong{自动化}计算梯度（这一过程被称之为自动微分）。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{支持数据管理和处理：}
机器学习的核心是数据。这些数据包括训练、评估、测试数据集和模型参数。因此，我们需要系统本身支持数据读取、存储和预处理（例如，数据增强和数据清洗）。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{支持模型的训练和部署：}
为了让机器学习模型达到最佳的性能，人们需要使用优化方法（例如，Mini\sphinxhyphen{}Batch
SGD）来通过多步迭代反复计算梯度（这一过程称之为训练）。训练完成后，系统需要将训练好的模型部署推理设备。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{高效使用硬件加速器：}
神经网络的相关计算往往通过矩阵计算实现。这一类计算可以被硬件加速器（例如，通用图形处理器\sphinxhyphen{}GPU）加速。因此，机器学习系统需要高效利用多种硬件加速器。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{分布式计算：}
随着训练数据量和神经网络参数量的上升，机器学习系统的内存用量远远超过了单个机器可以提供的内存。因此，机器学习框架需要天然具备分布式执行的能力。

\end{itemize}

\sphinxAtStartPar
在设计机器学习系统之初，开发者曾尝试拓展\sphinxstylestrong{神经网络开发库}（如Theano和Caffe）和\sphinxstylestrong{大数据计算框架}（如Apache
Spark和Google Pregel）来达到以上目标。可是他们发现（如
\hyperref[\detokenize{chapter_introduction/requirements_for_machine_learning_systems:comparison-of-ml-frameworks}]{表\ref{\detokenize{chapter_introduction/requirements_for_machine_learning_systems:comparison-of-ml-frameworks}}}所示），
神经网络库虽然提供了神经网络开发、自动微分和硬件加速器的支持，但是其缺乏管理和处理大型数据集、模型部署和分布式执行的能力，无法满足产品级机器学习应用的开发任务。
另一方面，虽然大数据计算框架具有成熟的分布式执行和数据管理能力，但是其缺乏对神经网络、自动微分和加速器的支持，使得其并不适合开发以神经网络为核心的机器学习应用。因此，业界设计出了包括MindSpore、PaddlePaddle、TensorFlow，PyTorch等一系列新型机器学习系统（框架）。


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{机器学习框架和相关系统的比较}\label{\detokenize{chapter_introduction/requirements_for_machine_learning_systems:id3}}\label{\detokenize{chapter_introduction/requirements_for_machine_learning_systems:comparison-of-ml-frameworks}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|}
\hline

\sphinxAtStartPar

&\sphinxstyletheadfamily 
\sphinxAtStartPar
神经网络
&\sphinxstyletheadfamily 
\sphinxAtStartPar
自动微分
&\sphinxstyletheadfamily 
\sphinxAtStartPar
数据管理
&\sphinxstyletheadfamily 
\sphinxAtStartPar
训练和部署
&\sphinxstyletheadfamily 
\sphinxAtStartPar
加速器
&\sphinxstyletheadfamily 
\sphinxAtStartPar
分布式
\\
\hline
\sphinxAtStartPar
神经网络库
&
\sphinxAtStartPar
是
&
\sphinxAtStartPar
是
&
\sphinxAtStartPar
否
&
\sphinxAtStartPar
否
&
\sphinxAtStartPar
是
&
\sphinxAtStartPar
否
\\
\hline
\sphinxAtStartPar
大数据框架
&
\sphinxAtStartPar
否
&
\sphinxAtStartPar
否
&
\sphinxAtStartPar
是
&
\sphinxAtStartPar
否
&
\sphinxAtStartPar
否
&
\sphinxAtStartPar
是
\\
\hline
\sphinxAtStartPar
机器学习框架
&
\sphinxAtStartPar
是
&
\sphinxAtStartPar
是
&
\sphinxAtStartPar
是
&
\sphinxAtStartPar
是
&
\sphinxAtStartPar
是
&
\sphinxAtStartPar
是
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\section{基本组成}
\label{\detokenize{chapter_introduction/components_of_machine_learning_systems:id1}}\label{\detokenize{chapter_introduction/components_of_machine_learning_systems::doc}}
\sphinxAtStartPar
一个完整的机器学习系统往往具有如
\hyperref[\detokenize{chapter_introduction/components_of_machine_learning_systems:framework-architecture}]{图\ref{\detokenize{chapter_introduction/components_of_machine_learning_systems:framework-architecture}}}所示的基本架构。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{framework_architecture}.png}
\caption{机器学习框架基本构成}\label{\detokenize{chapter_introduction/components_of_machine_learning_systems:id2}}\label{\detokenize{chapter_introduction/components_of_machine_learning_systems:framework-architecture}}\end{figure}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{编程接口:}
为了支持广泛的开发者，机器学习框架的编程接口不仅需要高层次简易编程（例如，Python，Julia和Java），同时也需要支持低层次高性能编程（利用C和C++函数调用操作系统和硬件加速器）。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{计算图：}
利用不同编程接口实现的机器学习程序需要共享一个运行后端。实现这一后端的关键技术是：应用无关的计算图。计算图包含计算节点，节点之间的边表达计算依赖。计算图可以被同步和异步执行。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{编译器前端：}
给定一个计算图，机器学习框架会对计算图做一系列优化。和硬件无关的优化由编译器前端实现。编译器前端实现包括：中间表达，自动微分，类型推导和静态分析等等。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{编译器后端和运行时：}
机器学习框架利用编译器后端对计算图可以进一步针对硬件的特性（例如说，L2/L3大小，指令流水线长度）进行性能优化。最终优化后的计算图通过运行时执行在通用处理器（CPU）或者是硬件加速器之上。运行时需要实现算子选择和内存分配等技术。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{硬件加速器：}
现代硬件加速器提供了丰富的编程接口。在本书中，我们将会介绍硬件加速器的基本组成原理和编程接口。我们同时会给出一个硬件加速器使用案例来从0到1讲述如何高效使用加速器。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{数据处理：}
机器学习系统拥有专门的数据处理框架来实现数据读取，存储和预处理的功能由数据处理模块（例如，TensorFlow的tf.data和PyTorch的DataLoader）。这一框架需要针对机器学习应用实现易用性，保序性和高效性等设计目标。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{模型部署：}
在模型完成训练后，下一个常用的系统功能是：模型部署。为了确保模型可以在内存有限的硬件上执行，我们会使用模型转换，量化，蒸馏等模型压缩技术。同时，我们也需要实现针对推理硬件平台（例如，英伟达Jetson）的模型算子优化。最后，为了保证模型的安全（不被黑客窃取），实践者还会对模型进行混淆设计。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{分布式训练：}
分布式训练日渐成为一个机器学习框架的核心组件。本书将介绍常见的分布式训练方法（数据并行，模型并行，混合并行和流水线并行）。同时我们会深入介绍这些方法的高效系统实现（包括集合通讯库和参数服务器）。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{拓展模块：}
机器学习系统的广泛部署使得许多的扩展模块陆续出现。本书将会介绍得到大量实践部署的拓展模块：深度学习推荐系统，联邦学习系统，强化学习系统，可解释性AI系统和机器人系统。

\end{itemize}

\sphinxAtStartPar
机器学习算法相关的理论知识是本书的预备知识，本书不做深入讨论。基础的机器学习理论知识可以在附录中找到。


\section{适用读者}
\label{\detokenize{chapter_introduction/applicable_readers:id1}}\label{\detokenize{chapter_introduction/applicable_readers::doc}}
\sphinxAtStartPar
本书由浅入深地讨论机器学习系统的设计原理和实现经验。其读者包括：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{学生：}
本书将帮助学生获得大量机器学习系统的设计原则和一手实践经验。从而帮助其更全面理解机器学习算法的实践挑战和理论优劣。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{科研人员：}
本书将帮助科研人员学习到机器学习落地实践中遇到的种种挑战，引导设计出能解决大规模实际问题的下一代机器学习算法。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{开发人员：}
本书将帮助开发人员深刻理解机器学习系统的内部架构，从而帮助其优化系统性能，调试问题，并且根据业务需求对机器学习系统进行定制。

\end{itemize}


\chapter{编程接口}
\label{\detokenize{chapter_programming_interface/index:id1}}\label{\detokenize{chapter_programming_interface/index::doc}}
\sphinxAtStartPar
现代机器学习框架包含大量的组件。这些组件使得用户得以高效开发机器学习算法，处理数据，部署模型，性能调优和使用硬件加速器。在设计这些组件的编程接口时，一个核心的诉求是：如何平衡框架性能和易用性？为了达到最优的性能，开发者需要利用硬件亲和的编程语言如：C和C++来进行开发。这是因为：C和C++的使用使得机器学习框架可以高效调用硬件的底层API，从而最大限度发挥硬件性能。同时，现代操作系统（如Linux和Windows）提供丰富的基于C和C++的编程接口（如文件系统，网络编程，多线程管理等），通过直接调用操作系统API，可以降低框架运行的开销。

\sphinxAtStartPar
从易用性的角度分析，机器学习框架的使用者往往具有丰富的行业背景（如数据科学家，生物学家，化学家，物理学家等）。他们常用的编程语言是高层次脚本语言：Python，Matlab，R和Julia。相比于C和C++，这些语言在提供编程的易用性的同时，丧失了C和C++对底层硬件和操作系统进行深度优化的能力。因此，机器学习框架的核心设计目标是：其要具有易用编程接口来支持用户用高层次语言如Python来实现机器学习算法，同时其也要具备以C和C++为核心的低层次编程接口，使得框架开发者可以用C和C++实现大量高性能组件，从而在硬件上高效执行。在本章中，我们将会讲述如何达到这个设计目标。

\sphinxAtStartPar
本章的学习目标包括：
\begin{itemize}
\item {} 
\sphinxAtStartPar
理解机器学习系统的工作流和以Python为核心的编程接口设计。

\item {} 
\sphinxAtStartPar
理解机器学习系统以神经网络模块为核心的接口设计原理和实现。

\item {} 
\sphinxAtStartPar
理解机器学习系统的底层C/C++执行算子的实现和与上层Python接口的调用实现。

\item {} 
\sphinxAtStartPar
了解机器学习系统编程接口的演进方向。

\end{itemize}


\section{机器学习系统编程模型的演进}
\label{\detokenize{chapter_programming_interface/development_history:id1}}\label{\detokenize{chapter_programming_interface/development_history::doc}}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{framework_development_history}.svg}
\caption{机器学习编程库发展历程}\label{\detokenize{chapter_programming_interface/development_history:id2}}\label{\detokenize{chapter_programming_interface/development_history:img-framedh}}\end{figure}

\sphinxAtStartPar
随着机器学习系统的诞生，如何设计易用且高性能的编程接口就一直成为了框架设计者首要解决的问题。在早期的机器学习框架中（如
\hyperref[\detokenize{chapter_programming_interface/development_history:img-framedh}]{图\ref{\detokenize{chapter_programming_interface/development_history:img-framedh}}}所示），人们选择用Lua（Torch）和Python（Theano）等高层次编程语言来编写机器学习程序。这些早期的机器学习框架提供了机器学习必须的模型定义，自动微分等功能，其适用于编写小型和科研为导向的机器学习应用。

\sphinxAtStartPar
在2011年，深度神经网络快速崛起，并很快在各个AI应用领域（计算机视觉，语音识别，自然语言处理等）取得了最先进的性能。训练深度神经网络需要消耗大量的算力，而这些算力无法被以Lua和Python所主导开发的Torch和Theano所满足。与此同时，计算加速卡（如英伟达GPU）的通用编程接口（例如CUDA
C）日趋成熟，而构建于CPU多核技术之上的多线程库（POSIX
Threads）也被广大开发者所接受。因此，许多的机器学习用户希望基于C和C++来开发高性能的深度学习应用。这一类需求被Caffe等一系列以C和C++作为核心编程接口的框架所满足。

\sphinxAtStartPar
然而，机器学习模型往往需要针对部署场景，数据类型，识别任务等需求进行深度定制，而这类定制任务需要被广大的AI应用领域的开发者所实现。这类开发者的背景多样，其往往不具有熟练使用C和C++的背景，因此Caffe这一类库与C和C++深度绑定的编程模型快速成为了制约这一类框架快速推广的巨大瓶颈。

\sphinxAtStartPar
在2016年，谷歌率先推出了TensorFlow。相比于传统的Caffe，Torch和Theano，TensorFlow提出利用高层次编程语言：Python作为面向用户的主要前端语言，而利用C和C++实现高性能后端。大量基于Python的前端API确保了TensorFlow可以被大量的数据科学家和机器学习科学家接受，同时帮助TensorFlow能够快速融入Python为主导的大数据生态（大量的大数据开发库如Numpy，Pandas，SciPy，
Matplotlib和PySpark）。同时，Python具有出色的和C语言的互操作性，这种互操作性已经在多个Python库中得到验证。因此，TensorFlow兼有Python的灵活性和生态，同时也通过C/C++后端得以实现高性能。这种设计在日后崛起的PyTorch，MXNet和CNTK的机器学习框架得到传承。

\sphinxAtStartPar
随着多个机器学习框架的出现，Keras和TensorLayer等高层次机器学习开发库提供了更高层次的Python
API从而可以快速导入已有的模型，
这些高层次API进一步屏蔽了底层框架的实现细节，因此Keras和TensorLayer可以运行在不同的机器学习框架之上。

\sphinxAtStartPar
随着深度神经网络的进一步发展，对于机器学习框架编程接口的挑战也日益增长。因此在2020年前后，新型的机器学习框架如MindSpore和JAX进一步出现。其中，MindSpore在继承了TensorFlow，PyTorch的Python和C/C++的混合接口的基础上，进一步拓展了机器学习编程模型从而可以高效支持多种AI后端芯片（如华为Ascend，英伟达GPU和ARM芯片），实现了机器学习应用在海量异构设备上的快速部署。

\sphinxAtStartPar
同时，超大型数据集和超大型深度神经网络崛起让分布式执行成为了机器学习框架编程模型的核心设计需求。为了实现分布式执行，TensorFlow和PyTorch的使用者需要进行大量编程来将数据集和神经网络分配到分布式节点上，而大量的AI开发人员并不具有分布式编程的能力。因此MindSpore进一步完善了机器学习框架的分布式编程模型的能力，从而让单节点的MindSpore程序可以无缝地运行在海量节点上。

\sphinxAtStartPar
在本小节中，我们将以MindSpore作为例子讲解一个现代机器学习框架的Python前端API和C/C++后端API的设计原则。这些设计原则和PyTorch，TensorFlow相似。


\section{机器学习工作流}
\label{\detokenize{chapter_programming_interface/ml_workflow:id1}}\label{\detokenize{chapter_programming_interface/ml_workflow::doc}}
\sphinxAtStartPar
机器学习系统编程模型的首要设计目标是：对开发者的整个工作流进行完整的编程支持。一个常见的机器学习任务一般包含如
\hyperref[\detokenize{chapter_programming_interface/ml_workflow:img-workflow}]{图\ref{\detokenize{chapter_programming_interface/ml_workflow:img-workflow}}}所示的流程。这个工作流完成了训练数据集的读取，模型的训练，测试和调试。通过归纳，我们可以将这一工作流中用户所需要自定义的部分通过定义以下API来支持（我们这里假设用户的高层次API以Python函数的形式提供）：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{数据处理：}
首先，用户需要数据处理API来支持将数据集从磁盘读入。进一步，用户需要对读取的数据进行预处理，从而可以将数据输入后续的机器学习模型中。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{模型结构：}
完成数据的读取后，用户需要模型定义API来定义机器学习模型。这些模型带有模型参数，可以对给定的数据进行推理。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{损失函数和优化算法：}
模型的输出需要和用户的标记进行对比，这个对比差异一般通过损失函数（Loss
function）来进行评估。因此，优化器定义API允许用户定义自己的损失函数，并且根据损失来引入（Import）和定义各种优化算法（Optimisation
algorithms）来计算梯度（Gradient），完成对模型参数的更新。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{训练过程：}
给定一个数据集，模型，损失函数和优化器，用户需要训练API来定义一个循环（Loop）从而将数据集中的数据按照小批量（mini\sphinxhyphen{}batch）的方式读取出来，反复计算梯度来更新模型。这个反复的过程称为训练。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{测试和调试：}
训练过程中，用户需要测试API来对当前模型的精度进行评估。当精度达到目标后，训练结束。这一过程中，用户往往需要调试API来完成对模型的性能和正确性进行验证。

\end{itemize}

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{img_workflow}.svg}
\caption{机器学习系统工作流}\label{\detokenize{chapter_programming_interface/ml_workflow:id8}}\label{\detokenize{chapter_programming_interface/ml_workflow:img-workflow}}\end{figure}


\subsection{环境配置}
\label{\detokenize{chapter_programming_interface/ml_workflow:id2}}
\sphinxAtStartPar
下面以MindSpore框架实现多层感知机为例，了解完整的机器学习工作流。代码运行环境为MindSpore1.5.2，Ubuntu16.04，CUDA10.1。
在构建机器学习工作流程前，MindSpore需要通过context.set\_context来配置运行需要的信息，如运行模式、后端信息、硬件等信息。
以下代码导入context模块，配置运行需要的信息。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{argparse}
\PYG{k+kn}{from} \PYG{n+nn}{mindspore} \PYG{k+kn}{import} \PYG{n}{context}
\PYG{n}{parser} \PYG{o}{=} \PYG{n}{argparse}\PYG{o}{.}\PYG{n}{ArgumentParser}\PYG{p}{(}\PYG{n}{description}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MindSpore MLPNet Example}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}argument}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}device\PYGZus{}target}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb}{type}\PYG{o}{=}\PYG{n+nb}{str}\PYG{p}{,} \PYG{n}{default}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{CPU}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{choices}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ascend}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{GPU}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CPU}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{args} \PYG{o}{=} \PYG{n}{parser}\PYG{o}{.}\PYG{n}{parse\PYGZus{}known\PYGZus{}args}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{context}\PYG{o}{.}\PYG{n}{set\PYGZus{}context}\PYG{p}{(}\PYG{n}{mode}\PYG{o}{=}\PYG{n}{context}\PYG{o}{.}\PYG{n}{GRAPH\PYGZus{}MODE}\PYG{p}{,} \PYG{n}{device\PYGZus{}target}\PYG{o}{=}\PYG{n}{args}\PYG{o}{.}\PYG{n}{device\PYGZus{}target}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
上述配置样例运行使用图模式。根据实际情况配置硬件信息，譬如代码运行在Ascend
AI处理器上，则–device\_target选择Ascend，代码运行在CPU、GPU同理。


\subsection{数据处理}
\label{\detokenize{chapter_programming_interface/ml_workflow:id3}}
\sphinxAtStartPar
配置好运行信息后，首先讨论数据处理API的设计。这些API提供了大量Python函数支持用户用一行命令即可读入常见的训练数据集（如MNIST，CIFAR，COCO等）。
在加载之前需要将下载的数据集存放在./datasets/MNIST\_Data路径中；MindSpore提供了用于数据处理的API模块
mindspore.dataset，用于存储样本和标签。在加载数据集前，通常会对数据集进行一些处理，mindspore.dataset也集成了常见的数据处理方法。
以下代码读取了MNIST的数据是大小为\(28 \times 28\)的图片，返回DataSet对象。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{dataset} \PYG{k}{as} \PYG{n+nn}{ds}
\PYG{n}{DATA\PYGZus{}DIR} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./datasets/MNIST\PYGZus{}Data/train}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{mnist\PYGZus{}dataset} \PYG{o}{=} \PYG{n}{ds}\PYG{o}{.}\PYG{n}{MnistDataset}\PYG{p}{(}\PYG{n}{DATA\PYGZus{}DIR}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
有了DataSet对象后，通常需要对数据进行增强，常用的数据增强包括翻转、旋转、剪裁、缩放等；在MindSpore中是使用map将数据增强的操作映射到数据集中的，之后进行打乱（Shuffle）和批处理（Batch）。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} 导入需要用到的模块}
\PYG{k+kn}{import} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{dataset} \PYG{k}{as} \PYG{n+nn}{ds}
\PYG{k+kn}{import} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{dataset}\PYG{n+nn}{.}\PYG{n+nn}{transforms}\PYG{n+nn}{.}\PYG{n+nn}{c\PYGZus{}transforms} \PYG{k}{as} \PYG{n+nn}{C}
\PYG{k+kn}{import} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{dataset}\PYG{n+nn}{.}\PYG{n+nn}{vision}\PYG{n+nn}{.}\PYG{n+nn}{c\PYGZus{}transforms} \PYG{k}{as} \PYG{n+nn}{CV}
\PYG{k+kn}{from} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{dataset}\PYG{n+nn}{.}\PYG{n+nn}{vision} \PYG{k+kn}{import} \PYG{n}{Inter}
\PYG{k+kn}{from} \PYG{n+nn}{mindspore} \PYG{k+kn}{import} \PYG{n}{dtype} \PYG{k}{as} \PYG{n}{mstype}
\PYG{c+c1}{\PYGZsh{} 数据处理过程}
\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}dataset}\PYG{p}{(}\PYG{n}{data\PYGZus{}path}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{32}\PYG{p}{,} \PYG{n}{repeat\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
                   \PYG{n}{num\PYGZus{}parallel\PYGZus{}workers}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} 定义数据集}
    \PYG{n}{mnist\PYGZus{}ds} \PYG{o}{=} \PYG{n}{ds}\PYG{o}{.}\PYG{n}{MnistDataset}\PYG{p}{(}\PYG{n}{data\PYGZus{}path}\PYG{p}{)}
    \PYG{n}{resize\PYGZus{}height}\PYG{p}{,} \PYG{n}{resize\PYGZus{}width} \PYG{o}{=} \PYG{l+m+mi}{32}\PYG{p}{,} \PYG{l+m+mi}{32}
    \PYG{n}{rescale} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{o}{/} \PYG{l+m+mf}{255.0}
    \PYG{n}{rescale\PYGZus{}nml} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{/} \PYG{l+m+mf}{0.3081}
    \PYG{n}{shift\PYGZus{}nml} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1} \PYG{o}{*} \PYG{l+m+mf}{0.1307} \PYG{o}{/} \PYG{l+m+mf}{0.3081}

    \PYG{c+c1}{\PYGZsh{} 定义所需要操作的map映射}
    \PYG{n}{resize\PYGZus{}op} \PYG{o}{=} \PYG{n}{CV}\PYG{o}{.}\PYG{n}{Resize}\PYG{p}{(}\PYG{p}{(}\PYG{n}{resize\PYGZus{}height}\PYG{p}{,} \PYG{n}{resize\PYGZus{}width}\PYG{p}{)}\PYG{p}{,} \PYG{n}{interpolation}\PYG{o}{=}\PYG{n}{Inter}\PYG{o}{.}\PYG{n}{LINEAR}\PYG{p}{)}
    \PYG{n}{rescale\PYGZus{}nml\PYGZus{}op} \PYG{o}{=} \PYG{n}{CV}\PYG{o}{.}\PYG{n}{Rescale}\PYG{p}{(}\PYG{n}{rescale\PYGZus{}nml} \PYG{o}{*} \PYG{n}{rescale}\PYG{p}{,} \PYG{n}{shift\PYGZus{}nml}\PYG{p}{)}
    \PYG{n}{hwc2chw\PYGZus{}op} \PYG{o}{=} \PYG{n}{CV}\PYG{o}{.}\PYG{n}{HWC2CHW}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{type\PYGZus{}cast\PYGZus{}op} \PYG{o}{=} \PYG{n}{C}\PYG{o}{.}\PYG{n}{TypeCast}\PYG{p}{(}\PYG{n}{mstype}\PYG{o}{.}\PYG{n}{int32}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} 使用map映射函数，将数据操作应用到数据集}
    \PYG{n}{mnist\PYGZus{}ds} \PYG{o}{=} \PYG{n}{mnist\PYGZus{}ds}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{operations}\PYG{o}{=}\PYG{n}{type\PYGZus{}cast\PYGZus{}op}\PYG{p}{,} \PYG{n}{input\PYGZus{}columns}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{label}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{num\PYGZus{}parallel\PYGZus{}workers}\PYG{o}{=}\PYG{n}{num\PYGZus{}parallel\PYGZus{}workers}\PYG{p}{)}
    \PYG{n}{mnist\PYGZus{}ds} \PYG{o}{=} \PYG{n}{mnist\PYGZus{}ds}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{operations}\PYG{o}{=}\PYG{p}{[}\PYG{n}{resize\PYGZus{}op}\PYG{p}{,} \PYG{n}{rescale\PYGZus{}nml\PYGZus{}op}\PYG{p}{,}\PYG{n}{hwc2chw\PYGZus{}op}\PYG{p}{]}\PYG{p}{,} \PYG{n}{input\PYGZus{}columns}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{image}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{num\PYGZus{}parallel\PYGZus{}workers}\PYG{o}{=}\PYG{n}{num\PYGZus{}parallel\PYGZus{}workers}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} 进行shuffle、batch操作}
    \PYG{n}{buffer\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{10000}
    \PYG{n}{mnist\PYGZus{}ds} \PYG{o}{=} \PYG{n}{mnist\PYGZus{}ds}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{buffer\PYGZus{}size}\PYG{o}{=}\PYG{n}{buffer\PYGZus{}size}\PYG{p}{)}
    \PYG{n}{mnist\PYGZus{}ds} \PYG{o}{=} \PYG{n}{mnist\PYGZus{}ds}\PYG{o}{.}\PYG{n}{batch}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{n}{drop\PYGZus{}remainder}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{mnist\PYGZus{}ds}
\end{sphinxVerbatim}


\subsection{模型定义}
\label{\detokenize{chapter_programming_interface/ml_workflow:id4}}
\sphinxAtStartPar
使用MindSpore定义神经网络需要继承mindspore.nn.Cell，神经网络的各层需要预先在\_\_init\_\_方法中定义，然后重载\_\_construct\_\_方法实现神经网络的前向传播过程。
因为输入大小被处理成\(32 \times 32\)的图片，所以需要用Flatten将数据压平为一维向量后给全连接层。
全连接层的输入大小为\(32 \times 32\)，输出是预测属于\(0 \sim 9\)中的哪个数字，因此输出大小为10，下面定义了一个三层的全连接层。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} 导入需要用到的模块}
\PYG{k+kn}{import} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{nn} \PYG{k}{as} \PYG{n+nn}{nn}
\PYG{c+c1}{\PYGZsh{} 定义线性模型}
\PYG{k}{class} \PYG{n+nc}{MLPNet}\PYG{p}{(}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{Cell}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MLPNet}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{flatten} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{Flatten}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dense1} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{32}\PYG{o}{*}\PYG{l+m+mi}{32}\PYG{p}{,} \PYG{l+m+mi}{128}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dense2} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{l+m+mi}{64}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dense3} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{construct}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{inputs}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{)}
        \PYG{n}{x} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dense1}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
        \PYG{n}{x} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dense2}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
        \PYG{n}{logits} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dense3}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{logits}
\PYG{c+c1}{\PYGZsh{} 实例化网络}
\PYG{n}{net} \PYG{o}{=} \PYG{n}{MLPNet}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{损失函数和优化器}
\label{\detokenize{chapter_programming_interface/ml_workflow:id5}}
\sphinxAtStartPar
有了神经网络组件构建的模型我们还需要定义\sphinxstylestrong{损失函数}来计算训练过程中输出和真实值的误差。\sphinxstylestrong{均方误差}(Mean
Squared
Error，MSE)是线性回归中常用的，是计算估算值与真实值差值的平方和的平均数。
\sphinxstylestrong{平均绝对误差}（Mean Absolute
Error，MAE）是计算估算值与真实值差值的绝对值求和再求平均。
\sphinxstylestrong{交叉熵}（Cross
Entropy，CE）是分类问题中常用的，衡量已知数据分布情况下，计算输出分布和已知分布的差值。

\sphinxAtStartPar
有了损失函数，我们就可以通过损失值利用\sphinxstylestrong{优化器}对参数进行训练更新。对于优化的目标函数\(f(x)\)；先求解其梯度\(\nabla\)\(f(x)\)，然后将训练参数\(W\)沿着梯度的负方向更新，更新公式为：\(W_t = W_{t-1} - \alpha\nabla(W_{t-1})\)，其中\(\alpha\)是学习率，\(W\)是训练参数，\(\nabla(W_{t-1})\)是方向。
神经网络的优化器种类很多，一类是学习率不受梯度影响的随机梯度下降（Stochastic
Gradient
Descent）及SGD的一些改进方法，如带有Momentum的SGD；另一类是自适应学习率如AdaGrad、RMSProp、Adam等。

\sphinxAtStartPar
\sphinxstylestrong{SGD}的更新是对每个样本进行梯度下降，因此计算速度很快，但是单样本更新频繁，会造成震荡；为了解决震荡问题，提出了带有Momentum的SGD，该方法的参数更新不仅仅由梯度决定，也和累计的梯度下降方向有关，使得增加更新梯度下降方向不变的维度，减少更新梯度下降方向改变的维度，从而速度更快也减少震荡。

\sphinxAtStartPar
自适应学习率\sphinxstylestrong{AdaGrad}是通过以往的梯度自适应更新学习率，不同的参数\(W_i\)具有不同的学习率。AdaGrad对频繁变化的参数以更小的步长更新，而稀疏的参数以更大的步长更新。因此对稀疏的数据表现比较好。\sphinxstylestrong{Adadelta}是对AdaGrad的改进，解决了AdaGrad优化过程中学习率\(\alpha\)单调减少问题；Adadelta不对过去的梯度平方进行累加，用指数平均的方法计算二阶动量，避免了二阶动量持续累积，导致训练提前结束。\sphinxstylestrong{Adam}可以理解为Adadelta和Momentum的结合，对一阶二阶动量均采用指数平均的方法计算。

\sphinxAtStartPar
MindSpore提供了丰富的API来让用户导入损失函数和优化器。在下面的例子中，计算了输入和真实值之间的softmax交叉熵损失，导入Momentum优化器。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} 定义损失函数}
\PYG{n}{net\PYGZus{}loss} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{SoftmaxCrossEntropyWithLogits}\PYG{p}{(}\PYG{n}{sparse}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{reduction}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} 定义优化器}
\PYG{n}{net\PYGZus{}opt} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{Momentum}\PYG{p}{(}\PYG{n}{net}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}params}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{n}{momentum}\PYG{o}{=}\PYG{l+m+mf}{0.9}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{训练及保存模型}
\label{\detokenize{chapter_programming_interface/ml_workflow:id6}}
\sphinxAtStartPar
MindSpore提供了回调Callback机制，可以在训练过程中执行自定义逻辑，使用框架提供的ModelCheckpoint为例。ModelCheckpoint可以保存网络模型和参数，以便进行后续的Fine\sphinxhyphen{}tuning（微调）操作。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} 导入模型保存模块}
\PYG{k+kn}{from} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{train}\PYG{n+nn}{.}\PYG{n+nn}{callback} \PYG{k+kn}{import} \PYG{n}{ModelCheckpoint}\PYG{p}{,} \PYG{n}{CheckpointConfig}
\PYG{c+c1}{\PYGZsh{} 设置模型保存参数}
\PYG{n}{config\PYGZus{}ck} \PYG{o}{=} \PYG{n}{CheckpointConfig}\PYG{p}{(}\PYG{n}{save\PYGZus{}checkpoint\PYGZus{}steps}\PYG{o}{=}\PYG{l+m+mi}{1875}\PYG{p}{,} \PYG{n}{keep\PYGZus{}checkpoint\PYGZus{}max}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} 应用模型保存参数}
\PYG{n}{ckpoint} \PYG{o}{=} \PYG{n}{ModelCheckpoint}\PYG{p}{(}\PYG{n}{prefix}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{checkpoint\PYGZus{}lenet}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{config}\PYG{o}{=}\PYG{n}{config\PYGZus{}ck}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
通过MindSpore提供的model.train接口可以方便地进行网络的训练，LossMonitor可以监控训练过程中loss值的变化。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} 导入模型训练需要的库}
\PYG{k+kn}{from} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{nn} \PYG{k+kn}{import} \PYG{n}{Accuracy}
\PYG{k+kn}{from} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{train}\PYG{n+nn}{.}\PYG{n+nn}{callback} \PYG{k+kn}{import} \PYG{n}{LossMonitor}
\PYG{k+kn}{from} \PYG{n+nn}{mindspore} \PYG{k+kn}{import} \PYG{n}{Model}

\PYG{k}{def} \PYG{n+nf}{train\PYGZus{}net}\PYG{p}{(}\PYG{n}{args}\PYG{p}{,} \PYG{n}{model}\PYG{p}{,} \PYG{n}{epoch\PYGZus{}size}\PYG{p}{,} \PYG{n}{data\PYGZus{}path}\PYG{p}{,} \PYG{n}{repeat\PYGZus{}size}\PYG{p}{,} \PYG{n}{ckpoint\PYGZus{}cb}\PYG{p}{,} \PYG{n}{sink\PYGZus{}mode}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}定义训练的方法\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{c+c1}{\PYGZsh{} 加载训练数据集}
    \PYG{n}{ds\PYGZus{}train} \PYG{o}{=} \PYG{n}{create\PYGZus{}dataset}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{data\PYGZus{}path}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{train}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{32}\PYG{p}{,} \PYG{n}{repeat\PYGZus{}size}\PYG{p}{)}
    \PYG{n}{model}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{epoch\PYGZus{}size}\PYG{p}{,} \PYG{n}{ds\PYGZus{}train}\PYG{p}{,} \PYG{n}{callbacks}\PYG{o}{=}\PYG{p}{[}\PYG{n}{ckpoint\PYGZus{}cb}\PYG{p}{,} \PYG{n}{LossMonitor}\PYG{p}{(}\PYG{l+m+mi}{125}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dataset\PYGZus{}sink\PYGZus{}mode}\PYG{o}{=}\PYG{n}{sink\PYGZus{}mode}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
其中，dataset\_sink\_mode用于控制数据是否下沉，数据下沉是指数据通过通道直接传送到Device上，可以加快训练速度，dataset\_sink\_mode为True表示数据下沉，否则为非下沉。

\sphinxAtStartPar
有了数据集、模型、损失函数、优化器后就可以进行训练了，这里把train\_epoch设置为1，对数据集进行1个迭代的训练。在train\_net和
test\_net方法中，我们加载了之前下载的训练数据集，mnist\_path是MNIST数据集路径。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{train\PYGZus{}epoch} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{mnist\PYGZus{}path} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{./datasets/MNIST\PYGZus{}Data}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{dataset\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{Model}\PYG{p}{(}\PYG{n}{net}\PYG{p}{,} \PYG{n}{net\PYGZus{}loss}\PYG{p}{,} \PYG{n}{net\PYGZus{}opt}\PYG{p}{,} \PYG{n}{metrics}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Accuracy}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{Accuracy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{train\PYGZus{}net}\PYG{p}{(}\PYG{n}{args}\PYG{p}{,} \PYG{n}{model}\PYG{p}{,} \PYG{n}{train\PYGZus{}epoch}\PYG{p}{,} \PYG{n}{mnist\PYGZus{}path}\PYG{p}{,} \PYG{n}{dataset\PYGZus{}size}\PYG{p}{,} \PYG{n}{ckpoint}\PYG{p}{,} \PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{测试和验证}
\label{\detokenize{chapter_programming_interface/ml_workflow:id7}}
\sphinxAtStartPar
测试是模型运行测试数据集得到的结果，通常在训练过程中，每训练一定的数据量后就会测试一次，以验证模型的泛化能力。MindSpore使用model.eval接口读入测试数据集。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{test\PYGZus{}net}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,} \PYG{n}{data\PYGZus{}path}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}定义验证的方法\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{ds\PYGZus{}eval} \PYG{o}{=} \PYG{n}{create\PYGZus{}dataset}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{data\PYGZus{}path}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{test}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{acc} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{eval}\PYG{p}{(}\PYG{n}{ds\PYGZus{}eval}\PYG{p}{,} \PYG{n}{dataset\PYGZus{}sink\PYGZus{}mode}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{acc}\PYG{p}{)}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} 验证模型精度}
\PYG{n}{test\PYGZus{}net}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,} \PYG{n}{mnist\PYGZus{}path}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
在训练完毕后，参数保存在checkpoint中，可以将训练好的参数加载到模型中进行验证。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{mindspore} \PYG{k+kn}{import} \PYG{n}{Tensor}
\PYG{k+kn}{from} \PYG{n+nn}{mindspore} \PYG{k+kn}{import} \PYG{n}{load\PYGZus{}checkpoint}\PYG{p}{,} \PYG{n}{load\PYGZus{}param\PYGZus{}into\PYGZus{}net}
\PYG{c+c1}{\PYGZsh{} 定义测试数据集，batch\PYGZus{}size设置为1，则取出一张图片}
\PYG{n}{ds\PYGZus{}test} \PYG{o}{=} \PYG{n}{create\PYGZus{}dataset}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{mnist\PYGZus{}path}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{test}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{create\PYGZus{}dict\PYGZus{}iterator}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{ds\PYGZus{}test}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} images为测试图片，labels为测试图片的实际分类}
\PYG{n}{images} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{image}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{asnumpy}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{labels} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{label}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{asnumpy}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} 加载已经保存的用于测试的模型}
\PYG{n}{param\PYGZus{}dict} \PYG{o}{=} \PYG{n}{load\PYGZus{}checkpoint}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{checkpoint\PYGZus{}lenet\PYGZhy{}1\PYGZus{}1875.ckpt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} 加载参数到网络中}
\PYG{n}{load\PYGZus{}param\PYGZus{}into\PYGZus{}net}\PYG{p}{(}\PYG{n}{net}\PYG{p}{,} \PYG{n}{param\PYGZus{}dict}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} 使用函数model.predict预测image对应分类}
\PYG{n}{output} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{Tensor}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{image}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} 输出预测分类与实际分类}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predicted: }\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{predicted}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{, Actual: }\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{labels}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\section{定义深度神经网络}
\label{\detokenize{chapter_programming_interface/neural_network_layer:id1}}\label{\detokenize{chapter_programming_interface/neural_network_layer::doc}}
\sphinxAtStartPar
在上一节我们使用MindSpore构建了一个多层感知机的网络结构，随着深度神经网络的飞速发展，各种深度神经网络结构层出不穷，但是不管结构如何复杂，神经网络层数量如何增加，构建深度神经网络结构始终遵循最基本的规则：1.承载计算的节点；2.可变化的节点权重（节点权重可训练）；3.允许数据流动的节点连接。因此在机器学习编程库中神经网络是以层为核心，它提供了各类神经网络层基本组件；将神经网络层组件按照网络结构进行堆叠、连接就能构造出神经网络模型。


\subsection{以层为核心定义神经网络}
\label{\detokenize{chapter_programming_interface/neural_network_layer:id2}}
\sphinxAtStartPar
神经网络层包含构建机器学习网络结构的基本组件，如计算机视觉领域常用到卷积(Convolution)、池化(Pooling)、全连接(Fully
Connected)；自然语言处理常用到循环神经网络(Recurrent Neural
Network，RNN)；为了加速训练，防止过拟合通常用到批标准化（BatchNorm）、Dropout等。

\sphinxAtStartPar
\sphinxstylestrong{全连接}是将当前层每个节点都和上一层节点一一连接，本质上是特征空间的线性变换；可以将数据从高维映射到低维，也能从低维映射到高维度。
\hyperref[\detokenize{chapter_programming_interface/neural_network_layer:fc-layer}]{图\ref{\detokenize{chapter_programming_interface/neural_network_layer:fc-layer}}}展示了全连接的过程，对输入的n个数据变换到另一个大小为m的特征空间，再从大小为m的特征空间变换到大小为p的特征空间；可见全连接层的参数量巨大，两次变换所需的参数大小为\(n \times m\)和\(m \times p\)。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{fc_layer_1}.svg}
\caption{全连接层}\label{\detokenize{chapter_programming_interface/neural_network_layer:id6}}\label{\detokenize{chapter_programming_interface/neural_network_layer:fc-layer}}\end{figure}

\sphinxAtStartPar
\sphinxstylestrong{卷积}操作是卷积神经网络中常用的操作之一，卷积相当于对输入进行滑动滤波。根据卷积核（Kernel）、卷积步长（Stride）、填充（Padding）对输入数据从左到右，从上到下进行滑动，每一次滑动操作是矩阵的乘加运算得到的加权值。
如
\hyperref[\detokenize{chapter_programming_interface/neural_network_layer:conv-comp}]{图\ref{\detokenize{chapter_programming_interface/neural_network_layer:conv-comp}}}卷积操作主要由输入、卷积核、输出组成输出又被称为特征图（Feature
Map）。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{conv_component}.svg}
\caption{卷积操作的组成}\label{\detokenize{chapter_programming_interface/neural_network_layer:id7}}\label{\detokenize{chapter_programming_interface/neural_network_layer:conv-comp}}\end{figure}

\sphinxAtStartPar
卷积的具体运算过程我们通过
\hyperref[\detokenize{chapter_programming_interface/neural_network_layer:single-conv}]{图\ref{\detokenize{chapter_programming_interface/neural_network_layer:single-conv}}}进行演示。该图输入为\(4 \times 4\)的矩阵，卷积核大小为\(3 \times 3\)，卷积步长为1，不填充，最终得到的\(2 \times 2\)的输出矩阵。
计算过程为将\(3 \times 3\)的卷积核作用到左上角\(3 \times 3\)大小的输入图上；输出为\(1 \times 1 + 2 \times 0 + 2 \times 1 + 3 \times 0 + 2 \times 1 + 3 \times 0 + 4 \times 1 + 1 \times 0 + 3 \times 1 = 12\),
同理对卷积核移动1个步长再次执行相同的计算步骤得到第二个输出为11；当再次移动将出界时结束从左往右，执行从上往下移动1步，再进行从左往右移动；依次操作直到从上往下再移动也出界时，结束整个卷积过程，得到输出结果。我们不难发现相比于全连接，卷积的优势是参数共享（同一个卷积核遍历整个输入图）和参数量小（卷积核大小即是参数量）。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{single_channel_conv}.svg}
\caption{卷积的具体运算过程}\label{\detokenize{chapter_programming_interface/neural_network_layer:id8}}\label{\detokenize{chapter_programming_interface/neural_network_layer:single-conv}}\end{figure}

\sphinxAtStartPar
在卷积过程中，如果我们需要对输出矩阵大小进行控制，那么就需要对步长和填充进行设置。还是上面的输入图，如需要得到和输入矩阵大小一样的输出矩阵，步长为1时就需要对上下左右均填充一圈全为0的数。

\sphinxAtStartPar
在上述例子中我们介绍了一个输入一个卷积核的卷积操作。通常情况下我们输入的是彩色图片，有三个输入，这三个输入称为通道（Channel），分别代表红、绿、蓝（RGB）。此时我们执行卷积则为多通道卷积，需要三个卷积核分别对RGB三个通道进行上述卷积过程，之后将结果加起来。
具体如
\hyperref[\detokenize{chapter_programming_interface/neural_network_layer:channels-conv}]{图\ref{\detokenize{chapter_programming_interface/neural_network_layer:channels-conv}}}描述了一个输入通道为3，输出通道为1，卷积核大小为\(3 \times 3\)，卷积步长为1的多通道卷积过程；需要注意的是，每个通道都有各自的卷积核，同一个通道的卷积核参数共享。如果输出通道为\(out_c\)，输入通道为\(in_c\)，那么需要\(out_c\)\(\times\)\(in_c\)个卷积核。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{channels_conv}.svg}
\caption{多通道卷积}\label{\detokenize{chapter_programming_interface/neural_network_layer:id9}}\label{\detokenize{chapter_programming_interface/neural_network_layer:channels-conv}}\end{figure}

\sphinxAtStartPar
\sphinxstylestrong{池化}是常见的降维操作，有最大池化和平均池化。池化操作和卷积的执行类似，通过池化核、步长、填充决定输出；最大池化是在池化核区域范围内取最大值，平均池化则是在池化核范围内做平均。与卷积不同的是池化核没有训练参数；池化层的填充方式也有所不同，平均池化填充的是0，最大池化填充的是\(-inf\)。
\hyperref[\detokenize{chapter_programming_interface/neural_network_layer:pooling}]{图\ref{\detokenize{chapter_programming_interface/neural_network_layer:pooling}}}是对\(4 \times 4\)的输入进行\(2 \times 2\)区域池化，步长为2，不填充；图左边是最大池化的结果，右边是平均池化的结果。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{pooling}.svg}
\caption{池化操作}\label{\detokenize{chapter_programming_interface/neural_network_layer:id10}}\label{\detokenize{chapter_programming_interface/neural_network_layer:pooling}}\end{figure}

\sphinxAtStartPar
有了卷积、池化、全连接组件就可以构建一个非常简单的卷积神经网络了，
\hyperref[\detokenize{chapter_programming_interface/neural_network_layer:nn-network}]{图\ref{\detokenize{chapter_programming_interface/neural_network_layer:nn-network}}}展示了一个卷积神经网络的模型结构。
给定输入\(3 \times 64 \times 64\)的彩色图片，使用16个\(3 \times 3 \times 3\)大小的卷积核做卷积，得到大小为\(16 \times 64 \times 64\)的特征图；
再进行池化操作降维，得到大小为\(16 \times 32 \times 32\)的特征图；
对特征图再卷积得到大小为\(32 \times 32 \times 32\)特征图，再进行池化操作得到\(32 \times 16 \times 16\)大小的特征图；
我们需要对特征图做全连接，此时需要把特征图平铺成一维向量这步操作称为Flatten，压平后输入特征大小为\(32\times 16 \times 16 = 8192\)；
之后做一次全连接对大小为8192特征变换到大小为128的特征，再依次做两次全连接分别得到64，10。
这里最后的输出结果是依据自己的实际问题而定，假设我们的输入是包含\(0 \sim 9\)的数字图片，做分类那输出对应是10个概率值，分别对应\(0 \sim 9\)的概率大小。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{nn_network}.svg}
\caption{卷积神经网络模型}\label{\detokenize{chapter_programming_interface/neural_network_layer:id11}}\label{\detokenize{chapter_programming_interface/neural_network_layer:nn-network}}\end{figure}

\sphinxAtStartPar
有了上述基础知识，我们对卷积神经网络所需组件接口和模型构建使用伪代码描述如下：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} 构建卷积神经网络的组件接口定义：
全连接层接口：fully\PYGZus{}connected(input, weights)
卷积层的接口：convolution(input, filters, stride, padding)
最大池化接口：pooling(input, pool\PYGZus{}size, stride, padding, mode=\PYGZsq{}max\PYGZsq{})
平均池化接口：pooling(input, pool\PYGZus{}size, stride, padding, mode=\PYGZsq{}mean\PYGZsq{})

\PYGZsh{} 构建卷积神经网络描述：
input:(3,64,64)大小的图片
\PYGZsh{} 创建卷积模型的训练变量,使用随机数初始化变量值
conv1\PYGZus{}filters = variable(random(size=(3, 3, 3, 16)))
conv2\PYGZus{}filters = variable(random(size=(3, 3, 16, 32)))
fc1\PYGZus{}weights = variable(random(size=(8192, 128)))
fc2\PYGZus{}weights = variable(random(size=(128, 64)))
fc3\PYGZus{}weights = variable(random(size=(64, 10)))
\PYGZsh{} 将所有需要训练的参数收集起来
all\PYGZus{}weights = [conv1\PYGZus{}filters, conv2\PYGZus{}filters, fc1\PYGZus{}weights, fc2\PYGZus{}weights, fc3\PYGZus{}weights]

\PYGZsh{} 构建卷积模型的连接过程
output = convolution(input, conv1\PYGZus{}filters, stride=1, padding=\PYGZsq{}same\PYGZsq{})
output = pooling(output, kernel\PYGZus{}size=3, stride=2, padding=\PYGZsq{}same\PYGZsq{}, mode=\PYGZsq{}max\PYGZsq{})
output = convolution(output, conv2\PYGZus{}filters, stride=1, padding=\PYGZsq{}same\PYGZsq{})
output = pooling(output, kernel\PYGZus{}size=3, stride=2, padding=\PYGZsq{}same\PYGZsq{}, mode=\PYGZsq{}max\PYGZsq{})
output = flatten(output)
output = fully\PYGZus{}connected(output, fc1\PYGZus{}weights)
output = fully\PYGZus{}connected(output, fc2\PYGZus{}weights)
output = fully\PYGZus{}connected(output, fc3\PYGZus{}weights)
\end{sphinxVerbatim}

\sphinxAtStartPar
随着深度神经网络应用领域的扩大，诞生出了丰富的模型构建组件。在卷积神经网络的计算过程中，前后的输入是没有联系的，然而在很多任务中往往需要处理序列信息，如语句、语音、视频等，为了解决此类问题诞生出循环神经网络（Recurrent
Neural Network，RNN）；
循环神经网络很好的解决了序列数据的问题，但是随着序列的增加，长序列又导致了训练过程中梯度消失和梯度爆炸的问题，因此有了长短期记忆（Long
Short\sphinxhyphen{}term Memory，LSTM）；
在语言任务中还有Seq2Seq它将RNN当成编解码（Encoder\sphinxhyphen{}Decoder）结构的编码器（Encoder）和解码器（Decode）；
在解码器中又常常使用注意力机制（Attention）；基于编解码器和注意力机制又有Transformer；
Transformer又是BERT模型架构的重要组成。随着深度神经网络的发展，未来也会诞生各类模型架构，架构的创新可以通过各类神经网络基本组件的组合来实现。


\subsection{神经网络层的实现原理}
\label{\detokenize{chapter_programming_interface/neural_network_layer:id3}}
\sphinxAtStartPar
2.3.1中使用伪代码定义了一些卷积神经网络接口和模型构建过程，整个构建过程，需要创建训练变量和构建连接过程；
随着网络层数的增加，手动管理训练变量是一个繁琐的过程，因此2.3.1中描述的接口在机器学习库中属于低级API。
机器学习编程库大都提供了更高级用户友好的API，它将神经网络层抽象成一个基类，所有的神经网络层实现都继承基类调用低级API。
如MindSpore提供的mindspore.nn.Cell、mindspore.nn.Conv2d、mindspore.dataset；
PyTorch提供的torch.nn.Module、torch.nn.Conv2d、torch.utils.data.Dataset。

\sphinxAtStartPar
\hyperref[\detokenize{chapter_programming_interface/neural_network_layer:model-build}]{图\ref{\detokenize{chapter_programming_interface/neural_network_layer:model-build}}}描述了神经网络构建过程中的基本细节。
神经网络层需要的功能有该层的训练参数（变量，包括初始化方法和训练状态）以及计算过程；
神经网络模型需要的功能是对神经网络层管理和神经网络层参数的管理。
在机器学习编程库中，承担此功能有MindSpore的Cell、PyTorch的Module。
Cell和Module是模型抽象方法也是所有网络的基类。 现有模型抽象方案有两种。
一种是抽象出两个方法分别为Layer（负责单个神经网络层的参数构建和前向计算），Model（负责对神经网络层进行连接组合和神经网络层参数管理）；
另一种是将Layer和Model抽象成一个方法，该方法既能表示单层神经网络层也能表示包含多个神经网络层堆叠的模型，Cell和Module就是这样实现的。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{model_build}.svg}
\caption{神经网络模型构建细节}\label{\detokenize{chapter_programming_interface/neural_network_layer:id12}}\label{\detokenize{chapter_programming_interface/neural_network_layer:model-build}}\end{figure}

\sphinxAtStartPar
\hyperref[\detokenize{chapter_programming_interface/neural_network_layer:cell-abs}]{图\ref{\detokenize{chapter_programming_interface/neural_network_layer:cell-abs}}}展示了设计神经网络层抽象方法的通用表示。通常在构造器会选择使用Python中collections模块的OrderedDict来初始化神经网络层和神经网络层参数的存储；它的输出是一个有序的，相比与Dict更适合深度学习这种模型堆叠的模式。参数和神经网络层的管理是在\_\_setattr\_\_中实现的，当检测到属性是属于神经网络层及神经网络层参数时就记录起来。神经网络模型比较重要的是计算连接过程，可以在\_\_call\_\_里重载，实现神经网络层时在这里定义计算过程。训练参数的返回接口是为了给优化器传所有训练参数。神经网络层返回为了遍历各层神经网络得到各个神经网络层的参数。这里只列出了一些重要的方法，在自定义方法中，通常需要实现参数插入删除方法、神经网络层插入删除、神经网络模型信息等。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{cell_abstract}.svg}
\caption{神经网络基类抽象方法}\label{\detokenize{chapter_programming_interface/neural_network_layer:id13}}\label{\detokenize{chapter_programming_interface/neural_network_layer:cell-abs}}\end{figure}

\sphinxAtStartPar
神经网络接口层基类实现，仅做了简化的描述，在实际实现时，执行计算的\_\_call\_\_方法并不会让用户直接重载，它往往在\_\_call\_\_之外定义一个执行操作的方法（对于神经网络模型该方法是实现网络结构的连接，对于神经网络层则是实现计算过程）后再\_\_call\_\_调用；如MindSpore的Cell因为动态图和静态图的执行是不一样的，因此在\_\_call\_\_里定义动态图和计算图的计算执行，在construct方法里定义层或者模型的操作过程。


\subsection{自定义神经网络层}
\label{\detokenize{chapter_programming_interface/neural_network_layer:id4}}
\sphinxAtStartPar
2.3.1中使用伪代码定义机器学习库中低级API，有了实现的神经网络基类抽象方法，那么就可以设计更高层次的接口解决手动管理参数的繁琐。假设已经有了神经网络模型抽象方法Cell，构建Conv2D将继承Cell，并重构\_\_init\_\_和\_\_call\_\_方法，在\_\_init\_\_里初始化训练参数和输入参数，在\_\_call\_\_里调用低级API实现计算逻辑。同样使用伪代码接口描述自定义卷积层的过程。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} 接口定义：
卷积层的接口：convolution(input, filters, stride, padding)
变量：Variable(value, trainable=True)
高斯分布初始化方法：random\PYGZus{}normal(shape)
神经网络模型抽象方法：Cell

\PYGZsh{} 定义卷积层
class Conv2D(Cell):
    def \PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}(self, in\PYGZus{}channels, out\PYGZus{}channels, ksize, stride, padding):
        \PYGZsh{} 卷积核大小为 ksize x ksize x inchannels x out\PYGZus{}channels
        filters\PYGZus{}shape = (out\PYGZus{}channels, in\PYGZus{}channels, ksize, ksize)
        self.stride = stride
        self.padding = padding
        self.filters = Variable(random\PYGZus{}normal(filters\PYGZus{}shape))

    def \PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}(self, inputs):
        outputs = convolution(inputs, self.filters, self.stride, self.padding)
\end{sphinxVerbatim}

\sphinxAtStartPar
有了上述定义在使用卷积层时，就不需要创建训练变量了。
如我们需要对\(30 \times 30\)大小10个通道的输入使用\(3 \times 3\)的卷积核做卷积，卷积后输出通道为20。
调用方式如下：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{conv} \PYG{o}{=} \PYG{n}{Conv2D}\PYG{p}{(}\PYG{n}{in\PYGZus{}channel}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{out\PYGZus{}channel}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{filter\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{stride}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{padding}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{output} \PYG{o}{=} \PYG{n}{conv}\PYG{p}{(}\PYG{n+nb}{input}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
其执行过程为，在初始化Conv2D时，\_\_setattr\_\_会判断属性，属于Cell把神经网络层Conv2D记录到self.\_cells，filters属于parameter把参数记录到self.\_params。查看神经网络层参数使用conv.parameters\_and\_names；查看神经网络层列表使用conv.cells\_and\_names；执行操作使用conv(input)。


\subsection{自定义神经网络模型}
\label{\detokenize{chapter_programming_interface/neural_network_layer:id5}}
\sphinxAtStartPar
神经网络层是Cell的子类（SubClass）实现，同样的神经网络模型也可以采用SubClass的方法自定义神经网络模型；构建时需要在\_\_init\_\_里将要使用的神经网络组件实例化，在\_\_call\_\_里定义神经网络的计算逻辑。同样的以2.3.1的卷积神经网络模型为例，定义接口和伪代码描述如下：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} 使用Cell子类构建的神经网络层接口定义：
\PYGZsh{} 构建卷积神经网络的组件接口定义：
全连接层接口：Dense(in\PYGZus{}channel, out\PYGZus{}channel)
卷积层的接口：Conv2D(in\PYGZus{}channel, out\PYGZus{}channel, filter\PYGZus{}size, stride, padding)
最大池化接口：MaxPool2D(pool\PYGZus{}size, stride, padding)
张量平铺：Flatten()

\PYGZsh{} 使用SubClass方式构建卷积模型
class CNN(Cell):
    def \PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}(self):
        self.conv1 = Conv2D(in\PYGZus{}channel=3, out\PYGZus{}channel=16, filter\PYGZus{}size=3, stride=1, padding=0)
        self.maxpool1 = MaxPool2D(pool\PYGZus{}size=3, stride=1, padding=0)
        self.conv2 = Conv2D(in\PYGZus{}channel=16, out\PYGZus{}channel=32, filter\PYGZus{}size=3, stride=1, padding=0)
        self.maxpool2 = MaxPool2D(pool\PYGZus{}size=3, stride=1, padding=0)
        self.flatten = Flatten()
        self.dense1 = Dense(in\PYGZus{}channels=768, out\PYGZus{}channel=128)
        self.dense2 = Dense(in\PYGZus{}channels=128, out\PYGZus{}channel=64)
        self.dense3 = Dense(in\PYGZus{}channels=64, out\PYGZus{}channel=10)

    def \PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}(self, inputs):
        z = self.conv1(inputs)
        z = self.maxpool1(z)
        z = self.conv2(z)
        z = self.maxpool2(z)
        z = self.flatten(z)
        z = self.dense1(z)
        z = self.dense2(z)
        z = self.dense3(z)
        return z
net = CNN()
\end{sphinxVerbatim}

\sphinxAtStartPar
上述卷积模型进行实例化，其执行将从\_\_init\_\_开始，第一个是Conv2D，Conv2D也是Cell的子类，会进入到Conv2D的\_\_init\_\_，此时会将第一个Conv2D的卷积参数收集到self.\_params，之后回到Conv2D，将第一个Conv2D收集到self.\_cells；第二个的组件是MaxPool2D，因为其没有训练参数，因此将MaxPool2D收集到self.\_cells；依次类推，分别收集第二个卷积参数和卷积层，三个全连接层的参数和全连接层。实例化之后可以调用net.parameters\_and\_names来返回训练参数；调用net.cells\_and\_names查看神经网络层列表。


\section{C/C++编程接口}
\label{\detokenize{chapter_programming_interface/c_python_interaction:c-c}}\label{\detokenize{chapter_programming_interface/c_python_interaction::doc}}
\sphinxAtStartPar
在上述小节中，我们讨论了开发者如何利用Python来定义机器学习的整个工作流，以及如何定义复杂的深度神经网络。然而，在很多时候，用户也需要添加自定义的算子来帮助实现新的模型，优化器，数据处理函数等。这些自定义算子需要通过C和C++实现，从而获得最优性能。但是为了帮助这些算子被用户使用，他们也需要暴露为Python函数，从而方便用户整合入已有的Python为核心编写的工作流和模型。在这一小节中，我们讨论这一过程是如何实现的。


\subsection{在Python中调用C/C++函数的原理}
\label{\detokenize{chapter_programming_interface/c_python_interaction:pythonc-c}}
\sphinxAtStartPar
由于Python的解释器是由C实现的，因此在Python中可以实现对于C和C++函数的调用。现代机器学习框架（包括TensorFlow，PyTorch和MindSpore）主要依赖Pybind11来将底层的大量C和C++函数自动生成对应的Python函数，这一过程一般被称为Python绑定（
Binding）。在Pybind11出现以前，将C和C++函数进行Python绑定的手段主要包括：
\begin{itemize}
\item {} 
\sphinxAtStartPar
Python的C\sphinxhyphen{}API。这种方式要求在一个C++程序中包含Python.h，并使用Python的C\sphinxhyphen{}API对Python语言进行操作。使用这套API需要对Python的底层实现有一定了解，比如如何管理引用计数等，具有较高的使用门槛。

\item {} 
\sphinxAtStartPar
简单包装界面产生器（Simplified Wrapper and Interface
Generator，SWIG)。SWIG可以将C和C++代码暴露给Python。SWIG是TensorFlow早期使用的方式。这种方式需要用户编写一个复杂的SWIG接口声明文件，并使用SWIG自动生成使用Python
C\sphinxhyphen{}API的C代码。自动生成的代码可读性很低，因此具有很大代码维护开销。

\item {} 
\sphinxAtStartPar
Python的ctypes模块，提供了C语言中的类型，以及直接调用动态链接库的能力。缺点是依赖于C的原生的类型，对自定义类型支持不好。

\item {} 
\sphinxAtStartPar
Cython是结合了Python和C语言的一种语言，可以简单的认为就是给Python加上了静态类型后的语法，使用者可以维持大部分的Python语法。Cython编写的函数会被自动转译为C和C++代码，因此在Cython中可以插入对于C/C++函数的调用。

\item {} 
\sphinxAtStartPar
Boost::Python是一个C++库。它可以将C++函数暴露为Python函数。其原理和Python
C\sphinxhyphen{}API类似，但是使用方法更简单。然而，由于引入了Boost库，因此有沉重的第三方依赖。

\end{itemize}

\sphinxAtStartPar
相对于上述的提供Python绑定的手段，Pybind11提供了类似于Boost::Python的简洁性和易用性，但是其通过专注支持C++
11，并且去除Boost依赖，因此成为了轻量级的Python库，从而特别适合在一个复杂的C++项目（例如本书讨论的机器学习系统）中暴露大量的Python函数。


\subsection{添加C++编写的自定义算子}
\label{\detokenize{chapter_programming_interface/c_python_interaction:c}}
\sphinxAtStartPar
算子是构建神经网络的基础，在前面也称为低级API；通过算子的封装可以实现各类神经网络层，当开发神经网络层遇到内置算子无法满足时，可以通过自定义算子来实现。以MindSpore为例，实现一个GPU算子需要如下步骤：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Primitive注册：算子原语是构建网络模型的基础单元，用户可以直接或者间接调用算子原语搭建一个神经网络模型。

\item {} 
\sphinxAtStartPar
GPU Kernel实现：GPU Kernel用于调用GPU实现加速计算。

\item {} 
\sphinxAtStartPar
GPU Kernel注册：算子注册用于将GPU
Kernel及必要信息注册给框架，由框架完成对GPU Kernel的调用。

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{1.注册算子原语}
算子原语通常包括算子名、算子输入、算子属性（初始化时需要填的参数，如卷积的stride、padding）、输入数据合法性校验、输出数据类型推导和维度推导。假设需要编写加法算子，主要内容如下：
\begin{itemize}
\item {} 
\sphinxAtStartPar
算子名：TensorAdd

\item {} 
\sphinxAtStartPar
算子属性：构造函数\_\_init\_\_中初始化属性，因加法没有属性，因此\_\_init\_\_不需要额外输入。

\item {} 
\sphinxAtStartPar
算子输入输出及合法性校验：infer\_shape方法中约束两个输入维度必须相同，输出的维度和输入维度相同。infer\_dtype方法中约束两个输入数据必须是float32类型，输出的数据类型和输入数据类型相同。

\item {} 
\sphinxAtStartPar
算子输出

\end{itemize}

\sphinxAtStartPar
MindSpore中实现注册TensorAdd代码如下：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} mindspore/ops/operations/math\PYGZus{}ops.py}
\PYG{k}{class} \PYG{n+nc}{TensorAdd}\PYG{p}{(}\PYG{n}{PrimitiveWithInfer}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Adds two input tensors element\PYGZhy{}wise.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n+nd}{@prim\PYGZus{}attr\PYGZus{}register}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{init\PYGZus{}prim\PYGZus{}io\PYGZus{}names}\PYG{p}{(}\PYG{n}{inputs}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{outputs}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{infer\PYGZus{}shape}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x1\PYGZus{}shape}\PYG{p}{,} \PYG{n}{x2\PYGZus{}shape}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{validator}\PYG{o}{.}\PYG{n}{check\PYGZus{}integer}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{input dims}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{x1\PYGZus{}shape}\PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{x2\PYGZus{}shape}\PYG{p}{)}\PYG{p}{,} \PYG{n}{Rel}\PYG{o}{.}\PYG{n}{EQ}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{name}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{x1\PYGZus{}shape}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{validator}\PYG{o}{.}\PYG{n}{check\PYGZus{}integer}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{input\PYGZus{}shape}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{x1\PYGZus{}shape}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{x2\PYGZus{}shape}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{Rel}\PYG{o}{.}\PYG{n}{EQ}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{name}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{x1\PYGZus{}shape}

    \PYG{k}{def} \PYG{n+nf}{infer\PYGZus{}dtype}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x1\PYGZus{}dtype}\PYG{p}{,} \PYG{n}{x2\PYGZus{}type}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{validator}\PYG{o}{.}\PYG{n}{check\PYGZus{}tensor\PYGZus{}type\PYGZus{}same}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x1\PYGZus{}dtype}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{x1\PYGZus{}dtype}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{p}{[}\PYG{n}{mstype}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{]}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{name}\PYG{p}{)}
        \PYG{n}{validator}\PYG{o}{.}\PYG{n}{check\PYGZus{}tensor\PYGZus{}type\PYGZus{}same}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x2\PYGZus{}dtype}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{x2\PYGZus{}dtype}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{p}{[}\PYG{n}{mstype}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{]}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{name}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{x1\PYGZus{}dtype}
\end{sphinxVerbatim}

\sphinxAtStartPar
在mindspore/ops/operations/math\_ops.py文件内注册加法算子原语后，需要在mindspore/ops/operations/\_\_init\_\_中导出，方便python导入模块时候调用。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} mindspore/ops/operations/\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}.py}
\PYG{k+kn}{from} \PYG{n+nn}{.}\PYG{n+nn}{math\PYGZus{}ops} \PYG{k+kn}{import} \PYG{p}{(}\PYG{n}{Abs}\PYG{p}{,} \PYG{n}{ACos}\PYG{p}{,} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{n}{TensorAdd}\PYG{p}{)}
\PYG{n}{\PYGZus{}\PYGZus{}all\PYGZus{}\PYGZus{}} \PYG{o}{=} \PYG{p}{[}
  \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ReverseSequence}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
  \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CropAndResize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,}
  \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TensorAdd}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{2.GPU算子开发}继承GPUKernel，实现加法使用类模板定义TensorAddGpuKernel，需要实现以下方法：
\begin{itemize}
\item {} 
\sphinxAtStartPar
Init(): 用于完成GPU
Kernel的初始化，通常包括记录算子输入/输出维度，完成Launch前的准备工作；因此在此记录Tensor元素个数。

\item {} 
\sphinxAtStartPar
GetInputSizeList():向框架反馈输入Tensor需要占用的显存字节数；返回了输入Tensor需要占用的字节数，TensorAdd有两个Input，每个Input占用字节数为element\_num\(\ast\)sizeof(T)。

\item {} 
\sphinxAtStartPar
GetOutputSizeList():向框架反馈输出Tensor需要占用的显存字节数；返回了输出Tensor需要占用的字节数，TensorAdd有一个output，占用element\_num\(\ast\)sizeof(T)字节。

\item {} 
\sphinxAtStartPar
GetWorkspaceSizeList():向框架反馈Workspace字节数，Workspace是用于计算过程中存放临时数据的空间；由于TensorAdd不需要Workspace，因此GetWorkspaceSizeList()返回空的std::vector<size\_t>。

\item {} 
\sphinxAtStartPar
Launch(): 通常调用CUDA kernel(CUDA kernel是基于Nvidia
GPU的并行计算架构开发的核函数)，或者cuDNN接口等方式，完成算子在GPU上加速；Launch()接收input、output在显存的地址，接着调用TensorAdd完成加速。

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{/}\PYG{o}{/} \PYG{n}{mindspore}\PYG{o}{/}\PYG{n}{ccsrc}\PYG{o}{/}\PYG{n}{backend}\PYG{o}{/}\PYG{n}{kernel\PYGZus{}compiler}\PYG{o}{/}\PYG{n}{gpu}\PYG{o}{/}\PYG{n}{math}\PYG{o}{/}\PYG{n}{tensor\PYGZus{}add\PYGZus{}v2\PYGZus{}gpu\PYGZus{}kernel}\PYG{o}{.}\PYG{n}{h}

\PYG{n}{template} \PYG{o}{\PYGZlt{}}\PYG{n}{typename} \PYG{n}{T}\PYG{o}{\PYGZgt{}}
\PYG{k}{class} \PYG{n+nc}{TensorAddGpuKernel} \PYG{p}{:} \PYG{n}{public} \PYG{n}{GpuKernel} \PYG{p}{\PYGZob{}}
 \PYG{n}{public}\PYG{p}{:}
  \PYG{n}{TensorAddGpuKernel}\PYG{p}{(}\PYG{p}{)} \PYG{p}{:} \PYG{n}{element\PYGZus{}num\PYGZus{}}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
  \PYG{o}{\PYGZti{}}\PYG{n}{TensorAddGpuKernel}\PYG{p}{(}\PYG{p}{)} \PYG{n}{override} \PYG{o}{=} \PYG{n}{default}\PYG{p}{;}

  \PYG{n+nb}{bool} \PYG{n}{Init}\PYG{p}{(}\PYG{n}{const} \PYG{n}{CNodePtr} \PYG{o}{\PYGZam{}}\PYG{n}{kernel\PYGZus{}node}\PYG{p}{)} \PYG{n}{override} \PYG{p}{\PYGZob{}}
    \PYG{n}{auto} \PYG{n}{shape} \PYG{o}{=} \PYG{n}{AnfAlgo}\PYG{p}{:}\PYG{p}{:}\PYG{n}{GetPrevNodeOutputInferShape}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}node}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{size\PYGZus{}t} \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{shape}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
      \PYG{n}{element\PYGZus{}num\PYGZus{}} \PYG{o}{*}\PYG{o}{=} \PYG{n}{shape}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}
    \PYG{n}{InitSizeLists}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{n}{true}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}

  \PYG{n}{const} \PYG{n}{std}\PYG{p}{:}\PYG{p}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{n}{size\PYGZus{}t}\PYG{o}{\PYGZgt{}} \PYG{o}{\PYGZam{}}\PYG{n}{GetInputSizeList}\PYG{p}{(}\PYG{p}{)} \PYG{n}{const} \PYG{n}{override} \PYG{p}{\PYGZob{}} \PYG{k}{return} \PYG{n}{input\PYGZus{}size\PYGZus{}list\PYGZus{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}
  \PYG{n}{const} \PYG{n}{std}\PYG{p}{:}\PYG{p}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{n}{size\PYGZus{}t}\PYG{o}{\PYGZgt{}} \PYG{o}{\PYGZam{}}\PYG{n}{GetOutputSizeList}\PYG{p}{(}\PYG{p}{)} \PYG{n}{const} \PYG{n}{override} \PYG{p}{\PYGZob{}} \PYG{k}{return} \PYG{n}{output\PYGZus{}size\PYGZus{}list\PYGZus{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}
  \PYG{n}{const} \PYG{n}{std}\PYG{p}{:}\PYG{p}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{n}{size\PYGZus{}t}\PYG{o}{\PYGZgt{}} \PYG{o}{\PYGZam{}}\PYG{n}{GetWorkspaceSizeList}\PYG{p}{(}\PYG{p}{)} \PYG{n}{const} \PYG{n}{override} \PYG{p}{\PYGZob{}} \PYG{k}{return} \PYG{n}{workspace\PYGZus{}size\PYGZus{}list\PYGZus{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}

  \PYG{n+nb}{bool} \PYG{n}{Launch}\PYG{p}{(}\PYG{n}{const} \PYG{n}{std}\PYG{p}{:}\PYG{p}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{n}{AddressPtr}\PYG{o}{\PYGZgt{}} \PYG{o}{\PYGZam{}}\PYG{n}{inputs}\PYG{p}{,} \PYG{n}{const} \PYG{n}{std}\PYG{p}{:}\PYG{p}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{n}{AddressPtr}\PYG{o}{\PYGZgt{}} \PYG{o}{\PYGZam{}}\PYG{p}{,}
              \PYG{n}{const} \PYG{n}{std}\PYG{p}{:}\PYG{p}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{n}{AddressPtr}\PYG{o}{\PYGZgt{}} \PYG{o}{\PYGZam{}}\PYG{n}{outputs}\PYG{p}{,} \PYG{n}{void} \PYG{o}{*}\PYG{n}{stream\PYGZus{}ptr}\PYG{p}{)} \PYG{n}{override} \PYG{p}{\PYGZob{}}
    \PYG{n}{T} \PYG{o}{*}\PYG{n}{x1} \PYG{o}{=} \PYG{n}{GetDeviceAddress}\PYG{o}{\PYGZlt{}}\PYG{n}{T}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{T} \PYG{o}{*}\PYG{n}{x2} \PYG{o}{=} \PYG{n}{GetDeviceAddress}\PYG{o}{\PYGZlt{}}\PYG{n}{T}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{T} \PYG{o}{*}\PYG{n}{y} \PYG{o}{=} \PYG{n}{GetDeviceAddress}\PYG{o}{\PYGZlt{}}\PYG{n}{T}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{n}{outputs}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{TensorAdd}\PYG{p}{(}\PYG{n}{element\PYGZus{}num\PYGZus{}}\PYG{p}{,} \PYG{n}{x1}\PYG{p}{,} \PYG{n}{x2}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{reinterpret\PYGZus{}cast}\PYG{o}{\PYGZlt{}}\PYG{n}{cudaStream\PYGZus{}t}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{n}{stream\PYGZus{}ptr}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{n}{true}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}

 \PYG{n}{protected}\PYG{p}{:}
  \PYG{n}{void} \PYG{n}{InitSizeLists}\PYG{p}{(}\PYG{p}{)} \PYG{n}{override} \PYG{p}{\PYGZob{}}
    \PYG{n}{input\PYGZus{}size\PYGZus{}list\PYGZus{}}\PYG{o}{.}\PYG{n}{push\PYGZus{}back}\PYG{p}{(}\PYG{n}{element\PYGZus{}num\PYGZus{}} \PYG{o}{*} \PYG{n}{sizeof}\PYG{p}{(}\PYG{n}{T}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{input\PYGZus{}size\PYGZus{}list\PYGZus{}}\PYG{o}{.}\PYG{n}{push\PYGZus{}back}\PYG{p}{(}\PYG{n}{element\PYGZus{}num\PYGZus{}} \PYG{o}{*} \PYG{n}{sizeof}\PYG{p}{(}\PYG{n}{T}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{output\PYGZus{}size\PYGZus{}list\PYGZus{}}\PYG{o}{.}\PYG{n}{push\PYGZus{}back}\PYG{p}{(}\PYG{n}{element\PYGZus{}num\PYGZus{}} \PYG{o}{*} \PYG{n}{sizeof}\PYG{p}{(}\PYG{n}{T}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}

 \PYG{n}{private}\PYG{p}{:}
  \PYG{n}{size\PYGZus{}t} \PYG{n}{element\PYGZus{}num\PYGZus{}}\PYG{p}{;}
  \PYG{n}{std}\PYG{p}{:}\PYG{p}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{n}{size\PYGZus{}t}\PYG{o}{\PYGZgt{}} \PYG{n}{input\PYGZus{}size\PYGZus{}list\PYGZus{}}\PYG{p}{;}
  \PYG{n}{std}\PYG{p}{:}\PYG{p}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{n}{size\PYGZus{}t}\PYG{o}{\PYGZgt{}} \PYG{n}{output\PYGZus{}size\PYGZus{}list\PYGZus{}}\PYG{p}{;}
  \PYG{n}{std}\PYG{p}{:}\PYG{p}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{n}{size\PYGZus{}t}\PYG{o}{\PYGZgt{}} \PYG{n}{workspace\PYGZus{}size\PYGZus{}list\PYGZus{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
TensorAdd中调用了CUDA
kernelTensorAddKernel来实现element\_num个元素的并行相加:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{/}\PYG{o}{/} \PYG{n}{mindspore}\PYG{o}{/}\PYG{n}{ccsrc}\PYG{o}{/}\PYG{n}{backend}\PYG{o}{/}\PYG{n}{kernel\PYGZus{}compiler}\PYG{o}{/}\PYG{n}{gpu}\PYG{o}{/}\PYG{n}{math}\PYG{o}{/}\PYG{n}{tensor\PYGZus{}add\PYGZus{}v2\PYGZus{}gpu\PYGZus{}kernel}\PYG{o}{.}\PYG{n}{h}

 \PYG{n}{template} \PYG{o}{\PYGZlt{}}\PYG{n}{typename} \PYG{n}{T}\PYG{o}{\PYGZgt{}}
 \PYG{n}{\PYGZus{}\PYGZus{}global\PYGZus{}\PYGZus{}} \PYG{n}{void} \PYG{n}{TensorAddKernel}\PYG{p}{(}\PYG{n}{const} \PYG{n}{size\PYGZus{}t} \PYG{n}{element\PYGZus{}num}\PYG{p}{,} \PYG{n}{const} \PYG{n}{T}\PYG{o}{*} \PYG{n}{x1}\PYG{p}{,} \PYG{n}{const} \PYG{n}{T}\PYG{o}{*} \PYG{n}{x2}\PYG{p}{,} \PYG{n}{T}\PYG{o}{*} \PYG{n}{y}\PYG{p}{)} \PYG{p}{\PYGZob{}}
  \PYG{k}{for} \PYG{p}{(}\PYG{n}{size\PYGZus{}t} \PYG{n}{i} \PYG{o}{=} \PYG{n}{blockIdx}\PYG{o}{.}\PYG{n}{x} \PYG{o}{*} \PYG{n}{blockDim}\PYG{o}{.}\PYG{n}{x} \PYG{o}{+} \PYG{n}{threadIdx}\PYG{o}{.}\PYG{n}{x}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{element\PYGZus{}num}\PYG{p}{;} \PYG{n}{i} \PYG{o}{+}\PYG{o}{=} \PYG{n}{blockDim}\PYG{o}{.}\PYG{n}{x} \PYG{o}{*} \PYG{n}{gridDim}\PYG{o}{.}\PYG{n}{x}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{y}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{x1}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{+} \PYG{n}{x2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}
 \PYG{p}{\PYGZcb{}}

 \PYG{n}{template} \PYG{o}{\PYGZlt{}}\PYG{n}{typename} \PYG{n}{T}\PYG{o}{\PYGZgt{}}
 \PYG{n}{void} \PYG{n}{TensorAdd}\PYG{p}{(}\PYG{n}{const} \PYG{n}{size\PYGZus{}t} \PYG{o}{\PYGZam{}}\PYG{n}{element\PYGZus{}num}\PYG{p}{,} \PYG{n}{const} \PYG{n}{T}\PYG{o}{*} \PYG{n}{x1}\PYG{p}{,} \PYG{n}{const} \PYG{n}{T}\PYG{o}{*} \PYG{n}{x2}\PYG{p}{,} \PYG{n}{T}\PYG{o}{*} \PYG{n}{y}\PYG{p}{,} \PYG{n}{cudaStream\PYGZus{}t} \PYG{n}{stream}\PYG{p}{)}\PYG{p}{\PYGZob{}}
    \PYG{n}{size\PYGZus{}t} \PYG{n}{thread\PYGZus{}per\PYGZus{}block} \PYG{o}{=} \PYG{l+m+mi}{256}\PYG{p}{;}
    \PYG{n}{size\PYGZus{}t} \PYG{n}{block\PYGZus{}per\PYGZus{}grid} \PYG{o}{=} \PYG{p}{(}\PYG{n}{element\PYGZus{}num} \PYG{o}{+} \PYG{n}{thread\PYGZus{}per\PYGZus{}block} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1} \PYG{p}{)} \PYG{o}{/} \PYG{n}{thread\PYGZus{}per\PYGZus{}block}\PYG{p}{;}
    \PYG{n}{TensorAddKernel}\PYG{o}{\PYGZlt{}\PYGZlt{}}\PYG{o}{\PYGZlt{}}\PYG{n}{block\PYGZus{}per\PYGZus{}grid}\PYG{p}{,} \PYG{n}{thread\PYGZus{}per\PYGZus{}block}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{stream}\PYG{o}{\PYGZgt{}\PYGZgt{}}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{n}{element\PYGZus{}num}\PYG{p}{,} \PYG{n}{x1}\PYG{p}{,} \PYG{n}{x2}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{;}
   \PYG{k}{return}\PYG{p}{;}
 \PYG{p}{\PYGZcb{}}

 \PYG{n}{template} \PYG{n}{void} \PYG{n}{TensorAdd}\PYG{p}{(}\PYG{n}{const} \PYG{n}{size\PYGZus{}t} \PYG{o}{\PYGZam{}}\PYG{n}{element\PYGZus{}num}\PYG{p}{,} \PYG{n}{const} \PYG{n+nb}{float}\PYG{o}{*} \PYG{n}{x1}\PYG{p}{,} \PYG{n}{const} \PYG{n+nb}{float}\PYG{o}{*} \PYG{n}{x2}\PYG{p}{,} \PYG{n+nb}{float}\PYG{o}{*} \PYG{n}{y}\PYG{p}{,} \PYG{n}{cudaStream\PYGZus{}t} \PYG{n}{stream}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{3.GPU算子注册}算子信息包含1.Primive；2.Input dtype, output
dtype；3.GPU Kernel class； 4.CUDA内置数据类型。框架会根据Primive和Input
dtype, output dtype，调用以CUDA内置数据类型实例化GPU Kernel
class模板类。如下代码中分别注册了支持float和int的TensorAdd算子。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{/}\PYG{o}{/} \PYG{n}{mindspore}\PYG{o}{/}\PYG{n}{ccsrc}\PYG{o}{/}\PYG{n}{backend}\PYG{o}{/}\PYG{n}{kernel\PYGZus{}compiler}\PYG{o}{/}\PYG{n}{gpu}\PYG{o}{/}\PYG{n}{math}\PYG{o}{/}\PYG{n}{tensor\PYGZus{}add\PYGZus{}v2\PYGZus{}gpu\PYGZus{}kernel}\PYG{o}{.}\PYG{n}{cc}

\PYG{n}{MS\PYGZus{}REG\PYGZus{}GPU\PYGZus{}KERNEL\PYGZus{}ONE}\PYG{p}{(}\PYG{n}{TensorAddV2}\PYG{p}{,} \PYG{n}{KernelAttr}\PYG{p}{(}\PYG{p}{)}
                                    \PYG{o}{.}\PYG{n}{AddInputAttr}\PYG{p}{(}\PYG{n}{kNumberTypeFloat32}\PYG{p}{)}
                                    \PYG{o}{.}\PYG{n}{AddInputAttr}\PYG{p}{(}\PYG{n}{kNumberTypeFloat32}\PYG{p}{)}
                                    \PYG{o}{.}\PYG{n}{AddOutputAttr}\PYG{p}{(}\PYG{n}{kNumberTypeFloat32}\PYG{p}{)}\PYG{p}{,}
                      \PYG{n}{TensorAddV2GpuKernel}\PYG{p}{,} \PYG{n+nb}{float}\PYG{p}{)}

\PYG{n}{MS\PYGZus{}REG\PYGZus{}GPU\PYGZus{}KERNEL\PYGZus{}ONE}\PYG{p}{(}\PYG{n}{TensorAddV2}\PYG{p}{,} \PYG{n}{KernelAttr}\PYG{p}{(}\PYG{p}{)}
                                    \PYG{o}{.}\PYG{n}{AddInputAttr}\PYG{p}{(}\PYG{n}{kNumberTypeInt32}\PYG{p}{)}
                                    \PYG{o}{.}\PYG{n}{AddInputAttr}\PYG{p}{(}\PYG{n}{kNumberTypeInt32}\PYG{p}{)}
                                    \PYG{o}{.}\PYG{n}{AddOutputAttr}\PYG{p}{(}\PYG{n}{kNumberTypeInt32}\PYG{p}{)}\PYG{p}{,}
                      \PYG{n}{TensorAddV2GpuKernel}\PYG{p}{,} \PYG{n+nb}{int}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
完成上述三步工作后，需要把MindSpore重新编译，在源码的根目录执行bash
build.sh \sphinxhyphen{}e gpu，最后使用算子进行验证。


\section{总结}
\label{\detokenize{chapter_programming_interface/summary:id1}}\label{\detokenize{chapter_programming_interface/summary::doc}}\begin{itemize}
\item {} 
\sphinxAtStartPar
现代机器学习系统需要兼有易用性和高性能，因此其一般选择Python作为前端编程语言，而使用C和C++作为后端编程语言。

\item {} 
\sphinxAtStartPar
一个机器学习框架需要对一个完整的机器学习应用工作流进行编程支持。这些编程支持一般通过提供高层次Python
API来实现。

\item {} 
\sphinxAtStartPar
数据处理编程接口允许用户下载，导入和预处理数据集。

\item {} 
\sphinxAtStartPar
模型定义编程接口允许用户定义和导入机器学习模型。

\item {} 
\sphinxAtStartPar
损失函数接口允许用户定义损失函数来评估当前模型性能。同时，优化器接口允许用户定义和导入优化算法来基于损失函数计算梯度。

\item {} 
\sphinxAtStartPar
机器学习框架同时兼有高层次Python
API来对训练过程，模型测试和调试进行支持。

\item {} 
\sphinxAtStartPar
复杂的深度神经网络可以通过叠加神经网络层来完成。

\item {} 
\sphinxAtStartPar
用户可以通过Python
API定义神经网络层，并指定神经网络层之间的拓扑来定义深度神经网络。

\item {} 
\sphinxAtStartPar
Python和C之间的互操作性一般通过CType等技术实现。

\item {} 
\sphinxAtStartPar
机器学习框架一般具有多种C和C++接口允许用户定义和注册C++实现的算子。这些算子使得用户可以开发高性能模型，数据处理函数，优化器等一系列框架拓展。

\end{itemize}


\section{扩展阅读}
\label{\detokenize{chapter_programming_interface/summary:id2}}\begin{itemize}
\item {} 
\sphinxAtStartPar
MindSpore编程指南：\sphinxhref{https://www.mindspore.cn/docs/programming\_guide/zh-CN/r1.6/index.html}{MindSpore}%
\begin{footnote}[2]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.mindspore.cn/docs/programming\_guide/zh-CN/r1.6/index.html}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
Python和C/C++混合编程：\sphinxhref{https://pybind11.readthedocs.io/en/latest/basics.html\#creating-bindings-for-a-simple-function}{Pybind11}%
\begin{footnote}[3]\sphinxAtStartFootnote
\sphinxnolinkurl{https://pybind11.readthedocs.io/en/latest/basics.html\#creating-bindings-for-a-simple-function}
%
\end{footnote}

\end{itemize}


\chapter{计算图}
\label{\detokenize{chapter_computational_graph/index:id1}}\label{\detokenize{chapter_computational_graph/index::doc}}
\sphinxAtStartPar
在上一章节中，我们展示了用户利用机器学习框架所编写的程序。这些用户程序包含了对于训练数据，模型和训练过程的定义。然而为了运行这些程序，机器学习系统依然需要解决诸多问题，包括：如何高效执行一个复杂的机器学习模型？如何识别出机器学习模型中需要训练的参数？如何自动计算更新模型所需的梯度？为了解决这些问题，现代机器学习框架实现了\sphinxstyleemphasis{计算图}(Computational
graph)这一技术。在本章中，我们详细讨论计算图的基本组成，生成和执行等关键设计。本章的学习目标包括：
\begin{itemize}
\item {} 
\sphinxAtStartPar
掌握计算图的基本构成。

\item {} 
\sphinxAtStartPar
掌握计算图静态生成和动态生成两种方法。

\item {} 
\sphinxAtStartPar
掌握计算图的常用执行方法。

\end{itemize}


\section{计算图的设计背景和作用}
\label{\detokenize{chapter_computational_graph/background_and_functionality:id1}}\label{\detokenize{chapter_computational_graph/background_and_functionality::doc}}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{dag}.svg}
\caption{基于计算图的架构}\label{\detokenize{chapter_computational_graph/background_and_functionality:id2}}\label{\detokenize{chapter_computational_graph/background_and_functionality:dag}}\end{figure}

\sphinxAtStartPar
早期的机器学习框架主要为了支持基于卷积神经网络的图像分类问题。这些神经网络的拓扑结构简单（神经网络层往往通过串行构建），他们的拓扑结构可以用简单的配置文件来表达（例如Caffe中基于Protocol
Buffer格式的模型定义）。随着机器学习的进一步发展，模型的拓扑日益复杂（包括混合专家，生成对抗网络，多注意力模型）。这些复杂的模型拓扑结构（例如：分支结构，带有条件的if\sphinxhyphen{}else循环）会影响模型算子的执行、自动化梯度计算（一般称为自动微分）以及训练参数的自动化判断。为此，我们需要一个更加通用的技术来执行任意机器学习模型，计算图应运而生。综合来看，计算图对于一个机器学习框架提供了以下几个关键作用：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{对于输入数据、算子和算子执行顺序的统一表达。}
机器学习框架用户可以用多种高层次编程语言（Python，Julia和C++）来编写训练程序。这些高层次程序需要统一的表达成框架底层C和C++算子的执行。因此，计算图的第一个核心作用是可以作为一个统一的数据结构来表达用户用不同语言编写的训练程序。这个数据结构可以准确表述用户的输入数据、模型所带有的多个算子，以及算子之间的执行顺序。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{定义中间状态和模型状态。}
在一个用户训练程序中，用户会生成中间变量（神经网络层之间传递的激活值和梯度）来完成复杂的训练过程。而这其中，只有模型参数需要最后持久化，从而为后续的模型推理做准备。通过计算图，机器学习框架可以准确分析出中间状态的生命周期（一个中间变量何时生成，以及何时销毁），从而帮助框架更好的管理内存。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{自动化计算梯度。}
用户给定的训练程序仅仅包含了一个机器学习模型如何将用户输入（一般为训练数据）转化为输出（一般为损失函数）的过程。而为了训练这个模型，机器学习框架需要分析任意机器学习模型和其中的算子，找出自动化计算梯度的方法。计算图的出现让自动化分析模型定义和自动化计算梯度成为可能。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{优化程序执行。}
用户给定的模型程序往往是“串行化”地连接起来多个神经网络层。通过利用计算图来分析模型中算子的执行关系，机器学习框架可以更好地发现将算子进行异步执行的机会，从而以更快的速度完成模型程序的执行。

\end{itemize}


\section{计算图的基本构成}
\label{\detokenize{chapter_computational_graph/components_of_computational_graph:id1}}\label{\detokenize{chapter_computational_graph/components_of_computational_graph::doc}}
\sphinxAtStartPar
计算图是用来表示深度学习网络模型在训练与推理过程中计算逻辑与状态的工具。计算框架在后端会将前端语言构建的神经网络模型前向计算与反向梯度计算以计算图的形式来进行表示。计算图由基本数据结构：张量(Tensor)和基本运算单元：算子(Operator)构成。在计算图中通常使用节点来表示算子，节点间的有向线段来表示张量状态，同时也描述了计算间的依赖关系。如
\hyperref[\detokenize{chapter_computational_graph/components_of_computational_graph:simpledag}]{图\ref{\detokenize{chapter_computational_graph/components_of_computational_graph:simpledag}}}所示，将\(\boldsymbol{Z}=relu(\boldsymbol{X}*\boldsymbol{Y})\)转化为计算图表示，数据流将根据图中流向与算子进行前向计算和反向梯度计算来更新图中张量状态，以此达到训练模型的目的。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{simpledag}.svg}
\caption{简单计算图}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:id6}}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:simpledag}}\end{figure}


\subsection{张量和算子}
\label{\detokenize{chapter_computational_graph/components_of_computational_graph:id2}}
\sphinxAtStartPar
在计算框架中，基础组件包含张量和算子，张量是基础数据结构，算子是基本运算单元。在数学中定义张量是基于向量与矩阵的推广，涵盖标量、向量与矩阵的概念。可以将标量理解为零阶张量，向量为一阶张量，我们熟悉的RGB彩色图像即为三阶张量。在计算框架中张量不仅存储数据，还存储数据类型、数据形状、维度或秩以及梯度传递状态等多个属性，如:numref:\sphinxtitleref{tensor\_attr}所示，列举了主要的属性和功能。可以通过\sphinxhref{https://github.com/openmlsys/openmlsys-pytorch/blob/master/chapter\_computational\_graph/tensor.py}{代码示例}%
\begin{footnote}[4]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/openmlsys/openmlsys-pytorch/blob/master/chapter\_computational\_graph/tensor.py}
%
\end{footnote}查看张量的属性和部分操作展示


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{张量属性}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:id7}}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:tensor-attr}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
张量属性
&\sphinxstyletheadfamily 
\sphinxAtStartPar
功能
\\
\hline
\sphinxAtStartPar
形状(shape)
&
\sphinxAtStartPar
存储张量的每个维度的长度，如{[}3,3,3{]}
\\
\hline
\sphinxAtStartPar
维度或秩(dim)
&
\sphinxAtStartPar
表示张量维度的数量，标量为0，向量为1、矩阵为2
\\
\hline
\sphinxAtStartPar
数据类型(dtype)
&
\sphinxAtStartPar
表示存储的数
据类型，如bool、int8、int16、float32、float64等
\\
\hline
\sphinxAtStartPar
存储位置(device)
&
\sphinxAtStartPar
创建张量时可以指定存储的设备位置，如CPU、GPU等
\\
\hline
\sphinxAtStartPar
名字(name)
&
\sphinxAtStartPar
张量的标识符
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
张量的形状是一个重要的属性，它记录了每个轴的长度，也就是张量每个维度的元素数量。秩则代表张量的轴数或者阶数。张量中通常可以保存布尔类型、浮点数、整型数以及复数和字符串数据。每一个张量都具有唯一的数据类型，在计算过程中会对所有参与运算的张量进行类型检查，当发现类型不匹配时就会报错。部分特殊的计算则必须使用指定的数据类型，比如逻辑运算应为布尔类型。在部分计算框架中张量的属性中包含可以指明张量存储的设备位置，比如存储于CPU、GPU等。张量数据的存储状态可以分为可变和不可变两种，不可变张量一般用于用户初始化的数据或者网络模型输入的数据；而可变张量则存储网络权重参数，根据梯度信息更新自身数据。

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_computational_graph/components_of_computational_graph:tensor}]{图\ref{\detokenize{chapter_computational_graph/components_of_computational_graph:tensor}}}，标量就是一个零阶张量，包含单个数值但没有轴信息。向量即为一阶张量，具有一个轴。二阶张量具有两个轴即秩为二。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{tensor}.svg}
\caption{张量}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:id8}}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:tensor}}\end{figure}

\sphinxAtStartPar
通常我们使用的张量是“整齐”的，每个轴上的具有相同的元素个数，就像一个“矩形”或者“立方体”。在特定的环境中，也会使用特殊类型的张量，比如不规则张量和稀疏张量，如
\hyperref[\detokenize{chapter_computational_graph/components_of_computational_graph:tensorclass}]{图\ref{\detokenize{chapter_computational_graph/components_of_computational_graph:tensorclass}}}中所示。不规则张量在某个轴上可能具有不同的元素个数，它们支持存储和处理包含非均匀形状的数据，在自然语言处理领域，不规则张量可以存储不同长度文本的信息。稀疏张量则通常应用于图数据与图神经网络中，采用特殊的存储格式如坐标表格式（Coordinate
List， COO），可以高效存储稀疏数据，节省存储空间。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{tensorclass}.svg}
\caption{张量分类}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:id9}}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:tensorclass}}\end{figure}

\sphinxAtStartPar
算子是构成神经网络的基本计算单元。算子按照功能可以分为张量操作、神经网络操作、数据流操作和控制流操作等。
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{张量操作}：包括张量的结构操作和张量的数学运算。张量结构操作有：张量创建、索引切片、维度变换和合并分割等。张量的数学运算包含标量运算、向量运算和矩阵运算。标量运算符的特点是对张量实施逐元素运算。向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。矩阵运算包括矩阵乘法、矩阵范数、矩阵行列式、矩阵求特征值、矩阵分解等运算。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{神经网络操作}：包括特征提取、激活函数、损失函数、优化算法等。特征提取是机器学习中的常见操作，核心是提取比原输入更具代表性的张量，常见的卷积操作就是特征提取算子。激活函数(Activation
Function)负责将神经网络层的输入映射到输出端。引入激活函数是为了增加神经网络模型的非线性，没有激活函数的每层都相当于矩阵相乘。常见的激活函数包括S型生长曲线(Sigmoid)、修正线性单元(Rectified
Linear Unit, ReLU)等。损失函数(Loss
Function)是用来估量模型的预测值与真实值之间的不一致程度。优化算法基于梯度采用不同策略更新参数权值来最小化损失函数，常见的优化算法有随机梯度下降法(Stochastic
Gradient Descent, SGD)、自适应矩估计(Adaptive Moment Estimation,
Adam)等。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{数据流操作}：包含数据的预处理与数据载入相关算子，数据预处理算子主要是针对图像数据和文本数据的裁剪填充、归一化、数据增强等操作。数据载入通常会对数据集进行随机乱序(Shuffle)、分批次载入(Batch)以及预载入(Prefetch)等操作。数据流操作主要功能是对原始数据进行处理后，转换为计算框架本身支持的数据格式，并且按照迭代次数输入给网络进行训练或者推理，提升数据载入速度，减少内存占用空间，降低网络训练等待时间。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{控制流操作}：可以控制计算图中的数据流向，当表示灵活复杂的模型时需要控制流。使用频率比较高的控制流算子有条件运算符和循环运算符。控制流操作一般分为两类，计算框架本身提供的控制流操作符和前端语言控制流操作符。控制流操作不仅会影响神经网络模型前向运算的数据流向，也会影响反向梯度运算的数据流向。

\end{itemize}


\subsection{计算依赖}
\label{\detokenize{chapter_computational_graph/components_of_computational_graph:id3}}
\sphinxAtStartPar
在计算图中，算子之间存在依赖关系，而这种依赖关系影响了算子的执行顺序与并行情况。此外在深度学习算法模型中，计算图是一个有向无环图，也即在计算图中造成循环依赖的数据流向是不被允许的。为了理解计算依赖关系并且分析计算图中循环与循环依赖之间的区别，下面将对计算图中的计算节点依赖关系进行讲解。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=400\sphinxpxdimen]{{dependence}.svg}
\caption{计算依赖}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:id10}}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:dependence}}\end{figure}

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_computational_graph/components_of_computational_graph:dependence}]{图\ref{\detokenize{chapter_computational_graph/components_of_computational_graph:dependence}}}中所示，在此简单的计算图中，若将\(\mathbf{Matmul1}\)算子移除则该节点无输出，导致后续的激活函数无法得到输入，从而计算图中的数据流动中断，这表明计算图中的算子间具有依赖关系并且存在传递性。我们对依赖关系进行区分如下：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{直接依赖}：节点\(\mathbf{ReLU1}\)直接依赖于节点\(\mathbf{Matmul1}\)，即如果节点\(\mathbf{ReLU1}\)要执行运算，必须接受直接来自节点\(\mathbf{Matmul1}\)的输出数据；

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{间接依赖}：节点\(\mathbf{Add}\)间接依赖于节点\(\mathbf{Matmul1}\)，即节点\(\mathbf{Matmul1}\)的数据并未直接传输给节点\(\mathbf{Add}\)，而是经过了某个或者某些中间节点进行处理后再传输给节点\(\mathbf{Add}\)，而这些中间节点可能是节点\(\mathbf{Add}\)的直接依赖节点，也可能是间接依赖节点；

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{相互独立}：在计算图中节点\(\mathbf{Matmul1}\)与节点\(\mathbf{Matmul2}\)之间并无数据输入输出依赖关系，所以这两个节点间相互独立。

\end{itemize}

\sphinxAtStartPar
掌握依赖关系后，分析
\hyperref[\detokenize{chapter_computational_graph/components_of_computational_graph:recurrent}]{图\ref{\detokenize{chapter_computational_graph/components_of_computational_graph:recurrent}}}可以得出节点\(\mathbf{Add}\)间接依赖于节点\(\mathbf{Matmul}\)，而节点\(\mathbf{Matmul}\)直接依赖于节点\(\mathbf{Add}\)，此时两个节点互相等待对方计算完成输出数据，将无法执行计算任务。若我们手动同时给两个节点赋予输入，计算将持续不间断进行，模型训练将无法停止造成死循环。循环依赖产生正反馈数据流，被传递的数值可能在正方向上无限放大，导致数值上溢，或者负方向上放大导致数值下溢，也可能导致数值无限逼近于0，这些情况都会致使模型训练无法得到预期结果。在构建深度学习模型时，应避免算子间产生循环依赖。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{recurrent}.svg}
\caption{循环依赖}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:id11}}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:recurrent}}\end{figure}

\sphinxAtStartPar
在深度学习计算框架中，表示循环关系通常是以\sphinxstylestrong{展开}机制（Unrolling）来实现。当需要实现循环关系时，循环体的计算子图按照迭代次数进行复制，将代表相邻迭代轮次的子图进行串联，相邻迭代轮次的计算子图之间就是直接依赖关系。循环三次的计算图进行展开如
\hyperref[\detokenize{chapter_computational_graph/components_of_computational_graph:unroll}]{图\ref{\detokenize{chapter_computational_graph/components_of_computational_graph:unroll}}}。在计算图中，每一个张量和运算符都具有独特的标识符，即使是相同的操作运算，在参与不同计算任务时都具有不同的标识符。区分循环关系和循环依赖的关键在于，是否两个独特标识符之间的运算互相具有直接依赖和相互依赖。循环关系在展开复制计算子图的时候会给复制的所有张量和运算符赋予新的标识符，区分被复制的原始子图，以避免形成循环依赖。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{unroll}.svg}
\caption{循环展开}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:id12}}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:unroll}}\end{figure}


\subsection{控制流}
\label{\detokenize{chapter_computational_graph/components_of_computational_graph:id4}}
\sphinxAtStartPar
控制流能够设定特定的顺序执行计算任务。若计算图中无控制流，则每个节点只执行一次，当所有节点按照顺序执行完时，计算图即完成计算。加入控制流后可以让计算图中某些节点循环执行任意次数，也可以根据条件判断选择某些节点不执行，控制流使得我们可以构建更加灵活和复杂的模型。许多机器学习模型依赖控制流进行训练和推理，特别是基于递归神经网络和强化学习的模型就依赖于循环递归关系和依据数据的条件执行。

\sphinxAtStartPar
为了提高性能、可扩展性和表达能力，计算框架必须支持控制流。目前主流的计算框架中通常使用两种方式来提供控制流：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{计算框架控制原语}：计算框架在内部设计了低级别细粒度的控制原语运算符，通过原语运算符的结合使用来实现控制流，这种实现方式也被称为图内方法（In\sphinxhyphen{}graph
approach）。此类方法的代表就是TensorFlow中的Switch、Merge、Enter、Exit、NextIteration五个原语。TensorFlow通过组合五个原语提供\sphinxstyleemphasis{tf.cond()}和\sphinxstyleemphasis{tf.while\_loop()}来实现条件控制和循环控制。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{前端语言控制流}：通过高级语言Python、C++的控制流语句来进行计算图中的控制决策，这类实现方式也被称为图外方法（Out\sphinxhyphen{}of\sphinxhyphen{}graph
approach）。计算框架PyTorch、MindSpore中就直接使用Python的控制流，将控制流和数据流之间保持了严格的分离。

\end{itemize}

\sphinxAtStartPar
图内方法控制流采用框架原语实现，在进行模型编译、优化与运行时都具备优势，并且可以准确的判定机器学习模型中计算梯度时需要缓存的变量，提高运行效率，同时由于不依赖外部语言便于部署到不同环境中去。但由于控制原语缺乏进一步的抽象，对于用户不友好，需要掌握控制原语的使用方法，结合前端语言使用才能描述复杂模型结构。

\sphinxAtStartPar
相对于图内方法，图外方法直接使用前端语言控制流则相对更加灵活易用，用户编写模型控制时更加便捷直观，其缺点在于若要将模型进行优化部署，则需要在编译阶段将前端语言的控制流转化为框架原语描述。

\sphinxAtStartPar
目前在主流的深度学习计算框架中，均提供图外方法和图内方法支持。为了便于理解控制流对前向计算与反向计算的影响，后续的讲解均使用\sphinxstylestrong{图外方法}实现控制流。常见的控制流包括条件分支与循环两种。当模型包含控制流操作时，梯度在反向传播经过控制流时，需要在反向梯度计算图中也构造生成相应的控制流，才能够正确计算参与运算的张量梯度。

\sphinxAtStartPar
下面这段代码描述了简单的条件控制，我们使用\sphinxstyleemphasis{matmul}表示矩阵乘法算子：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{control}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{B}\PYG{p}{,} \PYG{n}{C}\PYG{p}{,} \PYG{n}{conditional} \PYG{o}{=} \PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{conditional}\PYG{p}{:}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{B}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{C}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{y}
\end{sphinxVerbatim}

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{if}.svg}
\caption{条件控制计算图}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:id13}}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:if}}\end{figure}

\sphinxAtStartPar
\hyperref[\detokenize{chapter_computational_graph/components_of_computational_graph:if}]{图\ref{\detokenize{chapter_computational_graph/components_of_computational_graph:if}}}描述上述代码的前向计算图和反向计算图。对于具有if\sphinxhyphen{}条件的模型，梯度计算需要知道采用了条件的哪个分支，然后将梯度逻辑应用于该分支。在前向计算图中张量\({C}\)经过条件控制不参与计算，在反向计算时同样遵守控制流决策，不会计算关于张量\(C\)的梯度。

\sphinxAtStartPar
当模型中有循环控制时，循环中的操作可以执行零次或者多次。此时采用展开机制，对每一次操作都赋予独特的运算标识符，以此来区分相同运算操作的多次调用。每一次循环都直接依赖于前一次循环的计算结果，所以在循环控制中需要维护一个张量列表，将循环迭代的中间结果缓存起来，这些中间结果将参与前向计算和梯度计算。下面这段代码描述了简单的循环控制，将其展开得到等价代码后，可以清楚的理解需要维护张量\(\boldsymbol{X_i}\)和\(\boldsymbol{W_i}\)的列表。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{recurrent\PYGZus{}control}\PYG{p}{(}\PYG{n}{X} \PYG{p}{:} \PYG{n}{Tensor}\PYG{p}{,} \PYG{n}{W} \PYG{p}{:} \PYG{n}{Sequence}\PYG{p}{[}\PYG{n}{Tensor}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cur\PYGZus{}num} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{cur\PYGZus{}num}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{X} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{W}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{X}
\PYG{c+c1}{\PYGZsh{}利用展开机制将上述代码展开，可得到等价表示}
\PYG{k}{def} \PYG{n+nf}{recurrent\PYGZus{}control}\PYG{p}{(}\PYG{n}{X} \PYG{p}{:} \PYG{n}{Tensor}\PYG{p}{,} \PYG{n}{W} \PYG{p}{:} \PYG{n}{Sequence}\PYG{p}{[}\PYG{n}{Tensor}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{X1} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{W}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{}为便于表示与后续说明，此处W = W[0], W1 = W[1], W2 = W[2]}
    \PYG{n}{X2} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{X1}\PYG{p}{,} \PYG{n}{W1}\PYG{p}{)}
    \PYG{n}{Y} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{X2}\PYG{p}{,} \PYG{n}{W2}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{Y}
\end{sphinxVerbatim}

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_computational_graph/components_of_computational_graph:while}]{图\ref{\detokenize{chapter_computational_graph/components_of_computational_graph:while}}}描述了上述代码的前向计算图和反向计算图，循环控制的梯度同样也是一个循环，它与前向循环相迭代次数相同，执行循环体的梯度计算。循环体输出的梯度值作为下一次梯度计算的初始值，直至循环结束。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{while}.svg}
\caption{循环控制计算图}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:id14}}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:while}}\end{figure}


\subsection{基于链式法则计算梯度}
\label{\detokenize{chapter_computational_graph/components_of_computational_graph:id5}}
\sphinxAtStartPar
在上一小节循环展开的例子中，当神经网络接收输入张量\(\boldsymbol{Y}\)后，输入数据根据计算图逐层进行计算并保存中间结果变量，直至经过多层的计算后最终产生输出\(\boldsymbol{Y_3}\)，这个过程我们称之为\sphinxstylestrong{前向传播}（Forward
propagation）。在深度神经网络模型训练过程中，前向传播的输出结果与标签值可以产生一个损失函数结果。模型将来自损失函数的数据信息通过计算图反向流动，执行梯度计算来进行更新训练参数，这个过程我们称之为\sphinxstylestrong{反向传播}（Back
propagation）。在神经网络模型中，反向传播通常使用损失函数关于参数的梯度来进行更新，也可以使用其他信息进行反向传播，在这里我们仅讨论一般情况。

\sphinxAtStartPar
在这里我们简单回忆一下复合函数的链式法则公式。链式法则是微积分中的求导法则，用于求解复合函数中的导数。复合函数的导数是构成复合有限个函数在相应点的导数乘积。假设\sphinxstyleemphasis{f}和\sphinxstyleemphasis{g}是关于实数\sphinxstyleemphasis{x}的映射函数，设\(y=g(x)\)并且\(z=f(y)=f(g(x))\)，则\sphinxstyleemphasis{z}对\sphinxstyleemphasis{x}的导数即为：
\begin{equation}\label{equation:chapter_computational_graph/components_of_computational_graph:chapter_computational_graph/components_of_computational_graph:0}
\begin{split}\frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx}\end{split}
\end{equation}
\sphinxAtStartPar
神经网络的反向传播是根据反向计算图的特定运算顺序来执行链式法则的算法。由于神经网络的输入通常为三维张量，输出为一维向量。因此将上述复合函数关于标量的梯度法则进行推广和扩展。假设\(\boldsymbol{X}\)是\sphinxstyleemphasis{m}维张量，\(\boldsymbol{Y}\)为\sphinxstyleemphasis{n}维张量，\(\boldsymbol{z}\)为一维向量，\(\boldsymbol{Y}=g(\boldsymbol{X})\)并且\(\boldsymbol{z}=f(\boldsymbol{Y})\)，则\(\boldsymbol{z}\)关于\(\boldsymbol{X}\)每一个元素的偏导数即为：
\begin{equation}\label{equation:chapter_computational_graph/components_of_computational_graph:chapter_computational_graph/components_of_computational_graph:1}
\begin{split}\frac{\partial z}{\partial x_i}=\sum_j\frac{\partial z}{\partial y_j}\frac{\partial y_j}{\partial x_i}\end{split}
\end{equation}
\sphinxAtStartPar
上述公式可以等价的表示为：
\begin{equation}\label{equation:chapter_computational_graph/components_of_computational_graph:chapter_computational_graph/components_of_computational_graph:2}
\begin{split}\nabla_{\boldsymbol{X}}\boldsymbol{z} = (\frac{\partial \boldsymbol{Y}}{\partial \boldsymbol{X}})^{\top}\nabla_{\boldsymbol{Y}}\boldsymbol{z}\end{split}
\end{equation}
\sphinxAtStartPar
其中\(\nabla_{\boldsymbol{X}}\boldsymbol{z}\)表示\(\boldsymbol{z}\)关于\(\boldsymbol{X}\)的梯度矩阵。

\sphinxAtStartPar
上一小节中简单的循环控制模型前向传播可以表示为\(\boldsymbol{Y}=\boldsymbol{W_2}(\boldsymbol{W_1}(\boldsymbol{W}(\boldsymbol{X})))\)。在反向传播的过程中可以将前向计算等价为\(\boldsymbol{Y}=\boldsymbol{W_2}\boldsymbol{X_2}\)，首先得到参数\(\boldsymbol{W_2}\)的梯度表示。再接着根据\(\boldsymbol{X_2}=\boldsymbol{W_1}\boldsymbol{X_1}\)得到\(\boldsymbol{W_1}\)的梯度表示，按照层级即可推导得出\(\boldsymbol{W}\)的梯度表示：
\begin{equation}\label{equation:chapter_computational_graph/components_of_computational_graph:chapter_computational_graph/components_of_computational_graph:3}
\begin{split}\begin{aligned}
\nabla\boldsymbol{X_2} &= \nabla\boldsymbol{Y}\boldsymbol{W_2}^\top  \\
\nabla\boldsymbol{W_2} &= \boldsymbol{X_2}^\top\nabla\boldsymbol{Y}   \\
\nabla\boldsymbol{X_1} &= \nabla\boldsymbol{X_2}\boldsymbol{W_1}^\top = (\nabla\boldsymbol{Y}\boldsymbol{W_2}^\top)\boldsymbol{W_1}^\top   \\
\nabla\boldsymbol{W_1} &= \boldsymbol{X_1}^\top\nabla\boldsymbol{X_2} = \boldsymbol{X_1}^\top(\nabla\boldsymbol{Y}\boldsymbol{W_2}^\top)  \\
\nabla\boldsymbol{X} &= \nabla\boldsymbol{X_1}\boldsymbol{W}^\top = ((\nabla\boldsymbol{Y}\boldsymbol{W_2}^\top)\boldsymbol{W_1}^\top)\boldsymbol{W}^\top   \\
\nabla\boldsymbol{W} &= \boldsymbol{X}^\top\nabla\boldsymbol{X_1} = \boldsymbol{X}^\top((\nabla\boldsymbol{Y}\boldsymbol{W_2}^\top)\boldsymbol{W_1}^\top)
\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
根据链式法则，相应位置的导数乘积即可将网络得到的损失函数梯度信息传播到每一个权重参数，应用优化器的参数权重更新规则，即可达到神经网络模型参数训练迭代的目的。

\sphinxAtStartPar
根据上述公式我们可以得出循环控制的反向梯度计算过程如下，在下面代码中变量的前缀\sphinxstyleemphasis{grad}代表变量梯度变量，\sphinxstyleemphasis{transpose}代表矩阵转置算子。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{grad\PYGZus{}X2} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{grad\PYGZus{}Y}\PYG{p}{,} \PYG{n}{transpose}\PYG{p}{(}\PYG{n}{W2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{grad\PYGZus{}W2} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{X2}\PYG{p}{)}\PYG{p}{,} \PYG{n}{grad\PYGZus{}Y}\PYG{p}{)}
\PYG{n}{grad\PYGZus{}X1} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{grad\PYGZus{}X2}\PYG{p}{,} \PYG{n}{transpose}\PYG{p}{(}\PYG{n}{W1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{grad\PYGZus{}W1} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{X1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{grad\PYGZus{}X2}\PYG{p}{)}
\PYG{n}{grad\PYGZus{}X} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{grad\PYGZus{}X1}\PYG{p}{,} \PYG{n}{transpose}\PYG{p}{(}\PYG{n}{W}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{grad\PYGZus{}W} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{,} \PYG{n}{grad\PYGZus{}X1}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
结合公式、代码以及
\hyperref[\detokenize{chapter_computational_graph/components_of_computational_graph:chain}]{图\ref{\detokenize{chapter_computational_graph/components_of_computational_graph:chain}}}我们可以看出，在反向传播过程中使用到前向传播的中间变量。因此保存网络中间层输出状态和中间变量，尽管占用了部分内存但能够复用计算结果，达到了提高反向传播计算效率的目的。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{chain}.svg}
\caption{反向传播局部计算图}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:id15}}\label{\detokenize{chapter_computational_graph/components_of_computational_graph:chain}}\end{figure}

\sphinxAtStartPar
在深度学习计算框架中，控制流可以进行嵌套，比如多重循环和循环条件控制，计算图会对复杂控制流进行准确的描述，以便于执行正确的计算调度与执行任务。可以通过\sphinxhref{https://github.com/openmlsys/openmlsys-pytorch/blob/master/chapter\_computational\_graph/control\_flow.py}{代码示例}%
\begin{footnote}[5]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/openmlsys/openmlsys-pytorch/blob/master/chapter\_computational\_graph/control\_flow.py}
%
\end{footnote}查看在条件控制和循环控制下，前向和反向计算的数据流。


\section{计算图的生成}
\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:id1}}\label{\detokenize{chapter_computational_graph/generation_of_computational_graph::doc}}
\sphinxAtStartPar
计算框架执行深度学习模型训练时，会根据模型结构生成计算图，通过调度计算图完成模型计算。在计算框架中可以生成静态图和动态图两种计算图。静态图对应声明式编程范式，动态图对应命令式编程范式。静态生成可以根据前端语言描述的神经网络拓扑结构以及参数变量等信息构建一份固定的计算图，因此静态图在执行期间可以不依赖前端语言描述，常用于神经网络模型的部署，比如移动端人脸识别场景中的应用等。动态图则需要在每一次执行神经网络模型依据前端语言描述动态生成一份临时的计算图，这意味着计算图的动态生成过程灵活可变，该特性有助于我们在神经网络结构调整阶段提高效率。主流计算框架TensorFlow、MindSpore均支持动态图和静态图模式；PyTorch则可以通过工具将构建的动态图神经网络模型转化为静态结构，以获得高效的计算执行效率。了解两种计算图生成方式的优缺点及构建执行特点，可以针对待解决的任务需求，选择合适的生成方式调用执行神经网络模型。


\subsection{静态生成}
\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:id2}}
\sphinxAtStartPar
静态图的生成与执行原理如
\hyperref[\detokenize{chapter_computational_graph/generation_of_computational_graph:static}]{图\ref{\detokenize{chapter_computational_graph/generation_of_computational_graph:static}}}所示，采用先编译后执行的方式，该模式将计算图的定义和执行进行分离。在静态图模式下使用前端语言定义模型形成完整的程序表达后，并不使用前端语言解释器进行执行，而是将前端描述的完整模型交给计算框架。框架在执行模型计算之前会首先对神经网络模型进行分析，获取网络层之间的连接拓扑关系以及参数变量设置、损失函数等信息，接着用一种特殊的静态数据结构来描述拓扑结构及其他神经网络模型组件，这种特殊的静态数据结构通常被称为静态计算图。静态计算图可以通过优化策略转换成等价的更加高效的结构。当进行模型训练或者推理过程时，静态计算图接收数据并通过相应硬件调度执行图中的算子来完成任务。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{static}.svg}
\caption{静态图生成与执行}\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:id6}}\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:static}}\end{figure}

\sphinxAtStartPar
以构建并执行下列伪代码，来详细讲解静态图的生成与执行，\sphinxstyleemphasis{matmul}表示矩阵乘法算子，\sphinxstyleemphasis{relu}表示线性矫正单元算子。在部分计算框架中如TensorFlow进行前端定义时，需要声明并编写包含数据占位符、损失函数、优化函数、网络编译、执行环境以及网络执行器等在内的预定义配置项，此外还需要使用图内控制流算子编写控制语句，代码较为繁琐并缺乏可读性。随着计算框架设计的改进与发展，框架提供的编程接口和模型构建模式呈现出更加统一和友好的趋势，比如MindSpore提供动静态统一的前端编程表达。因此为了便于理解静态生成的过程与原理，此处使用更加简洁的语言逻辑描述模型。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{model}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{flag}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{flag}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{n}{Y} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{W1}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{Y} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{W2}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}
    \PYG{n}{Y} \PYG{o}{=} \PYG{n}{Y} \PYG{o}{+} \PYG{n}{b}
    \PYG{n}{Y} \PYG{o}{=} \PYG{n}{relu}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{Y}
\end{sphinxVerbatim}

\sphinxAtStartPar
完成前端语言的模型完整构建表达后，执行模型运算时不会直接接收输入数据进行计算，而是使用计算框架的编译器对模型进行编译。由于在进行静态生成编译时并不读取输入数据，此时需要一种特殊的张量来表示输入数据辅助构建完整的计算图，这种特殊张量就被称之为“数据占位符”。在上述的伪代码中输入数据\sphinxstylestrong{X}需要使用占位符在静态图中表示。构造伪代码中的条件控制时，由于在静态图模式下构建网络并没有执行任何计算，对于条件控制在编译阶段并不会进行逻辑运算完成判断，因此需要将条件控制算子以及所有的分支计算子图加入计算图中。在执行阶段网络接受数据流入，调度条件控制算子时进行逻辑判断，控制数据流入不同的分支计算子图中进行后续计算。由于控制流和静态生成的特殊性，在部分计算框架中前端语言Python的控制流不能够被正确编译为等价的静态图结构，因此需要使用复杂的图内方法实现控制流。

\sphinxAtStartPar
静态生成的过程是采用计算框架编译器将代码编译为中间表示。计算框架编译器受传统编译器方案启发，设计体系结构包含两部分：编译器前端和编译器后端。中间表示承上启下贯穿前端和后端，是前端源代码和目标硬件代码之间的中间数据格式。在计算框架编译器中中间表示以计算图形式存在，编译器会根据前端神经网络模型自动构建完整的前向计算图和反向计算图。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{static-gen}.svg}
\caption{静态生成}\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:id7}}\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:staticgen}}\end{figure}

\sphinxAtStartPar
经过编译后获取完整的计算图，能够根据全局信息完成图优化策略，进行编译优化形成与模型完全等价的静态图。编译器前端负责完成计算图与硬件无关的转换和优化，比如算子融合将网络中的两个或多个细粒度的算子融合为一个粗粒度算子，比如
\hyperref[\detokenize{chapter_computational_graph/generation_of_computational_graph:staticgen}]{图\ref{\detokenize{chapter_computational_graph/generation_of_computational_graph:staticgen}}}中将\sphinxstyleemphasis{add}算子与\sphinxstyleemphasis{relu}合并为一个操作，可节省中间计算结果的存储、读取等过程，降低框架底层算子调度的开销，从而提升执行性能和效率。编译器后端负责与硬件相关的计算图优化、代码指令生成和编译，优化手段包括硬件算子选择、内存分配、内存复用等，提高算子执行效率和内存利用效率，降低内存开销。编译器后端因此使用静态图模型运行往往能够获取更好的性能和更少的内存占用。在后续章节中将详细介绍更多编译器前端和编译器后端的优化策略。

\sphinxAtStartPar
优化完成的计算图通过编译器后端根据计算硬件来生成适配的执行代码。在执行阶段，调用执行器接受输入数据，依据计算图调度算子执行训练或者推理任务。在训练任务调度算子执行时，由于在执行阶段已经编译获取模型整体结构，计算框架可以利用自动并行算法制定合理的模型切分与并行策略，进一步提高计算效率。

\sphinxAtStartPar
使用静态图构建模型，编译构建完整的计算图后，计算图可以进行序列化保存，再次执行时允许使用序列化模型直接进行训练或推理，无需重新编译前端语言源代码。得益于编译器前端、中间表示、编译器后端多级的计算框架编译器体系结构，编译器后端可以将神经网络模型中间表示转换为不同硬件代码。结合计算图序列化和计算图可转换成多种硬件代码两种特性，静态图模型可以直接部署在不同的硬件上面，提供高效的推理服务。

\sphinxAtStartPar
尽管静态图具备强大的执行计算性能与直接部署能力，但是在部分计算框架中静态图模式下，使用前端语言编写神经网络模型以及定义模型训练过程代码较为繁琐，尤其掌握图内控制流方法具备一定的学习难度，因此熟练掌握并使用静态图模式对于初学者并不友好。其次，静态生成采用先编译后执行的方式，编译阶段和执行阶段分离，前端语言构建的神经网络模型经过编译后，计算图结构便固定执行阶段不再改变，并且经过优化用于执行的计算图结构与原始代码有较大的差距，导致代码中的错误难以定位到准确位置，增加了代码调试难度。此外在神经网络模型开发迭代环节，不能即时打印中间结果。若在源码中添加输出中间结果的代码，则需要将源码重新编译后，再调用执行器才能获取相关信息，降低了代码调试效率。而动态图模式则更加灵活，接下来讲解动态生成机制。


\subsection{动态生成}
\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:id3}}
\sphinxAtStartPar
动态图原理如
\hyperref[\detokenize{chapter_computational_graph/generation_of_computational_graph:dynamic}]{图\ref{\detokenize{chapter_computational_graph/generation_of_computational_graph:dynamic}}}所示，采用解析式的执行方式，其核心特点是编译与执行同时发生。动态图采用前端语言自身的解释器对代码进行解析，利用计算框架本身的算子分发功能，算子会即刻执行并输出结果。动态图模式采用用户友好的命令式编程范式，使用前端语言构建神经网络模型更加简洁。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{dynamic}.svg}
\caption{动态图原理}\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:id8}}\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:dynamic}}\end{figure}

\sphinxAtStartPar
由于动态图模式的编程友好性，动态图深受广大深度学习研究者青睐。接下来使用上一小节的伪代码来讲解动态生成和静态生成的区别。

\sphinxAtStartPar
尽管静态图和动态图在前端语言表达上略有差异，但本质的区别在于静态生成和动态生成的编译执行过程不同。使用前端语言构建完成模型表达后，动态生成并不采用计算框架编译器生成完整的静态计算图，而是采用前端语言的解释器Python
API调用计算框架，框架利用自身的算子分发功能，将Python调用的算子在相应的硬件如CPU、GPU、NPU等上进行加速计算，然后再将计算结果返回给前端。该过程并不产生静态的计算图，而是按照前端语言描述模型结构，按照计算依赖关系进行调度执行，动态生成临时的图拓扑结构。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{dynamic_gen}.png}
\caption{动态生成}\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:id9}}\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:dynamicgen}}\end{figure}

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_computational_graph/generation_of_computational_graph:dynamicgen}]{图\ref{\detokenize{chapter_computational_graph/generation_of_computational_graph:dynamicgen}}}中所示，神经网络前向计算按照模型声明定义的顺序进行执行。当模型接收输入数据\(\boldsymbol{X}\)后，计算框架开始动态生成图拓扑结构，添加输入节点并准备将数据传输给后续节点。模型中存在条件控制时，动态图模式下会即刻得到逻辑判断结果并确定数据流向，因此在图中假设判断结果为真的情况下，图结构中仅会添加关于张量\(\boldsymbol{W1}\)的\sphinxstyleemphasis{matmul}算子节点。按照代码制定的模型计算顺序与算子依赖关系，计算框架会依次添加\sphinxstyleemphasis{add}算子节点和\sphinxstyleemphasis{ReLU}算子节点。计算框架会在添加节点的同时完成算子分发计算并返回计算结果，同时做好准备向后续添加的节点传输数据。当模型再次进行前向计算时，动态生成的图结构则失效，并再次根据输入和控制条件生成新的图结构。相比于静态生成，可以发现动态生成的图结构并不能完整表示前端语言描述的模型结构，需要即时根据控制条件和数据流向产生图结构。由于计算框架无法通过动态生成获取完整的图结构，因此动态图模式下难以进行图结构优化以提高计算效率。

\sphinxAtStartPar
在静态生成环节，由于已经获取完整的神经网络模型定义，因此可以同时构建出完整的前向计算图和反向计算图。而在动态生成中，由于边解析边执行的特性，反向梯度计算的构建随着前向计算调用而进行。在执行前向过程中，计算框架根据前向算子的调用信息，记录对应的反向算子信息以及参与梯度计算的张量信息。前向计算完毕之后，反向算子与张量信息随之完成记录，计算框架会根据前向动态图拓扑结构，将所有反向过程串联起来形成整体反向计算图。最终，将反向图在计算硬件上执行计算得到梯度用于参数更新。

\sphinxAtStartPar
对应于
\hyperref[\detokenize{chapter_computational_graph/generation_of_computational_graph:dynamicgen}]{图\ref{\detokenize{chapter_computational_graph/generation_of_computational_graph:dynamicgen}}}中，当调用到关于张量\(\boldsymbol{W1}\)的\sphinxstyleemphasis{matmul}算子节点时，框架会执行两个操作：调用\sphinxstyleemphasis{matmul}算子，计算关于输入\(\boldsymbol{X}\)和\(\boldsymbol{W1}\)的乘积结果，同时根据反向计算过程\(\boldsymbol{Grad\_W1}=\boldsymbol{Grad\_Y}*\boldsymbol{X}\)，记录下需要参与反向计算的算子和张量\(\boldsymbol{X}\)。计算框架依照算子调度顺序记录参与反向计算的算子和张量。当前向计算执行完毕，计算框架根据动态生成的前向计算图结构拓扑关系，利用记录的反向计算算子和张量动态生成反向计算图，最终完成神经网络模型的梯度计算和参数更新。

\sphinxAtStartPar
尽管动态生成中完整的网络结构在执行前是未知的，不能使用静态图中的图优化技术来提高计算执行性能。但其即刻算子调用与计算的能力，使得模型代码在运行的时候，每执行一句就会立即进行运算并会返回具体的值，方便开发者在模型构建优化过程中进行错误分析、结果查看等调试工作，为研究和实验提供了高效的助力。

\sphinxAtStartPar
此外得益于动态图模式灵活的执行计算特性，动态生成可以使用前端语言的原生控制流，充分发挥前端语言的编程友好性特性。解决了静态图中代码难调试、代码编写繁琐以及控制流复杂等问题，对于初学者更加友好，提高了算法开发迭代效率和神经网络模型改进速率。


\subsection{动态和静态生成的比较}
\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:id4}}
\sphinxAtStartPar
静态生成和动态生成的过程各有利弊。从使用者的角度可以直观的感受到静态图不能实时获取中间结果、代码调试困难以及控制流编写复杂，而动态图可以实时获取结果、调试简单、控制流符合编程习惯。虽然静态图的编写、生成过程复杂，但是相应的执行性能却超过动态图，我们用一个简单的例子来说明在性能和内存占用方面静态图的优势。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{model}\PYG{p}{(}\PYG{n}{X1}\PYG{p}{,} \PYG{n}{X2}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{Y1} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{X1}\PYG{p}{,} \PYG{n}{W1}\PYG{p}{)}
    \PYG{n}{Y2} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{X2}\PYG{p}{,} \PYG{n}{W2}\PYG{p}{)}
    \PYG{n}{Y} \PYG{o}{=} \PYG{n}{Y1} \PYG{o}{+} \PYG{n}{Y2}
    \PYG{n}{output} \PYG{o}{=} \PYG{n}{relu}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{output}
\end{sphinxVerbatim}

\sphinxAtStartPar
在静态生成过程中，计算框架获取完整的计算图可以分析出计算\(\boldsymbol{Y_1}\)和\(\boldsymbol{Y_2}\)的过程相对独立，可以将其进行自动并行计算，加快计算效率。而动态生成的过程中，若无手动配置并行策略，计算框架无法获取图结构不能分析出算子之间的独立性，则只能按照代码顺序执行。模型在输出结果之前执行了\sphinxstyleemphasis{add}和\sphinxstyleemphasis{relu}算子操作，在静态生成过程中利用计算图优化策略中的算子融合方法，可以将这两个算子融合为一个算子执行，这样减少了中间变量\(\boldsymbol{Y}\)的存储与读取过程，加快了计算效率，减少了内存占用。而动态生成过程则需要按照顺序执行\sphinxstyleemphasis{add}和\sphinxstyleemphasis{relu}两步操作，需要存储变量\(\boldsymbol{Y}\)。除此之外，由于静态生成能够同时分析重构出前向计算图和反向计算图，可以提前确定反向计算中需要保存的前向中间变量信息。而动态生成则在完成前向计算后才能构建出反向计算图，为了保证反向计算效率需要保存更多的前向计算中间变量信息，相比之下静态生成的过程更加节省内存占用。

\sphinxAtStartPar
为了方便读者对比，将静态图和动态图特性总结见
\hyperref[\detokenize{chapter_computational_graph/generation_of_computational_graph:cmp-dynamic-static}]{表\ref{\detokenize{chapter_computational_graph/generation_of_computational_graph:cmp-dynamic-static}}}。


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{静态图和动态图对比}\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:id10}}\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:cmp-dynamic-static}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
特性
&\sphinxstyletheadfamily 
\sphinxAtStartPar
静态图
&\sphinxstyletheadfamily 
\sphinxAtStartPar
动态图
\\
\hline
\sphinxAtStartPar
即时获取中间结果
&
\sphinxAtStartPar
否
&
\sphinxAtStartPar
是
\\
\hline
\sphinxAtStartPar
代码调试难易
&
\sphinxAtStartPar
难
&
\sphinxAtStartPar
易
\\
\hline
\sphinxAtStartPar
控制流实现方式
&
\sphinxAtStartPar
特定的语法
&
\sphinxAtStartPar
前端语言语法
\\
\hline
\sphinxAtStartPar
性能
&
\sphinxAtStartPar
优化策略多，性能更佳
&
\sphinxAtStartPar
图优化受限，性能较差
\\
\hline
\sphinxAtStartPar
内存占用
&
\sphinxAtStartPar
内存占用少
&
\sphinxAtStartPar
内存占用相对较多
\\
\hline
\sphinxAtStartPar
内存占用
&
\sphinxAtStartPar
可直接部署
&
\sphinxAtStartPar
不可直接部署
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
针对两种模式的特性，结合任务需求选择合适的模式可以事半功倍，学术科研以及模型开发调试阶段，为了快速验证思想和迭代更新模型结构可以选择动态图模式进行构建算法；网络模型确定，为了加速训练过程或者为硬件部署模型，可以选择静态图模式。


\subsection{动态图与静态图的转换和融合}
\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:id5}}
\sphinxAtStartPar
动态图模式下拥有简洁的接口和编程体验，具备友好的调试交互机制。代码按照编写顺序即时执行，符合我们在编写模型的直观感受和习惯。可以快速将算法思想转化为实际代码。静态图模式下可以分离前后端语言，编译解析前端语言构建的整体网络结构，并进行优化后以高效后端语言执行，可以直接用于部署。为了兼顾动态图易用性和静态图部署性能两方面优势，目前TensorFlow、MindSpore、PyTorch、PaddlePaddle等主流计算框架均具备动态图转静态图的功能，支持使用动态图编写代码，框架自动转换为静态图网络结构。

\sphinxAtStartPar
动态图转换为静态图的实现方式有两种：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{基于追踪转换}：以动态图模式执行并记录调度的算子，构建和保存为静态图模型。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{基于源码转换}：分析前端代码来将动态图代码自动转写为静态图代码，并在底层自动帮用户使用静态图执行器运行。

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{基于追踪转换}的原理相对简单，当使用动态图模式构建好网络后，使用追踪（Tracing）进行转换将分为两个阶段。第一个阶段计算框架会创建一个新的计算图，此时以动态图模式执行代码，计算框架会自动追踪数据流的流动以及算子的调度，将所有的操作捕获并根据调度顺序构建静态图模型。第二个阶段，当执行完一次动态图后，计算框架已生成静态图，当再次调用相同的模型时，计算框架会自动指向静态图模型，以高效的性能执行计算。追踪技术只是记录第一次执行动态图时调度的算子，但若是模型中存在依赖于中间结果的条件分支控制流，只能追踪到根据第一次执行时触发的分支。此时构建的静态图模型并不是完整的，缺失了数据未流向的其他分支。在后续的调用中，因为静态模型已无法再改变，若计算过程中数据流向缺失分支会导致模型运行错误。同样的，依赖于中间数据结果的循环控制也无法追踪到全部的迭代状态。

\sphinxAtStartPar
动态图基于前端语言自身的解释器进行模型代码的解析执行。比如当Python作为前端语言，采取原生Python边运行边解释的特性，配合框架提供的数据处理/算子分发的功能计算，即可实现动态图的即时执行特性。而且静态图则采用计算框架自带的图编译器，对神经网络模型进行建图后，再调用图结构进行计算。动态图代码与静态图代码之间存在差异，不能直接使用静态图编译器，因此基于源码转换的方法需要将动态图代码转换为静态图代码描述。

\sphinxAtStartPar
\sphinxstylestrong{基于源码转换}的方式则能够改善基于追踪转换的缺陷。如
\hyperref[\detokenize{chapter_computational_graph/generation_of_computational_graph:ast}]{图\ref{\detokenize{chapter_computational_graph/generation_of_computational_graph:ast}}}中所示，基于源码转换的流程经历两个阶段。第一个阶段，对动态图模式下的代码扫描进行词法分析，通过词法分析器分析源代码中的所有字符，对代码进行分割并移除空白符、注释等，将所有的单词或字符都转化成符合规范的语法单元列表。接着进行语法分析即解析器，将得到的语法单元列表转换成树形式，并对语法进行检查避免错误。第二阶段，动态图转静态图的核心部分就是对抽象语法树进行转写，计算框架中对每一个需要转换的语法都预设有转换器，每一个转换器对语法树进行扫描改写，将动态图代码语法映射为静态图代码语法。其中最为重要的前端语言控制流，会在这一阶段分析转换为静态图接口进行实现。转写完毕之后，将新的语法树再还原回静态图代码，就可以使用静态生成执行。使用该方式可以避免基于追踪转换中控制流表达缺失的情况。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ast}.svg}
\caption{基于源码转换流程}\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:id11}}\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:ast}}\end{figure}

\sphinxAtStartPar
在使用上述功能的过程中，可以将整体模型动态图代码全部转换为静态图代码，提高计算效率并用于硬件部署。同时也可以将整体模型中的部分函数转化为局部静态子图，静态子图会被计算框架视为一个完整的算子并嵌入动态图中。执行整体动态图时，当计算到对应的函数会自动调用静态子图。使用该方式在一定程度上既保留代码调试改进的灵活性，又提高了计算效率。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@ms\PYGZus{}function} \PYG{c+c1}{\PYGZsh{}mindspore中基于源码转换的函数装饰器，可以将该函数转换为静态图}
\PYG{k}{def} \PYG{n+nf}{add\PYGZus{}and\PYGZus{}relu}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{Y} \PYG{o}{=} \PYG{n}{Y} \PYG{o}{+} \PYG{n}{b}
    \PYG{n}{Y} \PYG{o}{=} \PYG{n}{relu}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{Y}

\PYG{k}{def} \PYG{n+nf}{model}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{flag}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{flag}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{n}{Y} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{W1}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{Y} \PYG{o}{=} \PYG{n}{matmul}\PYG{p}{(}\PYG{n}{W2}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}
        \PYG{n}{Y} \PYG{o}{=} \PYG{n}{add\PYGZus{}and\PYGZus{}relu}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{Y}
\end{sphinxVerbatim}

\sphinxAtStartPar
代码中模型整体可以采用动态生成，而@ms\_function可以使用基于源码转换的技术将模块\sphinxstyleemphasis{add\_and\_relu}的转化为静态图结构。与动态生成中代码执行相同，模型接受输入按照模型定义的计算顺序进行调度执行，并生成临时图结构，当执行语句\sphinxstyleemphasis{Y=add\_and\_relu(Y,b)}
时，计算框架会自动调用该模块静态生成的图结构执行计算。模块\sphinxstyleemphasis{add\_and\_relu}
可以利用静态图中的优化技术来提高计算性能，实现动态图和静态图的混合执行。此外，动静态转换的技术常用于模型部署阶段，动态图预测部署时除了需要已经训练完成的参数文件，还须提供最初的模型组网前端代码，这使得动态图部署受到局限性，部署硬件中往往难以提供支持前端语言执行环境。因此当使用动态图模式训练完成模型参数后，可以将整体网络结构转换为静态图格式，将神经网络模型和参数文件进行序列化保存，与前端代码完全解耦，扩大模型部署的硬件支持范围。

\sphinxAtStartPar
主流的计算框架TensorFlow、MindSpore等均提供动静态相互转换与融合执行的技术，我们将各框架中支持源码转换和追踪转换技术的接口梳理如
\hyperref[\detokenize{chapter_computational_graph/generation_of_computational_graph:dynamic-static-switch}]{表\ref{\detokenize{chapter_computational_graph/generation_of_computational_graph:dynamic-static-switch}}}所示。可以通过\sphinxhref{https://github.com/openmlsys/openmlsys-pytorch/blob/master/chapter\_computational\_graph/generate\_static\_graph.py}{代码示例}%
\begin{footnote}[6]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/openmlsys/openmlsys-pytorch/blob/master/chapter\_computational\_graph/generate\_static\_graph.py}
%
\end{footnote}查看PyTorch计算框架中是如何将动态图模型转化为静态图模型，并且展示静态图结构信息。


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{主流框架动态图转换静态图支持}\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:id12}}\label{\detokenize{chapter_computational_graph/generation_of_computational_graph:dynamic-static-switch}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
框架
&\sphinxstyletheadfamily 
\sphinxAtStartPar
动态图转静态图
\\
\hline
\sphinxAtStartPar
TensorFlow
&
\sphinxAtStartPar
@tf\_function追踪算子调度构建静态
图，其中AutoGraph机制可以自动转换控制流为静态表达
\\
\hline
\sphinxAtStartPar
MindSpore
&
\sphinxAtStartPar
contex
t.set\_context(mode=context.PYNATIVE\_MODE)动态图模
式，context.set\_context(mode=context.GRAPH\_MODE)
静态图模式，@ms\_function支持基于源码转换
\\
\hline
\sphinxAtStartPar
PyTorch
&
\sphinxAtStartPar
torch.jit.script()支
持基于源码转换，torch.jit.trace()支持基于追踪转换
\\
\hline
\sphinxAtStartPar
PaddlePaddle
&
\sphinxAtStartPar
paddle.jit.to\_static()支持基于源码转换
，paddle.jit.TracedLayer.trace()支持基于追踪转换
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\section{计算图的调度}
\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:id1}}\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph::doc}}
\sphinxAtStartPar
模型训练就是计算图调度图中算子的执行过程。宏观来看训练任务是由设定好的训练迭代次数来循环执行计算图，此时我们需要优化迭代训练计算图过程中数据流载入和模型训练（推理）等多个任务之间的调度执行。微观上单次迭代需要考虑计算图内部的调度执行问题，根据计算图、计算依赖关系、计算控制分析算子的任务调度队列。优化计算图的调度和执行性能，目的是为了尽可能充分利用计算资源，提高计算效率，缩短模型训练和推理时间。接下来会详细介绍计算图的调度和执行。


\subsection{算子调度执行}
\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:id2}}
\sphinxAtStartPar
算子的执行调度包含两个步骤，第一个，根据拓扑排序算法，将计算图进行拓扑排序得到线性的算子调度序列；第二步，将序列中的算子分配到执行流进行运算。算子调度执行的目标是根据计算图中算子依赖关系，确定算子调度序列，尽可能将序列中的算子并行执行，提高计算资源的利用率。

\sphinxAtStartPar
计算图中依赖边和算子构成了一张有向无环图(Directed Acyclic
Graph)，计算框架后端需要将包含这种依赖关系的算子准确地发送到计算资源，比如GPU、NPU上执行。因此，就要求算子需要按照一定的顺序排列好再发送给GPU/NPU执行。针对有向无环图，我们通常使用拓扑排序来得到一串线性的序列。

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_computational_graph/schedule_of_computational_graph:schedule}]{图\ref{\detokenize{chapter_computational_graph/schedule_of_computational_graph:schedule}}}所示，左边是一张有向无环图。图中包含了a,b,c,d,e五个节点和a\sphinxhyphen{}>d,b\sphinxhyphen{}>c,c\sphinxhyphen{}>d,d\sphinxhyphen{}>e四条边(a\sphinxhyphen{}>d表示d依赖于a，称之为依赖边)。将图的依赖边表达成节点的入度(图论中通常指有向图中某点作为图中边的终点的次数之和)，可以得到各个节点的入度信息(a:0,
b:0, c:1, d:2,
e:1)。拓扑排序就是不断循环将入度为0的节点取出放入队列中，直至所有有向无环图中的节点都加入到队列中，循环结束。例如，第一步将入度为0的a,b节点放入到队列中，此时有向无环图中c,d的入度需要减1，得到新的入度信息(c:0,
d:1, e:1)。以此类推，将所有的将所有的节点都放入到队列中并结束排序。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{schedule}.svg}
\caption{算子调度执行}\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:id5}}\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:schedule}}\end{figure}

\sphinxAtStartPar
生成调度序列之后，需要将序列中的算子与数据分发到指定的GPU/NPU上执行运算。根据算子依赖关系和计算设备数量，可以将无相互依赖关系的算子分发到不同的计算设备，同时执行运算，这一过程称之为并行计算，与之相对应的按照序贯顺序在同一设备执行运算被称之为串行计算。在深度学习中，当数据集和参数量的规模越来越大，我们在分发数据与算子时通信消耗会随之而增加，计算设备会在数据传输的过程中处于闲置状态，此时采用同步与异步的任务调度机制可以更好的协调通信与训练任务，提高通信模块与计算设备的使用率，在后续的小节中将详细介绍串行与并行、同步与异步的概念。


\subsection{串行与并行}
\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:id3}}
\sphinxAtStartPar
根据任务队列的执行顺序，我们可以将计算图的任务调度队列分为以下两种：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{串行}：队列中的任务必须按照顺序进行调度执行直至队列结束；

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{并行}：队列中的任务可以同时进行调度执行，加快执行效率。

\end{itemize}

\sphinxAtStartPar
首先我们从微观上来分析计算图内部的串行调度。计算图中大多数算子之间存在直接依赖或者间接依赖关系，具有依赖关系的算子间任务调度则必定存在执行前后的时间顺序。如
\hyperref[\detokenize{chapter_computational_graph/schedule_of_computational_graph:order}]{图\ref{\detokenize{chapter_computational_graph/schedule_of_computational_graph:order}}}，计算图接受输入数据进行前向计算得到预测值，计算损失函数进行反向梯度计算，整体代码流程后序算子的计算有赖于前序算子的输出。此时算子的执行队列只能以串行的方式进行调度，保证算子都能正确接受到输入数据，才能完成计算图的一次完整执行。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{order}.svg}
\caption{算子的串行}\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:id6}}\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:order}}\end{figure}

\sphinxAtStartPar
宏观上来看迭代训练之间，每一轮迭代中计算图必须读取训练数据，执行完整的前向计算和反向梯度计算，将图中所有参数值更新完毕后，才能开始下一轮的计算图迭代计算更新。所以“数据载入\sphinxhyphen{}数据处理\sphinxhyphen{}模型训练”的计算图整体任务调度是以串行方式进行的。

\sphinxAtStartPar
在分析计算图内部算子依赖关系时，除了直接依赖和间接依赖之外，存在算子间相互独立的情况。如
\hyperref[\detokenize{chapter_computational_graph/schedule_of_computational_graph:para}]{图\ref{\detokenize{chapter_computational_graph/schedule_of_computational_graph:para}}}中op1和op2之间相互独立，此时可以将两个算子分配到两个硬件上进行并行计算。对比串行执行，并行计算可以同时利用更多的计算资源来缩短执行时间。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{para}.svg}
\caption{算子的并行}\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:id7}}\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:para}}\end{figure}

\sphinxAtStartPar
并行包括算子并行、模型并行以及数据并行。算子并行不仅可以在相互独立的算子间执行，同时也可以将单个算子合理的切分为相互独立的两个子操作，进一步提高并行性。模型并行就是将整体计算图进行合理的切分，分配到不同设备上进行并行计算，缩短单次计算图迭代训练时间。数据并行则同时以不同的数据训练多个相同结构的计算图，缩短训练迭代次数，加快训练效率。这三种并行方式将在后续章节中进行详细讲解。


\subsection{数据载入同步与异步机制}
\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:id4}}
\sphinxAtStartPar
一次完整计算图的训练执行过程包含：数据载入、数据预处理、网络训练三个环节。三个环节之间的任务调度是以串行方式进行，每一个环节都有赖于前一个环节的输出。但计算图的训练是多轮迭代的过程，多轮训练之间的三个环节可以用同步与异步两种机制来进行调度执行。
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{同步}：顺序执行任务，当前任务执行完后会等待后续任务执行情况，任务之间需要等待、协调运行；

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{异步}：当前任务完成后，不需要等待后续任务的执行情况，可继续执行当前任务下一轮迭代。

\end{itemize}

\sphinxAtStartPar
以同步机制来执行计算图训练时，如
\hyperref[\detokenize{chapter_computational_graph/schedule_of_computational_graph:synchronization}]{图\ref{\detokenize{chapter_computational_graph/schedule_of_computational_graph:synchronization}}}所示，每一轮迭代中，数据读取后进行数据预处理操作，然后传输给计算图进行训练。每一个环节执行完当前迭代中的任务后，会一直等待后续环节的处理，直至计算图完成一次迭代训练更新参数值后，才会进行下一轮迭代的数据读取、数据处理以及网络训练。当进行数据载入时，数据处理、模型训练处于等待的状态，相反模型处于训练时，数据载入的I/O通道处于空闲，同步机制造成计算资源和通信资源的浪费。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{synchronization}.svg}
\caption{同步机制}\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:id8}}\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:synchronization}}\end{figure}

\sphinxAtStartPar
以异步机制来执行计算图训练时，如
\hyperref[\detokenize{chapter_computational_graph/schedule_of_computational_graph:asynchronous}]{图\ref{\detokenize{chapter_computational_graph/schedule_of_computational_graph:asynchronous}}}所示，在迭代训练中，当数据通道将数据读取后交给后续的数据与处理环节后，不需要等待计算图训练迭代完成，直接读取下一批次的数据。对比同步机制，异步机制的引入减少了数据载入、数据预处理、网络训练三个环节的空闲等待时间，能够大幅度缩短循环训练的整体时间，提高任务执行效率。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{asynchronous}.svg}
\caption{异步机制}\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:id9}}\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:asynchronous}}\end{figure}

\sphinxAtStartPar
当我们将异步机制与并行计算结合在一起，如
\hyperref[\detokenize{chapter_computational_graph/schedule_of_computational_graph:asyn-para}]{图\ref{\detokenize{chapter_computational_graph/schedule_of_computational_graph:asyn-para}}}所示，利用丰富的计算资源可以进一步提高计算图训练效率，缩短训练时间。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{asyn_para}.svg}
\caption{异步并行}\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:id10}}\label{\detokenize{chapter_computational_graph/schedule_of_computational_graph:asyn-para}}\end{figure}


\section{总结}
\label{\detokenize{chapter_computational_graph/summary:id1}}\label{\detokenize{chapter_computational_graph/summary::doc}}\begin{itemize}
\item {} 
\sphinxAtStartPar
为了兼顾编程的灵活性和计算的高效性，设计了基于计算图的深度学习框架。

\item {} 
\sphinxAtStartPar
计算图的基本数据结构是张量，基本运算单元是算子。

\item {} 
\sphinxAtStartPar
计算图可以表示机器学习模型的计算逻辑和状态，利用计算图分析图结构并进行优化。

\item {} 
\sphinxAtStartPar
计算图是一个有向无环图，图中算子间可以存在直接依赖和间接依赖关系，或者相互关系独立，但不可以出现循环依赖关系。

\item {} 
\sphinxAtStartPar
可以利用控制流来改变数据在计算图中的流向，常用的控制流包括条件控制和循环控制。

\item {} 
\sphinxAtStartPar
计算图的生成可以分为静态生成和动态生成两种方式。

\item {} 
\sphinxAtStartPar
静态图计算效率高，内存使用效率高，但调试性能较差，可以直接用于模型部署。

\item {} 
\sphinxAtStartPar
动态图提供灵活的可编程性和可调试性，可实时得到计算结果，在模型调优与算法改进迭代方面具有优势。

\item {} 
\sphinxAtStartPar
利用计算图和算子间依赖关系可以解决模型中的算子执行调度问题。

\item {} 
\sphinxAtStartPar
根据计算图可以找到相互独立的算子进行并发调度，提高计算的并行性。而存在依赖关系的算子则必须依次调度执行。

\item {} 
\sphinxAtStartPar
计算图的训练任务可以使用同步或者异步机制，异步能够有效提高硬件使用率，缩短训练时间。

\end{itemize}


\section{扩展阅读}
\label{\detokenize{chapter_computational_graph/summary:id2}}\begin{itemize}
\item {} 
\sphinxAtStartPar
计算图是计算框架的核心理念之一，了解主流计算框架的设计思想，有助于深入掌握这一概念，建议阅读
\sphinxhref{https://arxiv.org/abs/1603.04467}{TensorFlow 设计白皮书}%
\begin{footnote}[7]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/1603.04467}
%
\end{footnote}、
\sphinxhref{https://arxiv.org/abs/1912.01703}{PyTorch计算框架设计论文}%
\begin{footnote}[8]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/1912.01703}
%
\end{footnote}、\sphinxhref{https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/white\_paper/MindSpore\_white\_paperV1.1.pdf}{MindSpore技术白皮书}%
\begin{footnote}[9]\sphinxAtStartFootnote
\sphinxnolinkurl{https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/white\_paper/MindSpore\_white\_paperV1.1.pdf}
%
\end{footnote}。

\item {} 
\sphinxAtStartPar
图外控制流直接使用前端语言控制流，熟悉编程语言即可掌握这一方法，而图内控制流则相对较为复杂，建议阅读\sphinxhref{http://download.tensorflow.org/paper/white\_paper\_tf\_control\_flow\_implementation\_2017\_11\_1.pdf}{TensorFlow控制流}%
\begin{footnote}[10]\sphinxAtStartFootnote
\sphinxnolinkurl{http://download.tensorflow.org/paper/white\_paper\_tf\_control\_flow\_implementation\_2017\_11\_1.pdf}
%
\end{footnote}论文。

\item {} 
\sphinxAtStartPar
动态图和静态图设计理念与实践，建议阅读\sphinxhref{https://arxiv.org/pdf/1903.01855.pdf}{TensorFlow Eager
论文}%
\begin{footnote}[11]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/pdf/1903.01855.pdf}
%
\end{footnote}、\sphinxhref{https://tensorflow.google.cn/guide/eager?hl=zh-cn}{TensorFlow Eager
Execution}%
\begin{footnote}[12]\sphinxAtStartFootnote
\sphinxnolinkurl{https://tensorflow.google.cn/guide/eager?hl=zh-cn}
%
\end{footnote}示例、\sphinxhref{https://tensorflow.google.cn/guide/intro\_to\_graphs?hl=zh-cn}{TensorFlow
Graph}%
\begin{footnote}[13]\sphinxAtStartFootnote
\sphinxnolinkurl{https://tensorflow.google.cn/guide/intro\_to\_graphs?hl=zh-cn}
%
\end{footnote}理念与实践、\sphinxhref{https://www.mindspore.cn/docs/programming\_guide/zh-CN/r1.6/design/dynamic\_graph\_and\_static\_graph.html}{MindSpore动静态图}%
\begin{footnote}[14]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.mindspore.cn/docs/programming\_guide/zh-CN/r1.6/design/dynamic\_graph\_and\_static\_graph.html}
%
\end{footnote}概念。

\end{itemize}


\chapter{第二部分：进阶篇}
\label{\detokenize{chapter_preface_advanced/index:id1}}\label{\detokenize{chapter_preface_advanced/index::doc}}
\sphinxAtStartPar
第一部分基础篇介绍了机器学习系统的基础，把机器学习系统作为一个黑盒，介绍了机器学习系统的使用场景和主要需求，提供给用户的编程接口。介绍了典型场景下使用机器学习系统进行计算图构建、生成和调度的流程和关键技术。第二部分进阶篇，将站在系统设计的角度，思考在设计现代机器学习系统中需要考虑的问题和思路，详细的技术方案介绍在后面的章节会具体展开，本节重点介绍机器学习系统的总体架构以及进阶篇各章节之间的关系。
机器学习系统面临的挑战可以从如下四个方面的变化趋势来看，如图所示，给出了AI框架系统主要的干系人，横向表示的是算法开发者需要使用AI框架系统进行算法研究，生产到部署的场景，纵向表示的是AI系统支撑的模型算法应用以及支撑AI系统运行的AI硬件：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
面向算法开发者：怎样提高算法开发的效率并兼顾运行性能？怎样让算法开发者减少学习成本随心所欲的表达算法思想？等一系列问题是机器学习系统需要考虑的问题。按照构建计算图的方式，AI框架分为了静态图和动态图两种类型：静态图在执行前先进行构图和编译优化，需要使用系统提供的构图API进行构图，其表达受限于API的灵活和丰富程度，但由于静态图先编译后执行，执行期性能较高；动态图则是边执行边构图，符合算法开发人员的使用习惯，由于直接使用Python生态，可以做到会Python编程语言就会写算法；动态图逐语句执行，调试方便，算法开发效率很高，但动态图的执行效率一般情况下不如静态图。机器学习系统就需要如何支持静态图和动态图的策略和方案。

\item {} 
\sphinxAtStartPar
面向部署：怎样将AI模型算法部署到每个设备、每个应用、每个行业？由于各个设备的算力和资源属性差别巨大，尤其是端侧和边缘侧的某些设备硬件资源极其受限，机器学习系统本身和模型大小和能效比有严苛的要求，以智能手表为例，其要求框架严格限制在100KB以内，能效/功耗比要能支持较长的待机时间要求。而另一方面，AI模型越来越大，即使部署在端侧的模型也动辄几十MB，占用大量的内存、计算和通信资源。因此机器学习系统需要解决跨系统、跨设备、极致轻量化部署的问题。

\item {} 
\sphinxAtStartPar
面向算法和数据：从计算规模看，AI模型规模呈现指数级增长，2021年模型规模已经达到十万亿参数，预计很快会增长到百万亿参数，怎么应对模型越来越大的挑战；从计算范式看，如何处理不断涌现的新的计算范式。机器学习系统过去是由机器学习、深度学习为主的，未来会不会支持其他的负载，比如：科学计算、数据分析、图形计算等，这样可以最大化的利用资源和提高开发易用性。

\item {} 
\sphinxAtStartPar
面向硬件：如果把AI加速器按照GPU、多核/众核、DataFlow架构来进行分类的话，DataFlow的架构占比一半左右，Dataflow架构虽然在能效比上有很大的优势，但是加大了可编程性和性能优化的难度，如何充分发挥芯片的性能进行极致的性能优化？

\end{enumerate}

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{preface3_1}.png}
\caption{影响机器学习系统架构的因素}\label{\detokenize{chapter_preface_advanced/index:id2}}\end{figure}

\sphinxAtStartPar
​
上述四个方面的不同需求，在设计AI框架系统的时候，需要基于场景充分的予以考虑，另外还需要考虑：

\sphinxAtStartPar
通用性：也可以称为泛化性，是不是所有的模型算法同一套代码，没有针对某个网络的特殊定制代码？
是不是所有硬件同一套机制，在机器学习系统中针对特定硬件版本的定制只存在于硬件相关层？上面提到的不同环境下部署要求千差万别，是同一套方案还是几套方案来支持呢？

\sphinxAtStartPar
易用性：对新用户而言，易用性关注更多的是入门的门槛，能不能一键式的安装、升级和运行常见的模型；对深度用户，如：算法研究人员而言，是不是能够轻松的表达算法、调试算法和部署算法模型是易用性的重点。另外，生态兼容性是易用性的一个重要考量，方便的使用常用的工具、第三方库兼容和对接，支持更多的硬件进行训练和部署是重要的因素。

\sphinxAtStartPar
性能：追求性价比、性能功耗比、大规模集群的性能加速比是机器学习系统性能的主要衡量指标，MLPerf、AIPerf榜单主要比拼的是典型网络在不同系统上的综合性能指标，机器学习系统中提供的系统优化机制对性能起到了重要作用。

\sphinxAtStartPar
​
在机器学习系统的架构设计中，采用三层架构来应对这些挑战和需要考虑的问题。
下图是MindSpore的系统架构图，就是以AI编译器为核心的三层架构设计。 ​
第一层MindExpression表示层，提供灵活的模型/算法表示，考虑到生态兼容性，采用Python作为主要的编程语言，后续也可以提供其他编程语言的扩展如：Julia、仓颉等编程语言。支持常见的第三库如：Numpy、Scipy等，数据处理支持常用的数据处理库，如：OpenCV的API等；
​
第二层MindCompiler编译优化层，负责机器学习系统的编译优化，包括自动微分、硬件无关的优化，硬件相关的优化，分布式系统相关的优化等都在AI编译器层完成，经过这一层后，用户表达的算法和模型转换成特定硬件上的高效执行的机器代码；前端负责静态分析、类型推导以及自动微分、分布式并行子图拆分等PASS优化；后端负责硬件相关的优化，如：内存优化、图算融合等。在第二层可以看到有MindData数据处理框架，主要是因为在系统中所处的层次和MindCompiler相同，关系也比较密切。数据处理框架主要是提供数据处理格式、数据处理加速和数据处理相关的保序、分布式缓存等操作。由于数据处理和AI训练是在同一套硬件集群系统中，需要根据训练和数据处理的负载消耗情况来进行资源分配，有时候也会把数据处理相关操作在AI计算中构成一张DAG图进行调度。
​
第三层MindRT运行时，提供不同部署环境下通用的分布式异构并行调度机制、内存分配等，对于不同的硬件使用不同的backend的设计方式。详细的介绍在后面的章节中会展开讨论，在本部分重点介绍设计思想和章节之间的关系。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{preface3_arc}.png}
\caption{MindSpore系统架构图}\label{\detokenize{chapter_preface_advanced/index:id3}}\end{figure}

\sphinxAtStartPar
​ 既要对上承接模型算法的变化，满足算法开发者研究不断探索的诉求，
又要在最终的二进制输出上满足多样性硬件的诉求，满足不同部署环境的资源要求。既要满足系统的通用，也要满足易用性的灵活性要求，还要满足性能的不断优化诉求，这里引入编译器的概念再合适不过了。
编译器概念可以很好抽象上面提到的挑战和问题，编译器输入的是用户编程代码，输出的是机器执行的高效代码，编译器的作用主要是转换和优化，这和机器学习系统的输入输出，机器学习系统的目标是完全一致的。所以在进阶篇我们将用两个章节详细介绍AI编译器，里面的很多概念是和通用编译器中的概念是相同的，比如AOT（Ahead
of Time提前编译）、JIT（Just in
time）、IR（中间表示）、PASS优化、AST（Abstract Struct
Trees）、副作用、闭包等概念和编译器中相关定义相同，对编译器相关概念需要了解的读者可以翻阅相关的编译原理教材了解。

\sphinxAtStartPar
​
AI编译器是一种相对较新的概念和工具，一个强大的AI编译器将让算法科学家和开发人员享受其带来的益处，包括表达的便捷和执行的高性能，是AI框架设计的核心。为了更好的理解AI编译器架构，先从如下图传统编译器LLVM架构说起，在这里可以把LLVM编译器分成三个部分：前端、IR和后端。前端将高级语言转换成IR，后端将IR转换成目标硬件上的机器指令，IR作为桥梁在前后端之间进行基于IR的各种优化。这样无论是新增硬件的支持，还是新的前端支持都可以尽可能的复用IR相关的部分。IR可以是单层的IR，也可以是多层的IR，
LLVM IR是典型的单层的IR，前后端优化都基于相同的LLVM IR进行。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{preface3_2}.png}
\caption{LLVM IR的架构}\label{\detokenize{chapter_preface_advanced/index:id4}}\end{figure}

\sphinxAtStartPar
​
在AI编译器领域往往采用多层级逐步优化的方式，对应的IR也是多层级IR的。TensorFlow等AI框架就是采用多层级IR来进行设计的，MindSpore早期版本也是使用多层次IR的，下图就是MLIR官方材料中给出的TensorFlow框架当前的IR现状，中间至少有三个层次的IR，即：TensorFlow
Graph IR， XLA （Accelerated Linear Algebra）HLO、 以及特定硬件的LLVM IR
或者TPU
IR，下面就不同的层级IR和其上的编译优化做一个简要介绍。在前一章节计算图部分提到的图优化，也称为：图编译优化，主要实现整图级别的优化和操作，如：
图优化、图切分，往往是基于Graph
IR这个层次进行的，比较适合静态图的执行模式；由于整图级别的IR缺少相应的硬件信息，难以进行硬件相关的优化，所以在中间层次就出现了相关的硬件通用编译优化，如：XLA、TensorRT、MindSpore的图算融合等都属于这一类编译器，来针对不同的硬件进行算子融合等优化，提升不同网络在特定硬件上的执行性能。在本书的第五章节编译器后端中硬件通用优化中有一个小节来介绍图算融合编译器的相关设计。最后一个层次的IR是特定硬件加速器专有的IR，一般由硬件厂商自带的编译器提供，如：Ascend硬件自带的TBE编译器就是基于TVM
IR生成高效的执行算子，在本书的第6章加速器章节也会有相应部分来介绍算子编译器。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{preface3_3}.png}
\caption{MLIR架构}\label{\detokenize{chapter_preface_advanced/index:id5}}\end{figure}

\sphinxAtStartPar
​
多层级IR的优势是IR表达上更加地灵活，可以在不同层级的IR上进行合适的PASS优化，更加方便，优化算法也更加地高效；但是多层级IR也有一些劣势：首先多层级IR需要进行不同IR之间的转换，IR转换要做到完全兼容非常困难，工程工作量很大；IR的转换除了工程上的工作量比较大以外，可能会带来信息的损失，
由于上一层IR的优化掉某些信息，给下一层优化带来困难，考虑到优化会造成信息丢失的影响，因此对优化执行的顺序也有更强的约束。其次多层级IR有些优化既可以在上一层IR进行，也可以在下一层IR进行，让系统开发者很难选择。最后不同层级IR定义的算子粒度大小不同，可能给精度带来一定的影响。所以在MindSpore的架构设计中采用了统一的MindIR设计，如下图所示详细给出了AI编译器内部的运行流程和分工，在本书中编译器前端主要是指图编译和硬件无关的优化，编译器后端主要是指硬件相关优化、算子选择等。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{preface3_4}.png}
\caption{MindSpore AI编译器处理流程}\label{\detokenize{chapter_preface_advanced/index:id6}}\end{figure}

\sphinxAtStartPar
​
性能在AI领域有一个很重要的需求就是要尽可能的充分利用AI硬件的设计，发挥AI硬件的算力。当前AI硬件除了常见的CPU、GPU外，各种DSA架构的加速器硬件如：华为推出的Ascend昇腾AI加速器是专门针对神经网络训练和推理进行设计的加速器。在第6章加速器部分，首先对加速器硬件的架构进行介绍，并站在系统编程的角度，介绍了加速器的编程方式，掌握这部分的内容将可以更好的在机器学习系统中使用加速器。

\sphinxAtStartPar
数据是机器学习的生命线，据统计，在一次训练计算中有30\%左右的计算时间用于数据处理，如果算上一个完整的机器学习模型训练，包括从公开的数据集中预处理数据等占比时间比例更高。由于原始数据的类型千变万化，模型中对数据处理的操作灵活多样，机器学习系统中数据处理功能需要提供灵活的数据处理能力，包括数据处理接口的设计和自定义算子的能力，也需要高性能的数据处理，包括数据处理加速和分布式数据处理等，在本书第7章节数据处理框架部分进行了介绍。

\sphinxAtStartPar
训练后的模型部署到各种形态的端边云设备中，将智慧赋能到各行各业。当前AI部署到企业的比例还很低，有报告统计还不到20\%，模型部署主要存在着两大方面的挑战：一方面资源受限设备的CPU处理能力不强，内存和硬盘空间都很小，同时对模型执行的功耗、精度和时延开销又有严苛的指标要求，在此场景下，模型部署遇到了性能、功耗和大小的挑战；另一方面超大模型部署对算力消耗较大，需要进行分布式部署。而很多大企业往往采用统一的模型部署的云服务，需要一套服务接口提供对外服务，机器学习系统需要能适配不同的模型，不同的数据处理方法，对接不同的硬件后端，提升云端部署的效率、成本和吞吐率。本书第8章节模型部署将从模型部署的一般流程出发，详细的介绍在模型训练完以后，部署到手机等设备上需要进行的模型转换、模型优化和模型压缩的过程和相关技术。对于分布式场景下的模型部署本章也进行了相关的介绍。值得说明的是，模型在部署前可以进行算子融合、代数化简以及硬件相关的优化，如果共享AI编译器相关的基础设施，采用和训练相同的IR，不仅可以很好的复用部署和训练的算子，还由于算子定义粒度相一致，可以减少训练到模型部署的精度损失。

\sphinxAtStartPar
机器学习系统中单机系统包括编译优化、加速器加速、模型部署和数据处理执行流程，如下图所示。随着模型的越来越大，AI训练单机系统已经不能满足需求，大型模型的训练需要使用到AI集群来进行训练，2021年鹏城实验室联合华为发布盘古大模型，就提到使用了2048个训练节点的超大规模的昇腾计算集群。在超大模型训练中，如果Scale
out模型训练到集群上，需要机器学习系统支持数据并行、模型并行、流水线并行、优化器并行的能力，同时允许多种策略灵活的组合使用。由于大规模集群的系统配置的复杂度很高，无论对于算法人员还是系统人员，配置这些策略的门槛都比较高，需要机器学习系统提供半自动和自动的策略支持，适配不同模型、不同规模的集群的灵活的分布式并行策略，这些技术将在第9章节分布式训练进行介绍。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{preface3_5}.png}
\caption{AI训练部署的过程}\label{\detokenize{chapter_preface_advanced/index:id7}}\end{figure}


\chapter{编译器前端}
\label{\detokenize{chapter_frontend_and_ir/index:id1}}\label{\detokenize{chapter_frontend_and_ir/index::doc}}
\sphinxAtStartPar
在上一章节中，我们详细讨论了计算图的生成和调度，在进阶部分的介绍中简单介绍了深度学习编译器的作用。定义深度学习模型、计算图使用系统为用户提供的高级编程API，我们将用户使用高级编程API编写的程序称为源程序，将与硬件相关的程序称为目标程序，深度学习编译器需要理解输入的源程序并将其映射到目标机。为了实现这两项任务，编译器的设计被分解为两个主要部分：前端和后端。传统编译器的前端专注于理解源程序，后端则专注于将功能映射到目标机。为了将前后端相连接，我们需要一种结构来表示转换后的源代码，这就是中间表示（Intermediate
Representation, IR）。

\sphinxAtStartPar
\hyperref[\detokenize{chapter_frontend_and_ir/index:compiler-frontend-structure}]{图\ref{\detokenize{chapter_frontend_and_ir/index:compiler-frontend-structure}}}展示了机器学习编译器的前端的流程。其中，对源程序的解析过程与传统编译器是大致相同的，本章节不对这部分进行更细致的讨论。机器学习框架的编译器前端的独特之处主要在于自动微分功能的支持。为了满足自动微分功能带来的新需求，机器学习框架需要在传统中间表示的基础上设计新的中间表示结构。因此，本章节的介绍重点会放在中间表示以及自动微分这两个部分。最后，我们会简要探讨类型系统，静态分析和前端优化等编译器基础概念。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=1000\sphinxpxdimen]{{编译器前端基础架构}.svg}
\caption{编译器前端基础结构}\label{\detokenize{chapter_frontend_and_ir/index:id2}}\label{\detokenize{chapter_frontend_and_ir/index:compiler-frontend-structure}}\end{figure}

\sphinxAtStartPar
本章的学习目标包括：
\begin{itemize}
\item {} 
\sphinxAtStartPar
理解中间表示的基础概念，特点和实现方法

\item {} 
\sphinxAtStartPar
理解自动微分的基础概念，特点和实现方法

\item {} 
\sphinxAtStartPar
了解类型系统和静态推导的基本原理

\item {} 
\sphinxAtStartPar
了解编译器优化的主要手段和常见优化方法

\end{itemize}


\section{概述}
\label{\detokenize{chapter_frontend_and_ir/overview_of_frontend:id1}}\label{\detokenize{chapter_frontend_and_ir/overview_of_frontend::doc}}
\sphinxAtStartPar
本章节将讨论重点放在中间表示与自动微分章节。自动微分作为机器学习框架的编译器的独有功能，其实现需要满足其需求的中间表示的支持。在讨论完这两部分后，我们会简要介绍类型系统，静态分析和前端编译优化等编译器基础概念。


\subsection{中间表示}
\label{\detokenize{chapter_frontend_and_ir/overview_of_frontend:id2}}
\sphinxAtStartPar
中间表示是编译器用于表示源代码的数据结构或代码，是程序编译过程中介于源语言和目标语言之间的程序表示。传统机器学习框架的中间表示分为三大类，分别是线性中间表示，图中间表示以及混合中间表示。然而，传统编译器的中间表示难以完全满足机器学习框架对于中间表示的一系列需求。因此，机器学习框架的开发者在传统中间表示的设计基础上不断扩展，提出了很多适用于机器学习框架的中间表示。


\subsection{自动微分}
\label{\detokenize{chapter_frontend_and_ir/overview_of_frontend:id3}}
\sphinxAtStartPar
自动微分（Automatic Differentiation，
AD）是一种介于符号微分和数值微分之间的针对计算图进行符号解析的求导方法，用于计算函数梯度值。深度学习等现代AI算法通过使用大量数据来学习拟合出一个优化后带参模型，其中使用的学习算法多是基于现实数据在模型中的经验误差，通过梯度下降的方法来更新模型的参数。因此，自动微分在深度学习中处于非常重要的地位，是整个训练算法的核心组件之一。自动微分通常在编译器前端优化中实现，通过对中间表示的符号解析来生成带有梯度函数的中间表示。


\subsection{类型系统与静态分析}
\label{\detokenize{chapter_frontend_and_ir/overview_of_frontend:id4}}
\sphinxAtStartPar
为了有效减少程序在运行时可能出现的错误，编译器的前端引入了类型系统（Type
System）和静态分析（Static
Analysis）系统。类型系统可以防止程序在运行时发生类型错误，而静态分析能够为编译优化提供线索和信息，有效减少代码中存在的结构性错误、安全漏洞等问题。


\subsection{前端编译优化}
\label{\detokenize{chapter_frontend_and_ir/overview_of_frontend:id5}}
\sphinxAtStartPar
编译优化意在解决代码的低效性，无论是在传统编译器还是在机器学习框架中都起着很重要的作用。前端的编译优化与硬件无关。


\section{中间表示}
\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:id1}}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation::doc}}
\sphinxAtStartPar
中间表示作为编译器的核心数据结构之一，无论是在传统编译器中，还是在机器学习框架中，
都有着极其重要的地位。本章节我们会先介绍中间表示的基本概念以及传统编译器的中间表示类型。在此基础上，我们会探讨针对机器学习框架，中间表示的设计所面临的新的需求和挑战。最后，我们会介绍现有机器学习框架的中间表示的种类及其实现。


\subsection{中间表示的基本概念}
\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:id2}}
\sphinxAtStartPar
中间表示(IR)，是编译器用于表示源代码的数据结构或代码，是程序编译过程中介于源语言和目标语言之间的程序表示。几乎所有的编译器都需要某种形式的中间表示，来对被分析、转换和优化的代码进行建模。在编译过程中，中间表示必须具备足够的表达力，在不丢失信息的情况下准确表达源代码，并且充分考虑从源代码到目标代码编译的完备性、编译优化的易用性和性能。

\sphinxAtStartPar
引入中间表示后，中间表示既能面向多个前端，表达多种源程序语言，又能对接多个后端，连接不同目标机器，如
\hyperref[\detokenize{chapter_frontend_and_ir/intermediate_representation:intermediate-representation}]{图\ref{\detokenize{chapter_frontend_and_ir/intermediate_representation:intermediate-representation}}}所示。在此基础上，编译流程就可以在前后端直接增加更多的优化流程，这些优化流程以现有IR为输入，又以新生成的IR为输出，被称为优化器。优化器负责分析并改进中间表示，极大程度的提高了编译流程的可拓展性，也降低了优化流程对前端和后端的破坏。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=850\sphinxpxdimen]{{中间表示-中间表示结构}.png}
\caption{中间表示}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:id12}}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:intermediate-representation}}\end{figure}

\sphinxAtStartPar
随着编译器技术的不断演进，中间表示主要经历了三个发展阶段。在早期阶段，中间表示是封闭在编译器内部的，供编译器编写者使用。在中期阶段，随着编译器的开源，中间表示逐步开源公开，主要供编译器设计者、分析工具设计者使用。现阶段，中间表示朝着软件生态构建的方向发展，旨在构建统一的中间表示。


\subsection{中间表示的种类}
\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:id3}}
\sphinxAtStartPar
上一节介绍了中间表示的基本概念，初步阐述了中间表示的重要作用和发展历程。接下来从组织结构的角度出发，介绍通用编译器的中间表示的类型以及各自特点
\DUrole{bibtex}{{[}2020MLIR{]}}，如下表所示。中间表示组织结构的设计，对编译阶段的分析优化、代码生成等有着重要影响。编译器的设计需求不同，采用的中间表示组织结构也有所不同。

\begin{sphinxuseclass}{tab-ch04-ch04-categorize}

\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{中间表示的分类}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:id13}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
组织结构 特点
&\sphinxstyletheadfamily 
\sphinxAtStartPar
举例
&\sphinxstyletheadfamily \\
\hline
\sphinxAtStartPar
Linear IR
&
\sphinxAtStartPar
基于线性代码 堆栈机代码、
&
\sphinxAtStartPar
三地址代码
\\
\hline
\sphinxAtStartPar
Graphical IR
&
\sphinxAtStartPar
基于图 抽象语
&
\sphinxAtStartPar
法树、有向无环图、控制流图
\\
\hline
\sphinxAtStartPar
Hybrid IR
&
\sphinxAtStartPar
基于图与线性代码混合 LLVM IR
&\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\end{sphinxuseclass}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{)}%
\item {} 
\sphinxAtStartPar
线性中间表示

\end{enumerate}

\sphinxAtStartPar
线性中间表示类似抽象机的汇编代码，将被编译代码表示为操作的有序序列，对操作序列规定了一种清晰且实用的顺序。由于大多数处理器采用线性的汇编语言，线性中间表示广泛应用于编译器设计。

\sphinxAtStartPar
常用线性中间表示有堆栈机代码(Stack\sphinxhyphen{}Machine Code)和三地址代码(Three
Address Code) \DUrole{bibtex}{{[}2007Compilers{]}}
。堆栈机代码是一种单地址代码，提供了简单紧凑的表示。堆栈机代码的指令通常只有一个操作码，其操作数存在一个栈中。大多数操作指令从栈获得操作数，并将其结果推入栈中。三地址代码，简称为3AC，模拟了现代RISC机器的指令格式。它通过一组四元组实现，每个四元组包括一个运算符和三个地址(两个操作数、一个目标)。对于表达式a\sphinxhyphen{}b*5，堆栈机代码和三地址代码如
\hyperref[\detokenize{chapter_frontend_and_ir/intermediate_representation:linear-ir}]{图\ref{\detokenize{chapter_frontend_and_ir/intermediate_representation:linear-ir}}}所示。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{中间表示-线性中间表示}.png}
\caption{堆栈机代码和三地址代码}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:id14}}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:linear-ir}}\end{figure}

\sphinxAtStartPar
2、图中间表示

\sphinxAtStartPar
图中间表示将编译过程的信息保存在图中，算法通过图中的对象如节点、边、列表、树等来表述。虽然所有的图中间表示都包含节点和边，但在抽象层次、图结构等方面各有不同。常见的图中间表示包括抽象语法树(Abstract
Syntax Tree，AST)、有向无环图(Directed Acyclic
Graph，DAG)、控制流图(Control\sphinxhyphen{}Flow Graph，CFG)等。

\sphinxAtStartPar
AST抽象语法树采用树型中间表示的形式，是一种接近源代码层次的表示。对于表达式\(a*5+a*5*b\)，其AST表示如
\hyperref[\detokenize{chapter_frontend_and_ir/intermediate_representation:ast-dag}]{图\ref{\detokenize{chapter_frontend_and_ir/intermediate_representation:ast-dag}}}所示。可以看到，AST形式包含\(a*5\)的两个不同副本，存在冗余。在AST的基础上，DAG提供了简化的表达形式，一个节点可以有多个父节点，相同子树可以重用。如果编译器能够证明\(a\)的值没有改变，则DAG可以重用子树，降低求值过程的代价。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=400\sphinxpxdimen]{{中间表示-ASTDAG}.svg}
\caption{AST图和DAG图}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:id15}}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:ast-dag}}\end{figure}

\sphinxAtStartPar
3、混合中间表示

\sphinxAtStartPar
混合中间表示是线性中间表示和图中间表示的结合，这里以LLVM IR
\DUrole{bibtex}{{[}2004LLVM{]}} 为例进行说明。LLVM(Low Level Virtual
Machine)是2000年提出的开源编译器框架项目，旨在为不同的前端后端提供统一的中间表示。LLVM
IR使用线性中间表示表示基本块，使用图中间表示表示这些块之间的控制流，如
\hyperref[\detokenize{chapter_frontend_and_ir/intermediate_representation:llvm-ir}]{图\ref{\detokenize{chapter_frontend_and_ir/intermediate_representation:llvm-ir}}}所示。基本块中，每条指令以静态单赋值(Static Single
Assignment， SSA) \DUrole{bibtex}{{[}Richard1995A{]}}
形式呈现，这些指令构成一个指令线性列表。SSA形式要求每个变量只赋值一次，并且每个变量在使用之前定义。控制流图中，每个节点为一个基本块，基本块之间通过边实现控制转移。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{中间表示-LLVMIR}.png}
\caption{LLVM IR}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:id16}}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:llvm-ir}}\end{figure}


\subsection{机器学习框架的中间表示}
\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:id8}}
\sphinxAtStartPar
上一节介绍了中间表示的类型，并举例说明了常见的中间表示形式。传统中间表示如LLVM
IR，能够很好地满足通用编译器的基本功能需求，包括类型系统、控制流和数据流分析等。然而，它们偏向机器语言，难以满足机器学习框架编译器的中间表示的需求。

\sphinxAtStartPar
在设计机器学习框架的中间表示时，需要充分考虑以下因素：

\sphinxAtStartPar
1)
张量表达。机器学习框架主要处理张量数据，因此正确处理张量数据类型是机器学习框架中间表示的基本要求。

\sphinxAtStartPar
2)
自动微分。自动微分是指对网络模型的自动求导，通过梯度指导对网络权重的优化。主流机器学习框架都提供了自动微分的功能，在设计中间表示时需要考虑自动微分实现的简洁性、性能以及高阶微分的扩展能力。

\sphinxAtStartPar
3)
计算图模式。主流机器学习框架如TensorFlow、PyTorch、MindSpore等都提供了静态图和动态图两种计算图模式，静态计算图模式先创建定义计算图，再显式执行，有利于对计算图进行优化，高效但不灵活。动态计算图模式则是每使用一个算子后，该算子会在计算图中立即执行得到结果，使用灵活、便于调试，但运行速度较低。机器学习框架的中间表示设计同时支持静态图和动态图，可以针对待解决的任务需求，选择合适的模式构建算法模型。

\sphinxAtStartPar
4) 支持高阶函数和闭包
\DUrole{bibtex}{{[}2010C{]}}。高阶函数和闭包是函数式编程的重要特性，高阶函数是指使用其它函数作为参数、或者返回一个函数作为结果的函数，闭包是指代码块和作用域环境的结合，可以在另一个作用域中调用一个函数的内部函数，并访问到该函数作用域中的成员。支持高阶函数和闭包，可以抽象通用问题、减少重复代码、提升框架表达的灵活性和简洁性。

\sphinxAtStartPar
5)
编译优化。机器学习框架的编译优化主要包括硬件无关的优化、硬件相关的优化、部署推理相关的优化等，这些优化都依赖于中间表示的实现。

\sphinxAtStartPar
6) JIT(Just In
Time)能力。机器学习框架进行编译执行加速时，经常用到JIT即时编译。JIT编译优化将会对中间表示中的数据流图的可优化部分实施优化，包括循环展开、融合、内联等。中间表示设计是否合理，将会影响机器学习框架的JIT编译性能和程序的运行能力。

\sphinxAtStartPar
针对上述需求，机器学习框架的开发者在传统中间表示的设计基础上不断扩展，提出了很多适用于机器学习框架的中间表示。接下来介绍一些主流机器学习框架的中间表示。

\sphinxAtStartPar
1、PyTorch

\sphinxAtStartPar
PyTorch框架是一个基于动态计算图机制的机器学习框架，以Python优先，具有很强的易用性和灵活性，方便用户编写和调试网络代码。为了保存和加载网络模型，PyTorch框架提供了TorchScript方法，用于创建可序列化和可优化模型。TorchScript
IR作为PyTorch模型的中间表示，通过JIT即时编译的形式，将Python代码转换成目标模型文件。任何TorchScript程序都可以在Python进程中保存，并加载到没有Python依赖的进程中。

\sphinxAtStartPar
PyTorch框架采用命令式编程方式，其TorchScript
IR以基于SSA的线性IR为基本组成形式，并通过JIT即时编译的Tracing和Scripting两种方法将Python代码转换成TorchScript
IR。如下Python代码使用了Scripting方法并打印其对应的中间表示图：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{torch}

\PYG{n+nd}{@torch}\PYG{o}{.}\PYG{n}{jit}\PYG{o}{.}\PYG{n}{script}
\PYG{k}{def} \PYG{n+nf}{test\PYGZus{}func}\PYG{p}{(}\PYG{n+nb}{input}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rv} \PYG{o}{=} \PYG{l+m+mf}{10.0}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{rv} \PYG{o}{=} \PYG{n}{rv} \PYG{o}{+} \PYG{n+nb}{input}
        \PYG{n}{rv} \PYG{o}{=} \PYG{n}{rv}\PYG{o}{/}\PYG{l+m+mi}{2}
    \PYG{k}{return} \PYG{n}{rv}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{test\PYGZus{}func}\PYG{o}{.}\PYG{n}{graph}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
该中间表示图的结构为：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{graph}\PYG{p}{(}\PYG{o}{\PYGZpc{}}\PYG{n+nb}{input}\PYG{l+m+mf}{.1} \PYG{p}{:} \PYG{n}{Tensor}\PYG{p}{)}\PYG{p}{:}
  \PYG{o}{\PYGZpc{}}\PYG{l+m+mi}{9} \PYG{p}{:} \PYG{n+nb}{int} \PYG{o}{=} \PYG{n}{prim}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Constant}\PYG{p}{[}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{(}\PYG{p}{)}
  \PYG{o}{\PYGZpc{}}\PYG{l+m+mi}{5} \PYG{p}{:} \PYG{n+nb}{bool} \PYG{o}{=} \PYG{n}{prim}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Constant}\PYG{p}{[}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} test.py:6:1}
  \PYG{o}{\PYGZpc{}}\PYG{n}{rv}\PYG{l+m+mf}{.1} \PYG{p}{:} \PYG{n+nb}{float} \PYG{o}{=} \PYG{n}{prim}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Constant}\PYG{p}{[}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mf}{10.}\PYG{p}{]}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} test.py:5:6}
  \PYG{o}{\PYGZpc{}}\PYG{l+m+mi}{2} \PYG{p}{:} \PYG{n+nb}{int} \PYG{o}{=} \PYG{n}{prim}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Constant}\PYG{p}{[}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} test.py:6:16}
  \PYG{o}{\PYGZpc{}}\PYG{l+m+mi}{14} \PYG{p}{:} \PYG{n+nb}{int} \PYG{o}{=} \PYG{n}{prim}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Constant}\PYG{p}{[}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} test.py:8:10}
  \PYG{o}{\PYGZpc{}}\PYG{n}{rv} \PYG{p}{:} \PYG{n+nb}{float} \PYG{o}{=} \PYG{n}{prim}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Loop}\PYG{p}{(}\PYG{o}{\PYGZpc{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{o}{\PYGZpc{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{o}{\PYGZpc{}}\PYG{n}{rv}\PYG{l+m+mf}{.1}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} test.py:6:1}
    \PYG{n}{block0}\PYG{p}{(}\PYG{o}{\PYGZpc{}}\PYG{n}{i} \PYG{p}{:} \PYG{n+nb}{int}\PYG{p}{,} \PYG{o}{\PYGZpc{}}\PYG{n}{rv}\PYG{l+m+mf}{.9} \PYG{p}{:} \PYG{n+nb}{float}\PYG{p}{)}\PYG{p}{:}
      \PYG{o}{\PYGZpc{}}\PYG{n}{rv}\PYG{l+m+mf}{.3} \PYG{p}{:} \PYG{n}{Tensor} \PYG{o}{=} \PYG{n}{aten}\PYG{p}{:}\PYG{p}{:}\PYG{n}{add}\PYG{p}{(}\PYG{o}{\PYGZpc{}}\PYG{n+nb}{input}\PYG{l+m+mf}{.1}\PYG{p}{,} \PYG{o}{\PYGZpc{}}\PYG{n}{rv}\PYG{l+m+mf}{.9}\PYG{p}{,} \PYG{o}{\PYGZpc{}}\PYG{l+m+mi}{9}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} \PYGZlt{}string\PYGZgt{}:5:9}
      \PYG{o}{\PYGZpc{}}\PYG{l+m+mi}{12} \PYG{p}{:} \PYG{n+nb}{float} \PYG{o}{=} \PYG{n}{aten}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FloatImplicit}\PYG{p}{(}\PYG{o}{\PYGZpc{}}\PYG{n}{rv}\PYG{l+m+mf}{.3}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} test.py:7:2}
      \PYG{o}{\PYGZpc{}}\PYG{n}{rv}\PYG{l+m+mf}{.6} \PYG{p}{:} \PYG{n+nb}{float} \PYG{o}{=} \PYG{n}{aten}\PYG{p}{:}\PYG{p}{:}\PYG{n}{div}\PYG{p}{(}\PYG{o}{\PYGZpc{}}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{o}{\PYGZpc{}}\PYG{l+m+mi}{14}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} test.py:8:7}
      \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{p}{(}\PYG{o}{\PYGZpc{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{o}{\PYGZpc{}}\PYG{n}{rv}\PYG{l+m+mf}{.6}\PYG{p}{)}
  \PYG{k}{return} \PYG{p}{(}\PYG{o}{\PYGZpc{}}\PYG{n}{rv}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
TorchScript是PyTorch的JIT实现，支持使用Python训练模型，然后通过JIT转换为语言无关的模块，从而提升模型部署能力，提高编译性能。同时，TorchScript
IR显著改善了Pytorch框架的模型可视化效果。

\sphinxAtStartPar
2、Jax

\sphinxAtStartPar
Jax机器学习框架同时支持静态图和动态图，其中间表示采用Jaxpr(JAX Program
Representation) IR。Jaxpr
IR是一种强类型、纯函数的中间表示，其输入、输出都带有类型信息，函数输出只依赖输入，不依赖全局变量。

\sphinxAtStartPar
Jaxpr IR的表达采用ANF(A\sphinxhyphen{}norm Form)函数式表达形式，ANF文法如下所示：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}aexp\PYGZgt{} ::=  NUMBER | STRING | VAR | BOOLEAN | PRIMOP
           |  (lambda (VAR ...) \PYGZlt{}exp\PYGZgt{})
\PYGZlt{}cexp\PYGZgt{} ::=  (\PYGZlt{}aexp\PYGZgt{} \PYGZlt{}aexp\PYGZgt{} ...)
           ｜ (if \PYGZlt{}aexp\PYGZgt{} \PYGZlt{}exp\PYGZgt{} \PYGZlt{}exp\PYGZgt{})
\PYGZlt{}exp\PYGZgt{} ::=  (let ([VAR \PYGZlt{}cexp\PYGZgt{}]) \PYGZlt{}exp\PYGZgt{}) | \PYGZlt{}cexp\PYGZgt{} | \PYGZlt{}aexp\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
ANF形式将表达式划分为两类：原子表达式(aexp)和复合表达式(cexp)。原子表达式用于表示常数、变量、原语、匿名函数，复合表达式由多个原子表达式组成，可看作一个匿名函数或原语函数调用，组合的第一个输入是调用的函数，其余输入是调用的参数。如下代码打印了一个函数对应的JaxPr：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{jax} \PYG{k+kn}{import} \PYG{n}{make\PYGZus{}jaxpr}
\PYG{k+kn}{import} \PYG{n+nn}{jax}\PYG{n+nn}{.}\PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{jnp}

\PYG{k}{def} \PYG{n+nf}{test\PYGZus{}func}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ret} \PYG{o}{=} \PYG{n}{x} \PYG{o}{+} \PYG{n}{jnp}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mi}{3}
    \PYG{k}{return} \PYG{n}{jnp}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{ret}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{make\PYGZus{}jaxpr}\PYG{p}{(}\PYG{n}{test\PYGZus{}func}\PYG{p}{)}\PYG{p}{(}\PYG{n}{jnp}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{,} \PYG{n}{jnp}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
其对应的JaxPr为：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}} \PYG{k}{lambda} \PYG{p}{;} \PYG{n}{a}\PYG{p}{:}\PYG{n}{f32}\PYG{p}{[}\PYG{l+m+mi}{8}\PYG{p}{]} \PYG{n}{b}\PYG{p}{:}\PYG{n}{f32}\PYG{p}{[}\PYG{l+m+mi}{8}\PYG{p}{]}\PYG{o}{.} \PYG{n}{let}
    \PYG{n}{c}\PYG{p}{:}\PYG{n}{f32}\PYG{p}{[}\PYG{l+m+mi}{8}\PYG{p}{]} \PYG{o}{=} \PYG{n}{sin} \PYG{n}{b}
    \PYG{n}{d}\PYG{p}{:}\PYG{n}{f32}\PYG{p}{[}\PYG{l+m+mi}{8}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mul} \PYG{n}{c} \PYG{l+m+mf}{3.0}
    \PYG{n}{e}\PYG{p}{:}\PYG{n}{f32}\PYG{p}{[}\PYG{l+m+mi}{8}\PYG{p}{]} \PYG{o}{=} \PYG{n}{add} \PYG{n}{a} \PYG{n}{d}
    \PYG{n}{f}\PYG{p}{:}\PYG{n}{f32}\PYG{p}{[}\PYG{p}{]} \PYG{o}{=} \PYG{n}{reduce\PYGZus{}sum}\PYG{p}{[}\PYG{n}{axes}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{p}{)}\PYG{p}{]} \PYG{n}{e}
  \PYG{o+ow}{in} \PYG{p}{(}\PYG{n}{f}\PYG{p}{,}\PYG{p}{)} \PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
Jax框架结合了Autograd 和 JIT，基于Jaxpr
IR，支持循环、分支、递归、闭包函数求导以及三阶求导，并且支持自动微分的反向传播和前向传播。

\sphinxAtStartPar
3、TensorFlow

\sphinxAtStartPar
TensorFlow框架同时支持静态图和动态图，是一个基于数据流编程的机器学习框架，使用数据流图作为数据结构进行各种数值计算。TensorFlow机器学习框架的静态图机制更为人所熟知。在静态图机制中，运行TensorFlow的程序会经历一系列的抽象以及分析，程序会逐步从高层的中间表示向底层的中间表示进行转换，我们把这种变换成为lowering。

\sphinxAtStartPar
为了适配不同的硬件平台，基于静态计算图，TensorFlow采用了多种IR设计，其编译生态系统如
\hyperref[\detokenize{chapter_frontend_and_ir/intermediate_representation:tfir}]{图\ref{\detokenize{chapter_frontend_and_ir/intermediate_representation:tfir}}}所示。蓝色部分是基于图的中间表示，绿色部分是基于SSA的中间表示。在中间表示的转换过程中，各个层级的中间表示各自为政，无法互相有效地沟通信息，也不清楚其他层级的中间表示做了哪些优化，因此每个中间表示只能尽力将当前的优化做到最好，造成了很多优化在每个层级的中间表示中重复进行,
从而导致优化效率的低下。尤其是从图中间表示到SSA中间表示的变化过大，转换开销极大。此外，各个层级的相同优化的代码无法复用，也降低了开发效率。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=1000\sphinxpxdimen]{{中间表示-MLIR}.png}
\caption{TensorFlow}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:id17}}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:tfir}}\end{figure}

\sphinxAtStartPar
4、MLIR

\sphinxAtStartPar
针对这个问题，TensorFlow团队提出了MLIR(Multi\sphinxhyphen{}Level Intermediate
Represent，多级中间表示)
\DUrole{bibtex}{{[}2020MLIR{]}}。MLIR不是一种具体的中间表示定义，而是为中间表示提供一个统一的抽象表达和概念。
开发者可以使用MLIR开发的一系列基础设施，来定义符合自己需求的中间表示，
因此我们可以把MLIR理解为“编译器的编译器”。MLIR不局限于TensorFlow框架，
还可以用于构建连接其他语言与后端（如LLVM）的中间表示。
MLIR深受LLVM设计理念的影响，但与LLVM不同的是，
MLIR是一个更开放的生态系统。 在MLIR中， 没有预设的操作与抽象类型，
这使得开发者可以更自由地定义中间表示，并更有针对性地解决其领域的问题。MLIR通过Dialect的概念来支持这种可拓展性，
Dialect在特定的命名空间下为抽象提供了分组机制，分别为每种中间表示定义对应的产生式并绑定相应的Operation，
从而生成一个MLIR类型的中间表示。Operation是MLIR中抽象和计算的核心单元，其具有特定的语意，可以用于表示LLVM中所有核心的IR结构，
例如指令， 函数以及模块等。 如下就是一个MLIR定义下的Operation：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{n}{tensor} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{toy.transpose}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{(}\PYG{o}{\PYGZpc{}}\PYG{n}{tensor}\PYG{p}{)} \PYG{p}{\PYGZob{}}\PYG{n}{inplace} \PYG{o}{=} \PYG{n}{true}\PYG{p}{\PYGZcb{}} \PYG{p}{:} \PYG{p}{(}\PYG{n}{tensor}\PYG{o}{\PYGZlt{}}\PYG{l+m+mi}{2}\PYG{n}{x3xf64}\PYG{o}{\PYGZgt{}}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{tensor}\PYG{o}{\PYGZlt{}}\PYG{l+m+mi}{3}\PYG{n}{x2xf64}\PYG{o}{\PYGZgt{}} \PYG{n}{loc}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{example/file/path}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+m+mi}{12}\PYG{p}{:}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\% tensor: Operation定义的结果的名字，
\(\%\)是为了避免冲突统一加入的。一个Operation可以定义0或者多个结果，它们是SSA值。

\item {} 
\sphinxAtStartPar
“toy.transpose”:
Operation的名字。它是一个唯一的字符串，其中Dialect为Toy。因此它可以理解为Toy
Dialect 中的transpose Operation。

\item {} 
\sphinxAtStartPar
(\%tensor)：输入操作数（或参数）的列表，它们是由其它操作定义或引用块参数的
SSA 值。

\item {} 
\sphinxAtStartPar
\{inplace =
true\}：零个或多个属性的字典，这些属性是始终为常量的特殊操作数。在这里，我们定义了一个名为“inplace”的布尔属性，它的常量值为
true。

\item {} 
\sphinxAtStartPar
(tensor<2x3xf64>)\sphinxhyphen{}>tensor<3x2xf64>：函数形式表示的操作类型，前者是输入，后者是输出。尖括号内代表输入与输出的数据类型以及形状，
例如\(<2x3xf64>\)代表一个形状位2X3，
数据类型为float64的张量。

\item {} 
\sphinxAtStartPar
loc(“example/file/path”:12:1)：此操作的源代码中的位置。

\end{itemize}

\sphinxAtStartPar
由于各层中间表示都遵循如上的样式进行定义，所以各个层级的中间表示之间可以更加方便的进行转换，
提高了中间表示转换的效率。各个不同层级的中间表示还可以协同进行优化。
此外，由于中间表示之间不再相互独立，
各层级的优化不必做到极致，而是可以将优化放到最适合的层级。
其他的中间表示只需要先转换为该层级的中间表示，就可以进行相关的优化，提高了优化的效率与开发效率。TensorFlow从图中间表示到SSA中间表示的转换也可以通过使用MLIR来进行多层转换，
使转换更加平滑， 降低了转化的难度。
针对MLIR的更多内容将会在第六章进行介绍。

\sphinxAtStartPar
5、MindSpore

\sphinxAtStartPar
与PyTorch、Jax、TensorFlow框架相同，MindSpore机器学习框架同时支持静态图和动态图。MindSpore框架采用的是一种基于图表示的函数式中间表示，即MindIR，全称MindSpore
IR。MindIR没有采用多层中间表示的结构，而是通过统一的中间表示，定义了网络的逻辑结构和算子的属性，能够消除不同后端的模型差异，连接不同的目标机器。

\sphinxAtStartPar
MindIR最核心的目的是服务于自动微分变换，而自动微分采用的是基于函数式编程框架的变换方法，因此MindIR采用了接近于ANF函数式的语义。MindIR具有以下特点：

\sphinxAtStartPar
（1）基于图的（Graph
based）。与TensorFlow类似，程序使用图来表示，使其容易去做优化。但跟TensorFlow不一样的是，在MindSpore中，函数是“一等公民”。函数可以被递归调用，也可以被当做参数传到其他的函数中，或者从其他函数中返回，使得MindSpore可以表达一系列的控制流结构。

\sphinxAtStartPar
（2）纯函数的（Purely functional）。

\sphinxAtStartPar
纯函数是指函数的结果只依赖函数的参数。若函数依赖或影响外部的状态，比如，函数会修改外部全局变量，或者函数的结果依赖全局变量的值，则称函数具有副作用
\DUrole{bibtex}{{[}spuler1994compiler{]}}。若使用了带有副作用的函数，代码的执行顺序必须得到严格的保证，否则可能会得到错误的结果，比如对全局变量的先写后读变成了先读后写。同时，副作用的存在也会影响自动微分，因为反向部分需要从前向部分获取中间变量，需要确保该中间变量的正确。因此需要保证自动微分的函数是纯函数。

\sphinxAtStartPar
由于Python语言具有高度动态性的特点，纯函数式编程对用户使用上有一些编程限制。有些机器学习框架的自动微分功能只支持对纯函数求导，且要求用户自行保证这一点。如果用户代码中写了带有副作用的函数，那么求导的结果可能会不符合预期。MindIR支持副作用的表达，能够将副作用的表达转换为纯函数的表达，从而在保持ANF函数式语义不变的同时，确保执行顺序的正确性，从而实现自由度更高的自动微分。

\sphinxAtStartPar
（3）支持闭包表示的（Closure
representation）。反向模式的自动微分，需要存储基本操作的中间结果到闭包中，然后再去进行组合连接。所以有一个自然的闭包表示尤为重要。闭包是指代码块和作用域环境的结合，在MindIR中，代码块是以函数图呈现的，而作用域环境可以理解为该函数被调用时的上下文环境。

\sphinxAtStartPar
（4）强类型的（Strongly
typed）。每个节点需要有一个具体的类型，这个对于性能最大化很重要。在机器学习应用中，因为算子可能很耗费时间，所以越早捕获错误越好。因为需要支持函数调用和高阶函数，相比于TensorFlow的数据流图，MindIR的类型和形状推导更加复杂且强大。

\sphinxAtStartPar
在结合MindSpore框架的自身特点后，MindIR的定义如
\hyperref[\detokenize{chapter_frontend_and_ir/intermediate_representation:mindir}]{图\ref{\detokenize{chapter_frontend_and_ir/intermediate_representation:mindir}}}所示。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=1100\sphinxpxdimen]{{中间表示-MindIR}.png}
\caption{MindIR文法。MindIR中的ANode对应于ANF的原子表达式，ValueNode用于表示常数值，ParameterNode用于表示函数的形参，CNode则对应于ANF的复合表达式，表示函数调用}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:id18}}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:mindir}}\end{figure}

\sphinxAtStartPar
接下来我们通过如下的一段程序作为示例，来进一步分析MindIR。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{func}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{x} \PYG{o}{/} \PYG{n}{y}

\PYG{n+nd}{@ms\PYGZus{}function}
\PYG{k}{def} \PYG{n+nf}{test\PYGZus{}f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{a} \PYG{o}{=} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}
    \PYG{n}{b} \PYG{o}{=} \PYG{n}{a} \PYG{o}{+} \PYG{n}{y}
    \PYG{n}{c} \PYG{o}{=} \PYG{n}{b} \PYG{o}{*} \PYG{n}{func}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{c}
\end{sphinxVerbatim}

\sphinxAtStartPar
该函数对应的ANF表达式为：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{lambda} \PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
    \PYG{n}{let} \PYG{n}{a} \PYG{o}{=} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1} \PYG{o+ow}{in}
    \PYG{n}{let} \PYG{n}{b} \PYG{o}{=} \PYG{n}{a} \PYG{o}{+} \PYG{n}{y} \PYG{o+ow}{in}
    \PYG{n}{let} \PYG{n}{func} \PYG{o}{=} \PYG{k}{lambda} \PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
        \PYG{n}{let} \PYG{n}{ret} \PYG{o}{=} \PYG{n}{x} \PYG{o}{/} \PYG{n}{y} \PYG{o+ow}{in}
        \PYG{n}{ret} \PYG{n}{end} \PYG{o+ow}{in}
    \PYG{n}{let} \PYG{o}{\PYGZpc{}}\PYG{l+m+mi}{1} \PYG{o}{=} \PYG{n}{func}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)} \PYG{o+ow}{in}
    \PYG{n}{let} \PYG{n}{c} \PYG{o}{=} \PYG{n}{b} \PYG{o}{*} \PYG{o}{\PYGZpc{}}\PYG{l+m+mi}{1} \PYG{o+ow}{in}
    \PYG{n}{c} \PYG{n}{end}
\end{sphinxVerbatim}

\sphinxAtStartPar
在ANF中，每个表达式都用let表达式绑定为一个变量，通过对变量的引用来表示对表达式输出的依赖，而在MindIR中，每个表达式都绑定为一个节点，通过节点与节点之间的有向边表示依赖关系。该函数对应的MindIR的可视化表示如
\hyperref[\detokenize{chapter_frontend_and_ir/intermediate_representation:mindir-graph}]{图\ref{\detokenize{chapter_frontend_and_ir/intermediate_representation:mindir-graph}}}所示。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{中间表示-MindIR图}.png}
\caption{MindIR的函数图表示}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:id19}}\label{\detokenize{chapter_frontend_and_ir/intermediate_representation:mindir-graph}}\end{figure}

\sphinxAtStartPar
MindIR同时支持静态计算图和动态计算图的构建方式，更好地兼顾了灵活性与高性能。相比传统计算图，MindIR不仅可以表达算子之间的数据依赖，还可以表达丰富的函数式语义，具备更自然的自动微分实现方式。MindIR原生支持闭包，并且支持高阶函数的表达。在处理控制流时，MindIR将控制流转换为高阶函数的数据流，不仅支持数据流的自动微分，还支持条件跳转、循环和递归等控制流的自动微分，从而提升MindSpore的自动微分能力。

\sphinxAtStartPar
在JIT即时编译方面，MindIR采用了基于图表示的形式，将控制流和数据流合一，支持更高效的JIT优化。在编译优化方面，MindIR引入优化器对计算图进行优化，采用前端\sphinxhyphen{}优化器\sphinxhyphen{}后端的三段式表达形式，支持硬件无关的优化(如类型推导、表达式化简等)、硬件相关的优化（如自动并行、内存优化、图算融合、流水线执行等）以及部署推理相关的优化（如量化、剪枝等），显著提升了MindSpore的编译执行能力。


\section{自动微分}
\label{\detokenize{chapter_frontend_and_ir/ad:id1}}\label{\detokenize{chapter_frontend_and_ir/ad::doc}}
\sphinxAtStartPar
上一节，我们介绍了机器学习框架的中间表示，设计这些中间表示的最核心的目的之一便是服务于自动微分变换。那么什么是自动微分？我们在这一节来详细介绍。


\subsection{自动微分的基本概念}
\label{\detokenize{chapter_frontend_and_ir/ad:id2}}
\sphinxAtStartPar
自动微分（Automatic
Differentiation，AD）是一种对计算机程序进行高效且准确求导的技术，在上个世纪六七十年代就已经被广泛应用于流体力学、天文学、数学金融等领域
\DUrole{bibtex}{{[}10.5555/1455489{]}}。时至今日，自动微分的实现及其理论仍然是一个活跃的研究领域。随着近些年深度学习在越来越多的机器学习任务上取得领先成果，自动微分被广泛的应用于机器学习领域。许多机器学习模型使用的优化算法都需要获取模型的导数，因此自动微分技术成为了一些热门的机器学习框架（例如TensorFlow和PyTorch）的核心特性。

\sphinxAtStartPar
常见的计算机程序求导的方法可以归纳为以下四种
\DUrole{bibtex}{{[}2015Automatic{]}}：手工微分（Manual
Differentiation）、数值微分（Numerical
Differentiation）、符号微分（Symbolic
Differentiation）和自动微分（Automatic Differentiation）。

\sphinxAtStartPar
（1）手工微分：需手工求解函数导数的表达式，并在程序运行时根据输入的数值直接计算结果。手工微分需根据函数的变化重新推导表达式，工作量大且容易出错。

\sphinxAtStartPar
（2）数值微分
\DUrole{bibtex}{{[}2015Numerical{]}}：数值微分通过差分近似方法完成，其本质是根据导数的定义推导而来。
\begin{equation}\label{equation:chapter_frontend_and_ir/ad:chapter_frontend_and_ir/ad:0}
\begin{split}f^{'}(x)=\lim_{h \to 0}\frac{f(x+h)-f(x)}{h}\end{split}
\end{equation}
\sphinxAtStartPar
当\(h\)充分小时，可以用差分\(\frac{f(x+h)-f(x)}{h}\)来近似导数结果。而近似的一部分误差，称为截断误差（Truncation
error）。理论上，数值微分中的截断误差与步长\(h\)有关，\(h\)越小则截断误差越小，近似程度越高。但实际情况下数值微分的精确度并不会随着\(h\)的减小而一直减小。这是因为计算机系统对于浮点数运算的精度有限导致另外一种误差的存在，这种误差称为舍入误差（Round\sphinxhyphen{}off
Error）。舍入误差会随着\(h\)变小而逐渐增大。当h较大时，截断误差占主导。而当h较小时，舍入误差占主导。
在截断误差和舍入误差的共同作用下，数值微分的精度将会在某一个\(h\)值处达到最小值，并不会无限的减小。因此，虽然数值微分容易实现，但是存在精度误差问题。

\sphinxAtStartPar
（3）符号微分
\DUrole{bibtex}{{[}2003Computer{]}}：利用计算机程序自动地通过如下的数学规则对函数表达式进行递归变换来完成求导。
\begin{equation}\label{equation:chapter_frontend_and_ir/ad:chapter_frontend_and_ir/ad:1}
\begin{split}\frac{d}{dx}(f(x)+g(x))\rightsquigarrow\frac{d}{dx}f(x)+\frac{d}{dx}g(x)\end{split}
\end{equation}\begin{equation}\label{equation:chapter_frontend_and_ir/ad:chapter_frontend_and_ir/ad:2}
\begin{split}\frac{d}{dx}(f(x)g(x))\rightsquigarrow(\frac{d}{dx}f(x))g(x)+f(x)(\frac{d}{dx}g(x))\end{split}
\end{equation}
\sphinxAtStartPar
符号微分常被应用于现代代数系统工具中，例如Mathematica、Maxima和Maple，以及机器学习框架，如Theano。符号微分虽然消除了手工微分硬编码的缺陷。但因为对表达式进行严格的递归变换和展开，不复用产生的变换结果，很容易产生表达式膨胀（expression
swell \DUrole{bibtex}{{[}10.5555/60181.60188{]}} ）问题。如
\hyperref[\detokenize{chapter_frontend_and_ir/ad:symbolic-differentiation}]{图\ref{\detokenize{chapter_frontend_and_ir/ad:symbolic-differentiation}}}
所示，用符号微分计算递归表达式\(l_{n+1}=4l_n(1-l_n)\)，\(l_1=x\)的导数表达式，其结果随着迭代次数增加快速膨胀。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{符号微分的表达式膨胀问题}.png}
\caption{符号微分的表达式膨胀问题}\label{\detokenize{chapter_frontend_and_ir/ad:id15}}\label{\detokenize{chapter_frontend_and_ir/ad:symbolic-differentiation}}\end{figure}

\sphinxAtStartPar
并且符号微分需要表达式被定义成闭合式的（closed\sphinxhyphen{}form），不能带有或者严格限制控制流的语句表达，使用符号微分会很大程度上地限制了机器学习框架网络的设计与表达。

\sphinxAtStartPar
（4）自动微分
\DUrole{bibtex}{{[}2000An{]}}：自动微分的思想是将计算机程序中的运算操作分解为一个有限的基本操作集合，且集合中基本操作的求导规则均为已知，在完成每一个基本操作的求导后，使用链式法则将结果组合得到整体程序的求导结果。自动微分是一种介于数值微分和符号微分之间的求导方法，结合了数值微分和符号微分的思想。相比于数值微分，自动微分可以精确地计算函数的导数；相比符号微分，自动微分将程序分解为基本表达式的组合，仅对基本表达式应用符号微分规则，并复用每一个基本表达式的求导结果，从而避免了符号微分中的表达式膨胀问题。而且自动微分可以处理分支、循环和递归等控制流语句。目前的深度学习框架基本都采用自动微分机制进行求导运算，下面我们将重点介绍自动微分机制以及自动微分的实现。


\subsection{前向与反向自动微分}
\label{\detokenize{chapter_frontend_and_ir/ad:id9}}
\sphinxAtStartPar
自动微分根据链式法则的不同组合顺序，可以分为前向模式（Forward
Mode）和反向模式（Reverse
Mode）。对于一个复合函数\(y=a(b(c(x)))\),其梯度值\(\frac{dy}{dx}\)的计算公式为：
\begin{equation}\label{equation:chapter_frontend_and_ir/ad:chapter_frontend_and_ir/ad:3}
\begin{split}\frac{dy}{dx}=\frac{dy}{da}\frac{da}{db}\frac{db}{dc}\frac{dc}{dx}\end{split}
\end{equation}
\sphinxAtStartPar
前向模式的自动微分是从输入方向开始计算梯度值的，其计算公式为：
\begin{equation}\label{equation:chapter_frontend_and_ir/ad:chapter_frontend_and_ir/ad:4}
\begin{split}\frac{dy}{dx}=(\frac{dy}{da}(\frac{da}{db}(\frac{db}{dc}\frac{dc}{dx})))\end{split}
\end{equation}
\sphinxAtStartPar
反向模式的自动微分是从输出方向开始计算梯度值的，其计算公式为：
\begin{equation}\label{equation:chapter_frontend_and_ir/ad:chapter_frontend_and_ir/ad:5}
\begin{split}\frac{dy}{dx}=(((\frac{dy}{da}\frac{da}{db})\frac{db}{dc})\frac{dc}{dx})\end{split}
\end{equation}
\sphinxAtStartPar
我们以下面的函数为例介绍两种模式的计算方式，我们希望计算函数在\((x_1, x_2)=(2,5)\)处的导数\(\frac{\partial y}{\partial x_1}\)：
\begin{equation}\label{equation:chapter_frontend_and_ir/ad:chapter_frontend_and_ir/ad:6}
\begin{split}y=f(x_1,x_2)=ln(x_1)+{x_1}{x_2}-sin(x_2)\end{split}
\end{equation}
\sphinxAtStartPar
该函数对应的计算图如 \hyperref[\detokenize{chapter_frontend_and_ir/ad:example-compute-graph}]{图\ref{\detokenize{chapter_frontend_and_ir/ad:example-compute-graph}}}：

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{自动微分-示例计算图}.svg}
\caption{示例计算图}\label{\detokenize{chapter_frontend_and_ir/ad:id16}}\label{\detokenize{chapter_frontend_and_ir/ad:example-compute-graph}}\end{figure}

\sphinxAtStartPar
（1）前向模式

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{自动微分-前向模式自动微分示例}.png}
\caption{前向模式自动微分示例}\label{\detokenize{chapter_frontend_and_ir/ad:id17}}\label{\detokenize{chapter_frontend_and_ir/ad:forward-ad}}\end{figure}

\sphinxAtStartPar
前向模式的计算过程如
\hyperref[\detokenize{chapter_frontend_and_ir/ad:forward-ad}]{图\ref{\detokenize{chapter_frontend_and_ir/ad:forward-ad}}}所示，左侧是源程序分解后得到的基本操作集合，右侧展示了运用链式法则和已知的求导规则，从上至下计算每一个中间变量\({\dot{v}_i}=\frac{\partial v_i}{\partial x_1}\)，从而计算出最后的变量\({\dot{v}_5}=\frac{\partial y}{\partial x_1}\)。

\sphinxAtStartPar
当我们想要对一个函数求导时，我们想要得到的是该函数的任意一个输出对任意一个输入的偏微分的集合。对于一个带有\(n\)个独立输入\(x_i\)和\(m\)个独立输出\(y_i\)的函数\(f:{\mathbf{R}^n}\to \mathbf{R}^m\)，该函数的求导结果可以构成如下的雅克比矩阵（Jacobian
Matrix）：
\begin{equation}\label{equation:chapter_frontend_and_ir/ad:chapter_frontend_and_ir/ad:7}
\begin{split}\mathbf{J}_{f}=
\begin{bmatrix}
    \frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_n}
\end{bmatrix}\end{split}
\end{equation}
\sphinxAtStartPar
前向模式中每次计算函数\(f\)的所有输出对某一个输入的偏微分，也就是雅克比矩阵的某一列，如下面的向量所示。因此，通过n次前向模式的自动微分就可以得到整个雅克比矩阵。
\begin{equation}\label{equation:chapter_frontend_and_ir/ad:chapter_frontend_and_ir/ad:8}
\begin{split}\begin{bmatrix}
    \frac{\partial y_1}{\partial x_i} \\
    \vdots \\
    \frac{\partial y_m}{\partial x_i}
\end{bmatrix}\end{split}
\end{equation}
\sphinxAtStartPar
前向模式通过计算雅克比向量积（Jacobian\sphinxhyphen{}vector
products）的方式来计算这一列的结果。我们初始化\(\dot{\mathbf{x}}=\mathbf{r}\)。基本操作的求导规则是已经定义好的，代表着基本操作的雅可比矩阵是已知量。在此基础上，我们应用链式法则从\(f\)的输入到输出传播求导结果，从而得到输入网络的雅克比矩阵中的一列。
\begin{equation}\label{equation:chapter_frontend_and_ir/ad:chapter_frontend_and_ir/ad:9}
\begin{split}\mathbf{J}_{f}\mathbf{r}=
\begin{bmatrix}
    \frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_n}
\end{bmatrix}
\begin{bmatrix}
    r_1 \\
    \vdots \\
    r_n
\end{bmatrix}\end{split}
\end{equation}
\sphinxAtStartPar
（2）反向模式

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{自动微分-反向模式自动微分示例}.png}
\caption{反向模式自动微分示例}\label{\detokenize{chapter_frontend_and_ir/ad:id18}}\label{\detokenize{chapter_frontend_and_ir/ad:backward-ad}}\end{figure}

\sphinxAtStartPar
反向模式的计算过程如上
\hyperref[\detokenize{chapter_frontend_and_ir/ad:backward-ad}]{图\ref{\detokenize{chapter_frontend_and_ir/ad:backward-ad}}}所示，左侧是源程序分解后得到的基本操作集合，右侧展示了运用链式法则和已知的求导规则，从\(\bar{v}_5=\bar{y}=\frac{\partial y}{\partial y}=1\)开始，
由下至上地计算每一个中间变量\({\bar{v}_i}=\frac{\partial y_j}{\partial v_i}\)，从而计算出最后的变量\({\bar{x}_1}=\frac{\partial y}{\partial x_1}\)和\({\bar{x}_2}=\frac{\partial y}{\partial x_2}\)。

\sphinxAtStartPar
反向模式每次计算的是函数\(f\)的某一个输出对任一输入的偏微分，也就是雅克比矩阵的某一行，如下面的向量所示。因此通过运行m次反向模式自动微分，我们就可以得到整个雅克比矩阵。
\begin{equation}\label{equation:chapter_frontend_and_ir/ad:chapter_frontend_and_ir/ad:10}
\begin{split}\begin{bmatrix}
    \frac{\partial y_j}{\partial x_1} & \cdots & \frac{\partial y_j}{\partial x_n}
\end{bmatrix}\end{split}
\end{equation}
\sphinxAtStartPar
类似地，我们可以通过计算向量雅克比积（Vector\sphinxhyphen{}jacobian
products）的方式来计算雅克比矩阵的一行。我们初始化\(\bar{\mathbf{y}}=\mathbf{r}\)，在已知基本操作的求导规则的前提下，应用链式法则从\(f\)的输出到输入传播求导结果，从而最后得到雅克比矩阵中的一行。
\begin{equation}\label{equation:chapter_frontend_and_ir/ad:chapter_frontend_and_ir/ad:11}
\begin{split}\mathbf{r}^{T}\mathbf{J}_{f}=
\begin{bmatrix}
    r_1 & \cdots & r_m
\end{bmatrix}
\begin{bmatrix}
    \frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_n}
\end{bmatrix}\end{split}
\end{equation}
\sphinxAtStartPar
在求解函数\(f\)的雅克比矩阵时，前向模式的迭代次数与雅克比矩阵的列数相关，而反向模式的迭代次数则与雅克比矩阵的行数相关。因此，在函数输出个数远远大于输入个数时\((f:{\mathbf{R}^n}\to \mathbf{R}^m, n << m)\)，前向模式效率更高；反之，在函数输入个数远远大于输出个数时\((f:{\mathbf{R}^n}\to \mathbf{R}^m, n >> m)\)，反向模式效率更高。在极端情况下的函数\(f:{\mathbf{R}^n}\to \mathbf{R}\)，只需要应用一次反向模式就已经能够把所有输出对输入的导数\((\frac{\partial y}{\partial x_1},\cdots,\frac{\partial y}{\partial x_n})\)都计算出来，而前向模式则需要执行n次。这种计算一个标量值的输出关于大量参数输入的梯度的场景恰好是机器学习实践中最常见的一种计算场景，这使得反向模式的自动微分成为反向传播算法使用的核心技术之一。

\sphinxAtStartPar
但是反向模式也存在一定的缺陷。在源程序分解为一系列基本操作后，前向模式由于求导顺序与基本操作的执行顺序一致，输入值可以在执行基本操作的过程中同步获得。而在反向模式中，由于求导顺序与源程序的执行顺序是相反的，计算过程需要分为两个阶段，第一个阶段先执行源程序，且将源程序的中间结果保存起来，在第二阶段才把中间结果取出来去计算导数。因此反向模式会有额外的内存消耗。业界也一直在研究反向模式的内存占用优化方法，例如检查点策略（checkpointing
strategies）和数据流分析（data\sphinxhyphen{}flow analysis）
\DUrole{bibtex}{{[}2006The{]}}\DUrole{bibtex}{{[}2017Divide{]}} 。


\subsection{自动微分的实现}
\label{\detokenize{chapter_frontend_and_ir/ad:id12}}
\sphinxAtStartPar
上一节我们介绍了自动微分的基本概念，可以总结为将程序分解为一系列微分规则已知的基本操作，然后运用链式法则将它们的微分结果组合起来得到程序的微分结果。而在机器学习的应用中，因为输入的数量远远大于输出的数量，所以反向模式的自动微分更受青睐。虽然自动微分的基本思想是明确的，但是具体的实现方法也分为几类
\DUrole{bibtex}{{[}2015Automatic{]}} ，大体可以划分为基本表达式法（Elemental
Libraries）、操作符重载法（Operator
Overloading，OO）和代码变换法（Source Code Transformation，ST）。

\sphinxAtStartPar
（1）基本表达式法：封装大多数的基本表达式及对应的微分表达式，通过库函数的方式提供给用户，用户在写代码时，需要手工分解程序为一系列的基本表达式，然后使用这些库函数去替换这些基本表达式。以程序\(a=(x+y)/z\)为例，用户需要手工地把这个程序分解为：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{t} \PYG{o}{=} \PYG{n}{x} \PYG{o}{+} \PYG{n}{y}
\PYG{n}{a} \PYG{o}{=} \PYG{n}{t} \PYG{o}{/} \PYG{n}{z}
\end{sphinxVerbatim}

\sphinxAtStartPar
然后使用自动微分的库函数去替换分解出来的基本表达式：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{/}\PYG{o}{/} \PYG{n}{参数为变量x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{t和对应的导数变量dx}\PYG{p}{,} \PYG{n}{dy}\PYG{p}{,} \PYG{n}{dt}
\PYG{n}{call} \PYG{n}{ADAdd}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{dx}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{dy}\PYG{p}{,} \PYG{n}{t}\PYG{p}{,} \PYG{n}{dt}\PYG{p}{)}
\PYG{o}{/}\PYG{o}{/} \PYG{n}{参数为变量t}\PYG{p}{,} \PYG{n}{z}\PYG{p}{,} \PYG{n}{a和对应的导数变量dt}\PYG{p}{,} \PYG{n}{dz}\PYG{p}{,} \PYG{n}{da}
\PYG{n}{call} \PYG{n}{ADDiv}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{dt}\PYG{p}{,} \PYG{n}{z}\PYG{p}{,} \PYG{n}{dz}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{da}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
库函数ADAdd和ADDiv运用链式法则，分别定义了Add和Div的微分表达式。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{ADAdd}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{dx}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{dy}\PYG{p}{,} \PYG{n}{z}\PYG{p}{,} \PYG{n}{dz}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{z} \PYG{o}{=} \PYG{n}{x} \PYG{o}{+} \PYG{n}{y}
    \PYG{n}{dz} \PYG{o}{=} \PYG{n}{dy} \PYG{o}{+} \PYG{n}{dx}

\PYG{k}{def} \PYG{n+nf}{ADDiv}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{dx}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{dy}\PYG{p}{,} \PYG{n}{z}\PYG{p}{,} \PYG{n}{dz}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{z} \PYG{o}{=} \PYG{n}{x} \PYG{o}{/} \PYG{n}{y}
    \PYG{n}{dz} \PYG{o}{=} \PYG{n}{dx} \PYG{o}{/} \PYG{n}{y} \PYG{o}{+} \PYG{p}{(}\PYG{n}{x} \PYG{o}{/} \PYG{p}{(}\PYG{n}{y} \PYG{o}{*} \PYG{n}{y}\PYG{p}{)}\PYG{p}{)} \PYG{o}{*} \PYG{n}{dy}
\end{sphinxVerbatim}

\sphinxAtStartPar
基本表达式法的优缺点显而易见，优点是实现简单直接，可为任意语言快速实现微分的库函数；而缺点是增加了用户的工作量，用户必须先手工分解程序为一些基本表达式，才能使用这些库函数进行编程，无法方便地使用语言原生的表达式。

\sphinxAtStartPar
（2）操作符重载法（Operator Overloading,
OO）：依赖于现代编程语言的多态特性，使用操作符重载对编程语言中的基本操作语义进行重定义，封装其微分规则。每个基本操作类型及其输入关系，在程序运行时会被记录在一个所谓的“tape”的数据结构里面，最后，这些“tape”会形成一个跟踪轨迹(trace)，我们就可以使用链式法则沿着轨迹正向或者反向地将基本操作组成起来进行微分。以自动微分库AutoDiff为例，对编程语言的基本运算操作符进行了重载：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{namespace}\PYG{+w}{ }\PYG{n+nn}{AutoDiff}
\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{k}{public}\PYG{+w}{ }\PYG{n}{abstract}\PYG{+w}{ }\PYG{k}{class} \PYG{n+nc}{Term}
\PYG{+w}{    }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{c+c1}{// 重载操作符 `+`，`*` 和 `/`，调用这些操作符时，会通过其中的}
\PYG{+w}{    }\PYG{c+c1}{// TermBuilder 将操作的类型、输入输出信息等记录至 tape 中}
\PYG{+w}{    }\PYG{k}{public}\PYG{+w}{ }\PYG{k}{static}\PYG{+w}{ }\PYG{n}{Term}\PYG{+w}{ }\PYG{k}{operator}\PYG{o}{+}\PYG{p}{(}\PYG{n}{Term}\PYG{+w}{ }\PYG{n}{left}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{Term}\PYG{+w}{ }\PYG{n}{right}\PYG{p}{)}
\PYG{+w}{    }\PYG{p}{\PYGZob{}}
\PYG{+w}{        }\PYG{k}{return}\PYG{+w}{ }\PYG{n}{TermBuilder}\PYG{p}{.}\PYG{n}{Sum}\PYG{p}{(}\PYG{n}{left}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{right}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{+w}{    }\PYG{k}{public}\PYG{+w}{ }\PYG{k}{static}\PYG{+w}{ }\PYG{n}{Term}\PYG{+w}{ }\PYG{k}{operator}\PYG{o}{*}\PYG{p}{(}\PYG{n}{Term}\PYG{+w}{ }\PYG{n}{left}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{Term}\PYG{+w}{ }\PYG{n}{right}\PYG{p}{)}
\PYG{+w}{    }\PYG{p}{\PYGZob{}}
\PYG{+w}{        }\PYG{k}{return}\PYG{+w}{ }\PYG{n}{TermBuilder}\PYG{p}{.}\PYG{n}{Product}\PYG{p}{(}\PYG{n}{left}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{right}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{+w}{    }\PYG{k}{public}\PYG{+w}{ }\PYG{k}{static}\PYG{+w}{ }\PYG{n}{Term}\PYG{+w}{ }\PYG{k}{operator}\PYG{o}{/}\PYG{p}{(}\PYG{n}{Term}\PYG{+w}{ }\PYG{n}{numerator}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{Term}\PYG{+w}{ }\PYG{n}{denominator}\PYG{p}{)}
\PYG{+w}{    }\PYG{p}{\PYGZob{}}
\PYG{+w}{        }\PYG{k}{return}\PYG{+w}{ }\PYG{n}{TermBuilder}\PYG{p}{.}\PYG{n}{Product}\PYG{p}{(}\PYG{n}{numerator}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{TermBuilder}\PYG{p}{.}\PYG{n}{Power}\PYG{p}{(}\PYG{n}{denominator}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{\PYGZhy{}1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}

\PYG{+w}{    }\PYG{c+c1}{// Tape 数据结构中的基本元素，主要包含：}
\PYG{+w}{    }\PYG{c+c1}{// 1) 操作的运算结果}
\PYG{+w}{    }\PYG{c+c1}{// 2) 操作的运算结果对应的导数结果}
\PYG{+w}{    }\PYG{c+c1}{// 3) 操作的输入}
\PYG{+w}{    }\PYG{c+c1}{// 除此外还通过函数 Eval 和 Diff 定义了该运算操作的计算规则和微分规则}
\PYG{+w}{    }\PYG{n}{internal}\PYG{+w}{ }\PYG{n}{abstract}\PYG{+w}{ }\PYG{k}{class} \PYG{n+nc}{TapeElement}
\PYG{+w}{    }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{k}{public}\PYG{+w}{ }\PYG{k+kt}{double}\PYG{+w}{ }\PYG{n}{Value}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{public}\PYG{+w}{ }\PYG{k+kt}{double}\PYG{+w}{ }\PYG{n}{Adjoint}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{public}\PYG{+w}{ }\PYG{n}{InputEdges}\PYG{+w}{ }\PYG{n}{Inputs}\PYG{p}{;}

\PYG{+w}{    }\PYG{k}{public}\PYG{+w}{ }\PYG{n}{abstract}\PYG{+w}{ }\PYG{k+kt}{void}\PYG{+w}{ }\PYG{n}{Eval}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{public}\PYG{+w}{ }\PYG{n}{abstract}\PYG{+w}{ }\PYG{k+kt}{void}\PYG{+w}{ }\PYG{n}{Diff}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
OO对程序的运行跟踪经过了函数调用和控制流，因此实现起来也是简单直接。而缺点是需要在程序运行时进行跟踪，特别在反向模式上还需要沿着轨迹反向地执行微分，所以会造成性能上的损耗，尤其对于本来运行就很快的基本操作。并且因为其运行时跟踪程序的特性，该方法不允许在运行前做编译时刻的图优化，控制流也需要根据运行时的信息来展开。Pytorch的自动微分框架使用了该方法。

\sphinxAtStartPar
（3）代码变换法（Source
Transformation，ST）：提供对编程语言的扩展，分析程序的源码或抽象语法树（AST），将程序自动地分解为一系列可微分的基本操作，而这些基本操作的微分规则已预定义好，最后使用链式法则对基本操作的微分表达式进行组合生成新的程序表达来完成微分。TensorFlow，MindSpore等机器学习框架都采用了该方式。

\sphinxAtStartPar
不同于OO在编程语言内部操作，ST需要语法分析器（parser）和操作中间表示的工具。除此以外，ST需要定义对函数调用和控制流语句（如循环和条件等）的转换规则。其优势在于对每一个程序，自动微分的转换只做一次，因此不会造成运行时的额外性能损耗。而且，因为整个微分程序在编译时就能获得，编译器可以对微分程序进行进一步的编译优化。但ST实现起来更加复杂，需要扩展语言的预处理器、编译器或解释器，且需要支持更多的数据类型和操作，需要更强的类型检查系统。另外，虽然ST不需要在运行时做自动微分的转换，但是对于反向模式，在反向部分执行时，仍然需要确保前向执行的一部分中间变量可以被获取到，有两种方式可以解决该问题
\DUrole{bibtex}{{[}van2018Automatic{]}} ：

\sphinxAtStartPar
（1）基于Tape的方式。该方式使用一个全局的“tape”去确保中间变量可以被获取到。原始函数被扩展为在前向部分执行时把中间变量写入到tape中的函数，在程序执行反向部分时会从tape中读取这些中间变量。除了存储中间变量外，OO中的tape还会存储执行的操作类型。然而因为tape是一个在运行时构造的数据结构，所以需要添加一些定制化的编译器优化方法。且为了支持高阶微分，对于tape的读写都需要是可微分的。而大多数基于tape的工具都没有实现对tape的读写操作的微分，因此它们都不支持多次嵌套执行反向模式的自动微分（reverse\sphinxhyphen{}over\sphinxhyphen{}reverse）。机器学习框架Tangent采用了该方式。

\sphinxAtStartPar
（2）基于闭包（closure）的方式。基于闭包的方式可以解决基于tape方式的缺陷。在函数式编程里，闭包可以捕获到语句的执行环境并识别到中间变量的非局部使用。因为这些它们是闭包里的自由变量，所以不需要再去定制化编译器优化方法。

\sphinxAtStartPar
MindSpore是使用基于闭包的代码变换法来实现的自动微分的。这需要一个定制的中间表示。MindIR的具体设计，在上一节中已经介绍过，这里不再赘述。

\sphinxAtStartPar
MindSpore的自动微分，使用基于闭包的代码变换法实现，转换程序根据正向部分的计算，构造了一个闭包的调用链。这些闭包包含了计算导数的代码以及从正向部分拿到的中间变量。程序中的每个函数调用，都会得到转换并且额外返回一个叫做“bprop”的函数，\(bprop\)根据给定的关于输出的导数，计算出关于输入的导数。由于每个基本操作的\(bprop\)是已知的，我们可以容易地反向构造出用户定义的整个函数的\(bprop\)。为了支持reverse\sphinxhyphen{}over\sphinxhyphen{}reverse调用去计算高阶导数，我们需要确保可以在已转换好的程序中再进行转换，这需要有处理函数自由变量（函数外定义的变量）的能力。为了达到这个目的，每个\(bprop\)除了关于原始函数输入的偏导数以外，还会返回一系列关于自由变量的偏导数，闭包里面的\(bprop\)负责把每个偏导数解开，将其分别累加贡献到各自的自由变量上。且闭包也是一种函数，可以作为其他闭包的输入。因此，MindSpore自动微分的算法设计可以总结为：

\sphinxAtStartPar
（1）应用链式求导法则，对每个函数（算子或子图）定义一个反向传播函数\(bprop: dout->(df, dinputs)\)，这里\(df\)表示函数对自由变量的导数，\(dinputs\)表示函数对输入的导数。

\sphinxAtStartPar
（2）应用全微分法则，将(\(df\), \(dinputs\))累加到对应的变量上。

\sphinxAtStartPar
涉及控制流语句时，因为MindIR实现了分支、循环和闭包等操作的函数式表达，我们对这些操作应用上述法则进行组合，即可完成微分。定义运算符K求解导数，MindSpore的自动微分算法可以简单表达如下：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{// func和inputs分别表示函数及其输入，dout为关于输出的梯度}
\PYG{n}{v}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{func}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{inputs}\PYG{p}{)}
\PYG{n}{F}\PYG{p}{(}\PYG{n}{v}\PYG{p}{)}\PYG{o}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{p}{(}\PYG{n}{result}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{bprop}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{(}\PYG{n}{func}\PYG{p}{)}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{)}
\PYG{+w}{    }\PYG{n}{df}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{dinputs}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{bprop}\PYG{p}{(}\PYG{n}{dout}\PYG{p}{)}
\PYG{+w}{    }\PYG{n}{v}\PYG{p}{.}\PYG{n}{df}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{=}\PYG{+w}{ }\PYG{n}{df}
\PYG{+w}{    }\PYG{n}{v}\PYG{p}{.}\PYG{n}{dinputs}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{=}\PYG{+w}{ }\PYG{n}{dinputs}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
MindSpore解析器模块首先根据Python的AST生成MindIR，再经过特化模块使得中间表示中的算子可识别，然后调用自动微分模块。自动微分模块的入口函数如下所示：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{function}\PYG{+w}{ }\PYG{n}{Grad}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n}{Init}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{    }\PYG{n}{MapObject}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}\PYG{+w}{  }\PYG{c+c1}{// 实现Parameter/Primitive/FuncGraph/FreeVariable对象的映射}
\PYG{+w}{    }\PYG{n}{MapMorphism}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}\PYG{+w}{  }\PYG{c+c1}{// 实现CNode的映射}
\PYG{+w}{    }\PYG{n}{Finish}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{    }\PYG{n}{Return}\PYG{+w}{ }\PYG{n+nf}{GetKGraph}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}\PYG{+w}{  }\PYG{c+c1}{// 获取梯度函数计算图}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
Grad函数先通过MapObject实现图上自由变量、Parameter和ValueNode（Primitive或FuncGraph）等节点到\(fprop\)的映射。\(fprop\)是\((forward\_result, bprop)\)形式的梯度函数对象。\(forward\_result\)是前向计算图的输出节点，\(bprop\)是以\(fprop\)的闭包对象形式生成的梯度函数，它只有\(dout\)一个入参，其余的输入则是引用的\(fprop\)的输入和输出。其中对于ValueNode<Primitive>类型的\(bprop\)，通过解析Python层预先注册的\(get\_bprop\)函数的得到，如下所示。对于ValueNode<FuncGraph>类型的节点，则递归求出它的梯度函数对象。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@bprop\PYGZus{}getters}\PYG{o}{.}\PYG{n}{register}\PYG{p}{(}\PYG{n}{P}\PYG{o}{.}\PYG{n}{ReLU}\PYG{p}{)}
\PYG{k}{def} \PYG{n+nf}{get\PYGZus{}bprop\PYGZus{}relu}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Grad definition for `ReLU` operation.\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{input\PYGZus{}grad} \PYG{o}{=} \PYG{n}{G}\PYG{o}{.}\PYG{n}{ReluGrad}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{bprop}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{out}\PYG{p}{,} \PYG{n}{dout}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{dx} \PYG{o}{=} \PYG{n}{input\PYGZus{}grad}\PYG{p}{(}\PYG{n}{dout}\PYG{p}{,} \PYG{n}{out}\PYG{p}{)}
        \PYG{k}{return} \PYG{p}{(}\PYG{n}{dx}\PYG{p}{,}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{bprop}
\end{sphinxVerbatim}

\sphinxAtStartPar
随后，MapMorphism函数从原函数的输出节点开始实现对CNode的映射，并建立起节点间的反向传播连接，实现梯度累加，最后返回原函数的梯度函数计算图。


\section{类型系统和静态分析}
\label{\detokenize{chapter_frontend_and_ir/type_system_and_static_analysis:id1}}\label{\detokenize{chapter_frontend_and_ir/type_system_and_static_analysis::doc}}
\sphinxAtStartPar
上一章节介绍了自动微分的基本概念和实现方法，自动微分是机器学习框架中不可或缺的核心功能。在编译器前端的设计中，为了提高编译器的抽象能力和程序运行的正确性，有效减少程序在运行时可能出现的错误，编译器引入了类型系统和静态分析系统，接下来将对它们的基本概念、主要功能、常见系统进行介绍。


\subsection{类型系统概述}
\label{\detokenize{chapter_frontend_and_ir/type_system_and_static_analysis:id2}}
\sphinxAtStartPar
程序设计语言中，类型是指数值、表达式、函数等属性内容。类型系统是指类型的集合以及使用类型来规定程序行为的规则。类型系统用于定义不同的类型，指定类型的操作和类型之间的相互作用，广泛应用于编译器、解释器和静态检查工具中。类型系统提供的主要功能有：

\sphinxAtStartPar
1）正确性。编译器的类型系统引入了类型检查技术，用于检测和避免运行时错误，确保程序运行时的安全性。通过类型推导与检查，编译器能够捕获大多数类型相关的异常报错，避免执行病态程序导致运行时错误，保证内存安全，避免类型间的无效计算和语义上的逻辑错误。

\sphinxAtStartPar
2）优化。静态类型检查可以提供有用的信息给编译器，从而使得编译器可以应用更有效的指令，节省运行时的时间。

\sphinxAtStartPar
3）抽象。在安全的前提下，一个强大的类型系统的标准是抽象能力。通过合理设计抽象，开发者可以更关注更高层次的设计。

\sphinxAtStartPar
4）可读性。阅读代码时，明确的类型声明有助于理解程序代码。

\sphinxAtStartPar
机器学习框架一般使用Python语言作为描述网络模型结构的前端语言。Python语言是一门动态强类型的语言，入门简单易学习，开发代码简洁高效，但由于其解释执行的方式，运行速度往往较慢。Python前端语言给用户带来了动态灵活的语义和高效的开发效率，但是若想要生成运行高效的后端代码，后端框架需要优化友好的静态强类型中间表示。因此，需要一种高效可靠的静态分析方法作为桥梁，将Python前端表示转换成等价的静态强类型中间表示，以此给用户同时带来高效的开发效率和运行效率，例如Hindley–Milner（HM）类型系统。这是一种具有参数多态性的简单类型lambda演算的类型系统。它最初由J.
Roger Hindley 提出 \DUrole{bibtex}{{[}1969The{]}}，并由Robin Milner 进行扩展和验证
\DUrole{bibtex}{{[}1978A{]}} 。后来，路易斯·达马斯（Luis
Damas）对HM类型推导方法进行了详尽的分析和证明
\DUrole{bibtex}{{[}1982Principal{]}}，并将其扩展到支持具有多态引用的系统。Hindley–Milner类型系统的目标是在没有给定类型注解的情况下，自动推导出任意表达式的类型。其算法具有抽象性和通用性，采用简洁的符号表示，能够根据表达式形式推导出明确直观的定义，常用于类型推导和类型检查。因此，Hindley–Milner类型系统广泛应用于编程语言设计中，比如Haskell和Ocaml。


\subsection{静态分析概述}
\label{\detokenize{chapter_frontend_and_ir/type_system_and_static_analysis:id6}}
\sphinxAtStartPar
在设计好类型系统后，编译器需要使用静态分析系统来对中间表示进行静态检查与分析。语法解析模块（parser）将程序代码解析为抽象语法树（AST）并生成中间表示。此时的中间表示缺少类型系统中定义的抽象信息，因此引入静态分析模块，对中间表示进行处理分析，并且生成一个静态强类型的中间表示，用于后续的编译优化、自动并行以及自动微分等。在编译器前端的编译过程中，静态分析可能会被执行多次，有些框架还会通过静态分析的结果判断是否终止编译优化。

\sphinxAtStartPar
静态分析模块基于抽象释义对中间表示进行类型推导、常量传播、泛型特化等操作，这些专业术语的含义分别为：

\sphinxAtStartPar
抽象释义：通过抽象解释器将语言的实际语义近似为抽象语义，只获取后续优化需要的属性，进行不确定性的解释执行。抽象值一般包括变量的类型和维度。

\sphinxAtStartPar
类型推导：在抽象释义的基础上，编译器推断出程序中变量或表达式的抽象类型，方便后续利用类型信息进行编译优化。

\sphinxAtStartPar
泛型特化：泛型特化的前提是编译器在编译期间可以进行类型推导，提供类型的上下文。在编译期间，编译器通过类型推导确定调用函数时的类型，然后，编译器会通过泛型特化，进行类型取代，为每个类型生成一个对应的函数方法。

\sphinxAtStartPar
接下来以MindSpore框架为例，简要介绍一下静态分析模块的具体实现。MindSpore采用抽象释义的方法，对抽象值做不确定的抽象语义的解释执行，函数图中每个节点的抽象值是所期望得到的程序静态信息。基本的抽象释义方法流程可以理解为，从MindIR的顶层函数图入口开始解释执行，将函数图中所有节点进行拓扑排序，根据节点的语义递归推导各节点的抽象值。当遇到函数子图时，递归进入函数子图进行解释执行，最后返回顶层函数输出节点的抽象值。根据抽象释义方法流程，MindSpore的静态分析模块主要分为抽象域模块、缓存模块、语义推导模块和控制流处理模块。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=850\sphinxpxdimen]{{静态分析-静态分析模块}.png}
\caption{静态分析模块}\label{\detokenize{chapter_frontend_and_ir/type_system_and_static_analysis:id7}}\label{\detokenize{chapter_frontend_and_ir/type_system_and_static_analysis:static-analysis-module}}\end{figure}


\section{常见前端编译优化方法}
\label{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:id1}}\label{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass::doc}}
\sphinxAtStartPar
和传统编译器相同，机器学习编译器也会进行编译优化。编译优化意在解决编译生成的中间表示的低效性，使得代码的长度变短，编译与运行的时间减少，执行期间处理器的能耗变低。编译优化可以分为与硬件无关的优化和与硬件相关的编译优化。因为前端是不感知具体后端硬件的，因此前端执行的全部都是与硬件无关的编译优化。


\subsection{前端编译优化简介}
\label{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:id2}}
\sphinxAtStartPar
大多数编译优化器会由一系列的“趟”（Pass）来组成。每个“趟”以中间表示为输入，又以新生成的中间表示为输出。一个“趟”还可以由几个小的“趟”所组成。一个“趟”可以运行一次，也可以运行多次。

\sphinxAtStartPar
在编译优化中，优化操作的选择以及顺序对于编译的整体具有非常关键的作用。优化操作的选择决定了优化器能够感知中间表示中的哪些低效性，也决定了编译器将要如何去重写中间表示以消除这种低效性。优化操作的顺序决定了各趟操作的执行顺序。编译器可以根据具体需要运行不同的编译优化操作。也可以根据编译优化级别来调整优化的次数，种类以及顺序。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{编译优化-pass结构}.svg}
\caption{编译优化的“趟”结构}\label{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:id4}}\label{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:pass-structure}}\end{figure}


\subsection{常见编译优化方法介绍及实现}
\label{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:id3}}
\sphinxAtStartPar
前端编译优化的方法有很多，机器学习框架也有很多不同于传统编译器的优化方式。在本小节当中，我们会介绍三种常见且通用的前端编译优化方法。
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
无用与不可达代码消除

\end{enumerate}

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:pass-useless-code-elimination}]{图\ref{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:pass-useless-code-elimination}}}所示。无用代码是指输出结果没有被任何其他代码所使用的代码。不可达代码是指没有有效的控制流路径包含该代码。删除无用或不可达的代码可以使得中间表示更小，提高程序的编译与执行速度。无用与不可达代码一方面有可能来自于程序编写者的编写失误，也有可能是其他编译优化所产生的结果。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{编译优化-无用代码消除}.svg}
\caption{无用代码消除}\label{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:id5}}\label{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:pass-useless-code-elimination}}\end{figure}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
常量传播、常量折叠

\end{enumerate}

\sphinxAtStartPar
常量传播：如
\hyperref[\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:pass-constant-broadcast}]{图\ref{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:pass-constant-broadcast}}}所示，如果某些量为已知值的常量，那么可以在编译时刻将使用这些量的地方进行替换。

\sphinxAtStartPar
常量折叠：如
\hyperref[\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:pass-constant-broadcast}]{图\ref{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:pass-constant-broadcast}}}所示，多个量进行计算时，如果能够在编译时刻直接计算出其结果，那么变量将由常量替换。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{编译优化-常量传播与常量折叠}.svg}
\caption{常量传播与常量折叠}\label{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:id6}}\label{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:pass-constant-broadcast}}\end{figure}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{2}
\item {} 
\sphinxAtStartPar
公共子表达式消除

\end{enumerate}

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:pass-cse}]{图\ref{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:pass-cse}}}所示，如果一个表达式E已经计算过了，并且从先前的计算到现在E中所有变量的值都没有发生变化，那么E就成为了公共子表达式。对于这种表达式，没有必要花时间再对它进行计算，只需要直接用前面计算过的表达式结果代替E就可以了。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{编译优化-公共子表达式消除}.svg}
\caption{公共子表达式消除}\label{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:id7}}\label{\detokenize{chapter_frontend_and_ir/common_frontend_optimization_pass:pass-cse}}\end{figure}


\section{总结}
\label{\detokenize{chapter_frontend_and_ir/summary:id1}}\label{\detokenize{chapter_frontend_and_ir/summary::doc}}\begin{itemize}
\item {} 
\sphinxAtStartPar
中间表示是编译器的核心数据结构之一，是程序编译过程中介于源语言和目标语言之间的程序表示。

\item {} 
\sphinxAtStartPar
传统编译器的中间表示从组织结构出发，可以分为线性中间表示，图中间表示以及混合中间表示。

\item {} 
\sphinxAtStartPar
机器学习框架对中间表示有一系列新的需求，这些新的需求是传统中间表示所不能完美支持的。因此需要在传统中间表示的基础上扩展新的，更适用于机器学习框架的中间表示。

\item {} 
\sphinxAtStartPar
自动微分的基本思想是将计算机程序中的运算操作分解为一个有限的基本操作集合，且集合中基本操作的求导规则均为已知，在完成每一个基本操作的求导后，使用链式法则将结果组合得到整体程序的求导结果。

\item {} 
\sphinxAtStartPar
自动微分根据链式法则的组合顺序，可以分为前向自动微分与反向自动微分。

\item {} 
\sphinxAtStartPar
前向自动微分更适用于对输入维度小于输出维度的网络求导，反向自动微分则更适用于对输出维度小于输入维度的网络求导。

\item {} 
\sphinxAtStartPar
自动微分的实现方法大体上可以划分为基本表达式法、操作符重载法以及代码变化法。

\item {} 
\sphinxAtStartPar
类型系统是指类型的集合以及使用类型来规定程序行为的规则，用于定义不同的类型，指定类型的操作和类型之间的相互作用，广泛应用于编译器、解释器和静态检查工具中。

\item {} 
\sphinxAtStartPar
静态分析，是指在不实际运行程序的情况下，通过词法分析、语法分析、控制流、数据流分析等技术对代码进行分析验证的技术

\item {} 
\sphinxAtStartPar
编译优化意在解决编译生成的中间表示的低效性，前端执行的均为与硬件无关的编译优化。

\end{itemize}


\section{扩展阅读}
\label{\detokenize{chapter_frontend_and_ir/summary:id2}}\begin{itemize}
\item {} 
\sphinxAtStartPar
一种基于图的中间表示类型：
\sphinxhref{https://dl.acm.org/doi/10.1145/202530.202534}{综述}%
\begin{footnote}[15]\sphinxAtStartFootnote
\sphinxnolinkurl{https://dl.acm.org/doi/10.1145/202530.202534}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
机器学习框架中的自动微分：
\sphinxhref{https://arxiv.org/abs/1502.05767}{综述}%
\begin{footnote}[16]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/1502.05767}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
函数式框架中的反向自动微分：
\sphinxhref{https://dl.acm.org/doi/10.1145/1330017.1330018}{综述}%
\begin{footnote}[17]\sphinxAtStartFootnote
\sphinxnolinkurl{https://dl.acm.org/doi/10.1145/1330017.1330018}
%
\end{footnote}

\end{itemize}


\chapter{编译器后端和运行时}
\label{\detokenize{chapter_backend_and_runtime/index:id1}}\label{\detokenize{chapter_backend_and_runtime/index::doc}}
\sphinxAtStartPar
在上一章节，我们详细讲述了一个编译器前端的主要功能，重点介绍了中间表示以及自动微分。在得到中间表示后，如何充分利用硬件资源高效地执行，是编译器后端和运行时要解决的问题。

\sphinxAtStartPar
在本章节中，
我们将会介绍编译器后端的一些基本概念，详细描述后端的计算图优化、算子选择等流程。通过对编译器前端提供的中间表示进行优化，充分发挥硬件能力，从而提高程序的执行效率。在此基础上，介绍运行时是如何对计算任务进行内存分配以及高效的调度执行。

\sphinxAtStartPar
本章的学习目标包括：
\begin{itemize}
\item {} 
\sphinxAtStartPar
了解编译器后端和运行时的作用

\item {} 
\sphinxAtStartPar
掌握计算图优化的常用方法

\item {} 
\sphinxAtStartPar
掌握算子选择的常用方法

\item {} 
\sphinxAtStartPar
掌握内存分配的常用方法

\item {} 
\sphinxAtStartPar
掌握计算图调度和执行的常用方法

\end{itemize}


\section{概述}
\label{\detokenize{chapter_backend_and_runtime/overview:id1}}\label{\detokenize{chapter_backend_and_runtime/overview::doc}}
\sphinxAtStartPar
编译器前端主要将用户代码进行解析翻译得到计算图IR并对其进行设备信息无关的优化，此时我们并不考虑程序执行的底层硬件信息。编译器后端的主要职责对前端下发的IR做进一步的计算图优化，让其更加贴合硬件，并为IR中的计算节点选择适合在硬件上执行的算子，然后为每个算子的输入输出分配硬件内存，最终生成一个可以在硬件上执行的任务序列。

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_backend_and_runtime/overview:compiler-backend-architecture}]{图\ref{\detokenize{chapter_backend_and_runtime/overview:compiler-backend-architecture}}}所示，编译器后端处于前端和硬件驱动层中间，主要负责计算图优化、算子选择和内存分配的任务。首先，需要根据硬件设备的特性将IR图进行等价图变换，以便在硬件上能够找到对应的执行算子，该过程是计算图优化的重要步骤之一。前端IR生成是解析用户代码，属于一个较高的抽象层次，隐藏一些底层运行的细节信息，此时无法直接对应硬件上的算子（算子是设备上的基本计算序列，例如MatMul、Convolution和ReLU等），需要将细节信息进行展开后，才能映射到目标硬件上的算子。对于某些前端IR的子集来说，一个算子便能够执行对应的功能，此时可以将这些IR节点合并成为一个计算节点，该过程称之为算子融合；对于一些复杂计算，后端并没有直接与之对应的算子，但是可以通过几个基本运算的算子组合达到同样的计算效果，此时可以将前端IR节点拆分成多个小算子。然后，我们需要进行算子选择。算子选择是在得到优化的IR图后，需要选取最合适的目标设备算子。针对用户代码所产生的IR往往可以映射成多种不同的硬件算子，但是生成不同的算子执行效率往往有很大的差别，如何根据前端IR选择出最高效的算子，是算子选择的核心问题。算子选择本质上是一个模式匹配问题。其最简单的方法就是每一个IR节点对应一个目标硬件的算子，但是这种方法往往对目标硬件的资源利用比较差。目前来说对于现有的编译器一般都对每一个IR节点提供了多个候选的算子，算子选择目标就是从中选择最优的一个算子作为最终执行在设备上的算子。总的来说，在机器学习系统中，对前端生成的IR图上的各个节点进行拆分和融合，让前端所表示的高层次IR逐步转换为可以在硬件设备上执行的低层次IR。得到了这种更加贴合硬件的IR后，对于每个单节点的IR可能仍然有很多种不同的选择，例如可以选择不同的输入输出格式和数据类型，我们需要对IR图上每个节点选择出最为合适的算子，算子选择过程可以认为是针对IR图的细粒度优化过程，最终生成完整的算子序列。最后，遍历算子序列，为每个算子分配相应的输入输出内存，然后将算子加载到设备上执行计算。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{compiler-backend-architecture}.png}
\caption{编译器后端总体架构简图}\label{\detokenize{chapter_backend_and_runtime/overview:id6}}\label{\detokenize{chapter_backend_and_runtime/overview:compiler-backend-architecture}}\end{figure}


\subsection{计算图优化}
\label{\detokenize{chapter_backend_and_runtime/overview:id2}}
\sphinxAtStartPar
计算图优化是在不影响模型的数值特性的基础上，通过图变换达到简化计算、减少资源开销、适配硬件的执行能力、提升执行性能的目的。


\subsection{算子选择}
\label{\detokenize{chapter_backend_and_runtime/overview:id3}}
\sphinxAtStartPar
算子选择是将IR图上的每个计算节点映射到设备上可执行算子的过程，一个IR图上的计算节点往往可以对应多个设备上的算子，这个过程中需要考虑算子的规格，算子的执行效率等问题，算子选择目标就是从中选择最优的一个算子。


\subsection{内存分配}
\label{\detokenize{chapter_backend_and_runtime/overview:id4}}
\sphinxAtStartPar
经过计算图优化和算子选择之后，我们可以得到IR图中每个算子的输入输出的形状（Shape）、数据类型、存储格式。根据这些信息，计算输入输出数据的大小，并为输入输出分配设备上的内存，然后将算子加载到设备上才能真正执行计算。此外，为了更充分地例用设备内存资源，可以对内存进行复用，提高内存利用率。


\subsection{计算调度与执行}
\label{\detokenize{chapter_backend_and_runtime/overview:id5}}
\sphinxAtStartPar
经过算子选择与内存分配之后，计算任务可以通过运行时完成计算的调度与在硬件上的执行。根据是否将算子编译为计算图，计算的调度可以分为单算子调度与计算图调度两种方式。而根据硬件提供的能力差异，计算图的执行方式又可以分为逐算子下发执行的交互式执行以及将整个计算图或者部分子图一次性下发到硬件的下沉式执行两种模式。


\section{计算图优化}
\label{\detokenize{chapter_backend_and_runtime/graph_optimizer:id1}}\label{\detokenize{chapter_backend_and_runtime/graph_optimizer::doc}}
\sphinxAtStartPar
后端的计算图优化主要是针对硬件的优化，根据优化适用于所有硬件还是只适合特定硬件，可以分为通用硬件优化和特定硬件优化，例如为了适配硬件指令限制而做的子图变换和与特定硬件无关的算子内存IO优化。


\subsection{通用硬件优化}
\label{\detokenize{chapter_backend_and_runtime/graph_optimizer:id2}}
\sphinxAtStartPar
通用硬件优化主要指与特定硬件类型无关系的计算图优化，优化的核心是子图的等价变换：在计算图中尝试匹配特定的子图结构，找到目标子图结构后，通过等价替换方式，将其替换成对硬件更友好的子图结构。

\sphinxAtStartPar
以优化内存IO为例。深度学习算子按其对资源的需求可以分为两类：
计算密集型算子，这些算子的时间绝大部分花在计算上，如卷积、全连接等；
访存密集型算子，这些算子的时间绝大部分花在访存上，他们大部分是Element\sphinxhyphen{}Wise算子，例如
ReLU、Element\sphinxhyphen{}Wise Sum等。
在典型的深度学习模型中，一般计算密集型和访存密集型算子是相伴出现的，最简单的例子是“Conv
+
ReLU”。Conv卷积算子是计算密集型，ReLU算子是访存密集型算子，ReLU算子可以直接取Conv算子的计算结果进行计算，因此我们可以将二者融合成一个算子来进行计算，从而减少内存访问延时和带宽压力，提高执行效率。

\sphinxAtStartPar
例如：“Conv + Conv + Sum + ReLU”的融合，从
\hyperref[\detokenize{chapter_backend_and_runtime/graph_optimizer:conv-sum-relu}]{图\ref{\detokenize{chapter_backend_and_runtime/graph_optimizer:conv-sum-relu}}}中我们可以看到融合后的算子减少了两个内存的读和写的操作，优化了Conv的输出和Sum的输出的读和写的操作。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{conv_sum_relu}.png}
\caption{Elementwise算子融合}\label{\detokenize{chapter_backend_and_runtime/graph_optimizer:id4}}\label{\detokenize{chapter_backend_and_runtime/graph_optimizer:conv-sum-relu}}\end{figure}

\sphinxAtStartPar
除了上述针对特定算子类型结构的融合优化外，基于自动算子生成技术，还可以实现更灵活、更极致的通用优化。以
MindSpore
的图算融合技术为例，图算融合通过“算子拆解、算子聚合、算子重建”三个主要阶段（如图）让计算图中的计算更密集，并进一步减少低效的内存访问。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{graph_kernel}.png}
\caption{图算融合}\label{\detokenize{chapter_backend_and_runtime/graph_optimizer:id5}}\label{\detokenize{chapter_backend_and_runtime/graph_optimizer:graph-kernel}}\end{figure}

\sphinxAtStartPar
\hyperref[\detokenize{chapter_backend_and_runtime/graph_optimizer:graph-kernel}]{图\ref{\detokenize{chapter_backend_and_runtime/graph_optimizer:graph-kernel}}}中，算子拆解阶段（Expander）将计算图中一些复杂算子（composite
op，图中Op1、Op3、Op4）展开为计算等价的基本算子组合（
图中虚线正方形框包围着的部分）；在算子聚合阶段（Aggregation），将计算图中将基本算子（basic
op，如图中Op2）、拆解后的算子（expanded
op）组合融合，形成一个更大范围的算子组合；在算子重建阶段（Reconstruction）中，按照输入tensor到输出tensor的仿射关系将基本算子进行分类：elemwise、
broadcast、reduce、transform等，并在这基础上归纳出不同的通用计算规则（如
elemwise + reduce 规则：elemwise +
reduce在满足一定条件后可以高效执行），根据这些计算规则不断地从这个大的算子组合上进行分析、筛选，最终重新构建成新的算子（如图中虚线正方形包围的两个算子
New Op1 和 New
Op2）。图算融合通过对计算图结构的拆解和聚合，可以实现跨算子边界的联合优化；并在算子重建中，通过通用的计算规则，以必要的访存作为代价，生成对硬件更友好、执行更高效的新算子。


\subsection{特定硬件优化}
\label{\detokenize{chapter_backend_and_runtime/graph_optimizer:id3}}
\sphinxAtStartPar
特定硬件优化是指该计算图的优化是在特定硬件上才能做的优化，常见的基于硬件的优化包括由于硬件指令的限制而做的优化，特定硬件存储格式导致的优化等。

\sphinxAtStartPar
1、硬件指令限制

\sphinxAtStartPar
在一些特定的硬件上，IR中计算节点没有直接对应的硬件算子，只能通过子图的变换来达到子图中所有算子在对应的硬件上的存在。例如在MindSpore中，昇腾芯片上的Concat算子，只支持有限的输入个数（63个），因此当前端IR上的输入个数大于限制输入的时候，需要将该计算节点拆分成等价的多个Concat节点，如
\hyperref[\detokenize{chapter_backend_and_runtime/graph_optimizer:concat}]{图\ref{\detokenize{chapter_backend_and_runtime/graph_optimizer:concat}}}所示：
当Concat有100个输入时，单个算子只支持最多63个输入，此时会将该计算节点拆分成两个Concat节点，分别为63个输入和37个输入的两个算子。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{concat}.png}
\caption{Concat算子拆分}\label{\detokenize{chapter_backend_and_runtime/graph_optimizer:id6}}\label{\detokenize{chapter_backend_and_runtime/graph_optimizer:concat}}\end{figure}

\sphinxAtStartPar
2、数据排布格式的限制

\sphinxAtStartPar
针对不同特点的计算平台和不同的算子，为了追求最好的性能，一般都需要选择不同的数据排布格式（Format），而这些排布格式可能跟框架缺省的排布格式是不一样的。在这种情况下，一般的做法是算子在执行完成后对输出插入一个格式转换操作，把排布格式转换回框架的缺省排布格式，这就引入了额外的内存操作。以
\hyperref[\detokenize{chapter_backend_and_runtime/graph_optimizer:transdata}]{图\ref{\detokenize{chapter_backend_and_runtime/graph_optimizer:transdata}}}为例，在昇腾平台上Conv算子在输入和输出的内存排布为5HD时是性能最优的，所以可以看到Conv算子输出结果的格式是5HD，然后通过一个转换操作转回了框架缺省的NCHW，紧接着，后面又是一个Conv算子，它需要5HD的输入，所以又做了一个NCHW到5HD的转换。我们很容易看出，虚线框内的两个转换操作互为逆操作，可以相互抵消。通过对计算图的模式匹配，可以将该类型的操作消除。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{transdata}.png}
\caption{数据排布格式转换消除}\label{\detokenize{chapter_backend_and_runtime/graph_optimizer:id7}}\label{\detokenize{chapter_backend_and_runtime/graph_optimizer:transdata}}\end{figure}


\section{算子选择}
\label{\detokenize{chapter_backend_and_runtime/kernel_selecter:id1}}\label{\detokenize{chapter_backend_and_runtime/kernel_selecter::doc}}
\sphinxAtStartPar
经过计算图优化后，需要对IR图上的每个节点进行算子选择，才能生成真正在设备上执行的算子序列。由于IR图上的节点可能有后端的很多算子与其对应，不同规格的算子在不同的情况下执行效率各不相同，在算子选择阶段的主要任务就是如何根据IR图中的信息在众多算子中选择出最合适的一个算子去目标设备上执行。


\subsection{算子选择的基础概念}
\label{\detokenize{chapter_backend_and_runtime/kernel_selecter:id2}}
\sphinxAtStartPar
经历了后端的图优化后，IR图中的每一个节点都有一组算子与之对应。此时的IR图中的每一个节点可以认为是用户可见的最小硬件执行单元。但是此时IR图中的一个节点代表了用户代码的一个操作，对于这个操作还没有具体生成有关设备信息的细节描述。这些信息是算子选择所选择的内容信息，我们称之为算子信息。算子信息主要包括以下内容：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
针对不同特点的计算平台和不同的算子，为了追求最好的性能，一般都需要选择不同的数据排布格式。机器学习系统常见的数据排布格式有NCHW和NHWC等。

\item {} 
\sphinxAtStartPar
对于不同的硬件支持不同的计算精度，例如float32、float16和int32等。算子选择需要在所支持各种数据类型的算子中选择出用户所设定的数据类型最为相符的算子。

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{数据排布格式}

\sphinxAtStartPar
机器学习系统中很多运算都会转换成为矩阵的乘法，例如卷积运算。我们知道矩阵乘法\(A\times B = C\)
是以A的一行乘以B的一列求和后得到C的一个元素。以
\hyperref[\detokenize{chapter_backend_and_runtime/kernel_selecter:matmuldatalayout}]{图\ref{\detokenize{chapter_backend_and_runtime/kernel_selecter:matmuldatalayout}}}为例，在
\hyperref[\detokenize{chapter_backend_and_runtime/kernel_selecter:matmuldatalayout}]{图\ref{\detokenize{chapter_backend_and_runtime/kernel_selecter:matmuldatalayout}}}的上方，矩阵数据的存储是按照行优先来进行存储，虽然B在存储时是按照行存储，但是读取数据时却按照列进行读取，假如我们能把B的格式进行转换转换为列存储，例如
\hyperref[\detokenize{chapter_backend_and_runtime/kernel_selecter:matmuldatalayout}]{图\ref{\detokenize{chapter_backend_and_runtime/kernel_selecter:matmuldatalayout}}}下方所示，这样就可以通过访问连续内存的方式加快数据访问速度进而提升运算速度。由此可见不同的数据排布方式对性能有很大影响。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{matmuldatalayout}.png}
\caption{矩阵乘法数据排布示意图}\label{\detokenize{chapter_backend_and_runtime/kernel_selecter:id4}}\label{\detokenize{chapter_backend_and_runtime/kernel_selecter:matmuldatalayout}}\end{figure}

\sphinxAtStartPar
在机器学习系统中我们常见的数据格式一般有两种，分别为NCHW类型和NHWC类型。其中N代表了数据输入的BatchSize大小，C代表了图像的通道，H和W分别代表图像输入的长和宽。
\hyperref[\detokenize{chapter_backend_and_runtime/kernel_selecter:data-format}]{图\ref{\detokenize{chapter_backend_and_runtime/kernel_selecter:data-format}}}展示了BatchSize为2，通道数16和长度为5*4大小的数据逻辑示意图。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{data_format}.png}
\caption{常见数据格式}\label{\detokenize{chapter_backend_and_runtime/kernel_selecter:id5}}\label{\detokenize{chapter_backend_and_runtime/kernel_selecter:data-format}}\end{figure}

\sphinxAtStartPar
但是计算机的存储并不能够直接将这样的矩阵放到内存中，需要将其展平成1维后存储，这样就涉及我们逻辑上的索引如何映射成为内存中的索引，即我们如何根据逻辑数据索引来映射到内存中的1维数据索引。

\sphinxAtStartPar
对于NCHW的数据是先取W轴方向数据，再取H轴方向数据，再取C轴方向，最后取N轴方向。其中物理存储与逻辑存储的之间的映射关系为
\begin{equation}\label{equation:chapter_backend_and_runtime/kernel_selecter:chapter_backend_and_runtime/kernel_selecter:0}
\begin{split}offsetnchw(n,c,h,w) = n*CHW + c*HW + h*W +w\end{split}
\end{equation}
\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_backend_and_runtime/kernel_selecter:nchw}]{图\ref{\detokenize{chapter_backend_and_runtime/kernel_selecter:nchw}}}所示，这种格式中，是按照最低维度W轴方向进行展开，W轴相邻的元素在内存排布中同样是相邻的。如果需要取下一个图片上的相同位置的元素，就必须跳过整个图像的尺寸（\(C*H*W\)）。比如我有8张32*32的RGB图像，此时\(N=8,C=3,H=32,W=32\)。在内存中存储它们需要先按照W轴方向进行展开，然后按照H轴排列，这样之后便完成了一个通道的处理，之后按照同样的方式处理下一个通道。处理完全部通道后，处理下一张图片。PyTorch和MindSpore框架默认使用NCHW格式。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{nchw}.png}
\caption{RGB图片下的NHWC数据格式}\label{\detokenize{chapter_backend_and_runtime/kernel_selecter:id6}}\label{\detokenize{chapter_backend_and_runtime/kernel_selecter:nchw}}\end{figure}

\sphinxAtStartPar
类似的NHWC数据格式是先取C方向数据，再取W方向，然后是H方向，最后取N方向。NHWC是Tensorflow默认的数据格式。这种格式在PyTorch中称为Channel\sphinxhyphen{}Last。
\begin{equation}\label{equation:chapter_backend_and_runtime/kernel_selecter:chapter_backend_and_runtime/kernel_selecter:1}
\begin{split}offsetnhwc(n,h,w,c) = n*HWC + h*WC + w*C +c\end{split}
\end{equation}
\sphinxAtStartPar
\hyperref[\detokenize{chapter_backend_and_runtime/kernel_selecter:nchwandnhwc}]{图\ref{\detokenize{chapter_backend_and_runtime/kernel_selecter:nchwandnhwc}}}展示了不同数据格式下逻辑排布到内存物理侧数据排布的映射。{[}x:1{]}代表从最内侧维度到最下一维度的索引变换。比如{[}a:1{]}表示当前行W轴结束后，下一个H轴排布。{[}b:1{]}表示最内侧C轴排布完成后进行按照W轴进行排列。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{nchwandnhwc}.png}
\caption{NCHW与NHWC数据存储格式}\label{\detokenize{chapter_backend_and_runtime/kernel_selecter:id7}}\label{\detokenize{chapter_backend_and_runtime/kernel_selecter:nchwandnhwc}}\end{figure}

\sphinxAtStartPar
上述的数据存储格式具有很大的灵活性，很多框架都采用上述的两种格式作为默认的数据排布格式。但是在硬件上对数据操作时，此时的数据排布可能还不是最优的。在机器学习系统中，用户输入的数据往往会远远大于计算部件一次性计算所能容纳的最大范围，所以此时必须将输入的数据进行切片分批送到运算部件中进行运算。为了加速运算很多框架又引入了一些块布局格式来进行进一步的优化，这种优化可以使用一些硬件的加速指令，对数据进行搬移和运算。比如oneDNN上的nChw16c
和nChw8c
格式，以及Ascend芯片的5HD等格式。这种特殊的数据格式与硬件更为贴合，可以快速的将矩阵向量化，并且极大的利用片内缓存。

\sphinxAtStartPar
\sphinxstylestrong{数据精度}

\sphinxAtStartPar
通常深度学习的系统，一般使用的是单精度float（Single
Precision）浮点表示。这种数据类型占用32位内存。还有一种精度较低的数据类型float16，其内部占用了16位的内存。由于很多硬件会对float16数据类型进行优化，float16半精度的计算吞吐量可以是float32的\(2\sim 8\)倍，且float16可以占用的数据更小，这样可以输入更大的BatchSize，进而减少总体训练时间。接下来我们详细看一下半精度浮点数与单精度浮点数的区别。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{floatdtype}.png}
\caption{浮点数的二进制表示}\label{\detokenize{chapter_backend_and_runtime/kernel_selecter:id8}}\label{\detokenize{chapter_backend_and_runtime/kernel_selecter:floatdtype}}\end{figure}

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_backend_and_runtime/kernel_selecter:floatdtype}]{图\ref{\detokenize{chapter_backend_and_runtime/kernel_selecter:floatdtype}}}其中sign代表符号位，占1位，表示了机器数的正负，exponent表示指数位，Mantissa为尾数位。其中float16类型的数据采用二进制的科学计数法转换为十进制的计算方式如下：
\begin{equation}\label{equation:chapter_backend_and_runtime/kernel_selecter:chapter_backend_and_runtime/kernel_selecter:2}
\begin{split}(-1)^{sign}\times 2^{exponent-15}\times (\frac{mantissa}{1024}+1)\end{split}
\end{equation}
\sphinxAtStartPar
其中如果指数位全为0时，且尾数位全为0时表示数字0。
如果指数位全为0，尾数位不全为0则表示一个非常小的数值。
当指数全为1，尾数位全为0表示根据符号位正无穷大，或者负无穷大。
若指数全为1，但是尾数位不为0，则表示NAN。
其中bfloat16并不属于一个通用的数据类型，是google提出的一种特殊的类型，现在一般只在一些TPU上训练使用，其指数位数与float32位数保持一致，可以较快的与float32进行数据转换。由于并不是一种通用类型。IEEE中也并没有提出该类型的标准。

\sphinxAtStartPar
\sphinxstylestrong{算子信息库}

\sphinxAtStartPar
前面我们讲述了数据格式和数据精度的概念，基于这两个概念，在不同硬件下会有不同的算子支持，此时需要有一个硬件上支持的所有算子的集合，该集合我们称之为算子信息库。算子选择过程就是从算子信息库中选择最合适的算子的过程。


\subsection{算子选择的过程}
\label{\detokenize{chapter_backend_and_runtime/kernel_selecter:id3}}
\sphinxAtStartPar
前文介绍了算子选择主要是针对IR图中的每一个操作节点选择出最为合适的算子。其中算子信息主要包括了支持设备类型、数据类型和数据排布格式三个方面。经过编译器前端类型推导与静态分析的阶段后，IR图中已经推导出了用户代码侧的数据类型。下面介绍算子选择的基本过程。

\sphinxAtStartPar
首先，选择算子执行的硬件设备。不同的硬件设备上，算子的实现、支持数据类型、执行效率通常会有所差别。这一步往往是用户自己指定的，若用户未指定，则编译器后端会为用户匹配一个默认的设备。
然后，后端会根据IR图中推导出的数据类型和内存排布格式选择对应的算子。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{select_kernel}.png}
\caption{算子选择过程}\label{\detokenize{chapter_backend_and_runtime/kernel_selecter:id9}}\label{\detokenize{chapter_backend_and_runtime/kernel_selecter:select-kernel}}\end{figure}

\sphinxAtStartPar
理想情况下算子选择所选择出的算子类型，应该与用户预期的类型保持一致。但是由于软硬件的限制，很可能算子的数据类型不能满足用户所期待的数据类型，此时需要对该节点进行升精度或者降精度处理才能匹配到合适的算子。比如在MindSpore
的Ascend后端由于硬件限制导致Conv2D算子只存在float16一种数据类型。如果用户设置的整网使用的数据类型为float32数据，那么只能对Conv2D算子的输入数据进行降精度处理，即将输入数据类型从float32转换成float16。

\sphinxAtStartPar
算子的数据排布格式转换是一个比较耗时的操作，为了避免频繁的格式转换所带来的内存搬运开销，数据应该尽可能地以同样的格式在算子之间传递，算子和算子的衔接要尽可能少的出现数据排布格式不一致的现象。另外，数据类型不同导致的降精度可能会使得误差变大，收敛速度变慢甚至不收敛，所以数据类型的选择也要结合具体算子分析。

\sphinxAtStartPar
总的来说，一个好的算子选择算法应该尽可能的保持数据类型与用户设置的数据类型一致，且尽可能少的出现数据格式转换。


\section{内存分配}
\label{\detokenize{chapter_backend_and_runtime/memory_allocator:ch05-sec-memory-pool}}\label{\detokenize{chapter_backend_and_runtime/memory_allocator:id1}}\label{\detokenize{chapter_backend_and_runtime/memory_allocator::doc}}
\sphinxAtStartPar
内存在传统计算机存储器层次结构中有着重要的地位，它是连接高速缓存和磁盘之间的桥梁，有着比高速缓存更大的空间，比磁盘更快的访问速度。随着深度学习的发展，深度神经网络的模型越来越复杂，AI芯片上的内存很可能无法容纳一个大型网络模型。因此，对内存进行复用是一个重要的优化手段。此外，通过连续内存分配和
In\sphinxhyphen{}Place内存分配还可以提高某些算子的执行效率。


\subsection{Device内存概念}
\label{\detokenize{chapter_backend_and_runtime/memory_allocator:device}}
\sphinxAtStartPar
在深度学习体系结构中，我们通常将与硬件加速器（如GPU、AI芯片等）相邻的内存称之为设备（Device）内存，而与CPU相邻的内存称之为主机（Host）内存。如
\hyperref[\detokenize{chapter_backend_and_runtime/memory_allocator:host-device-memory}]{图\ref{\detokenize{chapter_backend_and_runtime/memory_allocator:host-device-memory}}}所示，CPU可以合法地访问主机上的内存，而无法直接访问设备上的内存；同理，AI芯片可以访问设备上的内存，却无法访问主机上的内存。因此，在网络训练过程中，我们往往需要从磁盘加载数据到主机内存中，然后在主机内存中做数据处理，再从主机内存拷贝到设备内存中，最后设备才能合法地访问数据。算子全部计算完成后，用户要获取训练结果，又需要把数据从设备内存拷贝到主机内存中。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{host-device-memory}.png}
\caption{主机内存和设备内存}\label{\detokenize{chapter_backend_and_runtime/memory_allocator:id7}}\label{\detokenize{chapter_backend_and_runtime/memory_allocator:id2}}\label{\detokenize{chapter_backend_and_runtime/memory_allocator:host-device-memory}}\end{figure}


\subsection{内存分配}
\label{\detokenize{chapter_backend_and_runtime/memory_allocator:id3}}
\sphinxAtStartPar
内存分配模块主要负责给图中算子的输入、输出分配Device内存。用户的前端脚本经过编译器前端处理后得到中间表达，后端根据中间表达进行算子选择和相关优化，可以得到算子最终的输入输出Tensor的形状、数据类型（Data
Type）、格式（Format）等信息，根据这些信息我们可以计算出算子输入、输出Tensor的尺寸大小。基本的计算方法为：
\begin{equation}\label{equation:chapter_backend_and_runtime/memory_allocator:chapter_backend_and_runtime/memory_allocator:0}
\begin{split}size=\left (\prod_{i=0}^{dimension}shape_i\right ) * sizeof\left ( data type \right )\end{split}
\end{equation}
\sphinxAtStartPar
得到Tensor的尺寸大小后，往往还需要对内存大小进行对齐操作。内存通常以4字节、8字节或16字节为一组进行访问，如果被搬运的内存大小不是这些值的倍数，内存后面会填充相应数量的空数据以使得内存长度达到这些值的倍数。因此，访问非对齐的内存可能会更加耗时。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{memory_allocate}.png}
\caption{内存分配示例}\label{\detokenize{chapter_backend_and_runtime/memory_allocator:id8}}\label{\detokenize{chapter_backend_and_runtime/memory_allocator:memory-allocate}}\end{figure}

\sphinxAtStartPar
下面以
\hyperref[\detokenize{chapter_backend_and_runtime/memory_allocator:memory-allocate}]{图\ref{\detokenize{chapter_backend_and_runtime/memory_allocator:memory-allocate}}}为例介绍内存分配的大致流程。首先我们会给Input
Tensor、Conv2D的权重和Conv2D的输出分配内存地址。然后为BatchNorm的输入分配地址时，我们发现BatchNorm的输入就是Conv2D算子的输出，而该Tensor的地址已经在之前分配过了，因此只需要将Conv2D算子的输出地址共享给BatchNorm的输入，就可以避免内存的重复申请以及内存的冗余拷贝。以此类推，可以发现整个过程中可以将待分配的内存分成三种类型：一是整张图的输入Tensor，二是算子的权重或者属性，三是算子的输出Tensor，三种类型在训练过程中的生命周期有所不同。

\sphinxAtStartPar
在CPU上我们常常使用malloc函数直接申请内存，这种方式申请内存好处是随时申请随时释放，简单易用。然而在许多对性能要求严苛的计算场景中，由于所申请内存块的大小不定，频繁申请释放会降低性能。通常我们会使用内存池的方式去管理内存，先申请一定数量的内存块留作备用，当程序有内存申请需求时，直接从内存池中的内存块中申请。当程序释放该内存块时，内存池会进行回收并用作后续程序内存申请时使用。
在深度学习框架中，Device内存的申请也是非常频繁的，往往也是通过内存池的方式去管理Device内存，并让Device内存的生命周期与Tensor的生命周期保持一致。不同的深度学习框架在内存池的设计上大同小异，我们以
\hyperref[\detokenize{chapter_backend_and_runtime/memory_allocator:device-malloc}]{图\ref{\detokenize{chapter_backend_and_runtime/memory_allocator:device-malloc}}}的MindSpore框架内存申请为例，进程会从Device上申请足够大的内存，然后通过双游标从两端偏移为Tensor分配内存。首先从申请的首地址开始进行偏移，为算子权重的Tensor分配内存，这部分Tensor生命周期较长，往往持续整个训练过程。然后从申请Device地址的末尾开始偏移，为算子的输出Tensor分配内存，这部分内存的生命周期较短，往往在该算子计算结束并且后续计算过程中无需再次使用该算子的输出的情况下，其生命周期就可以结束。通过这种方式，我们只需要从Device上申请一次足够大的内存，后续算子的内存分配都是通过指针偏移进行分配，减少了直接从设备申请内存的耗时。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{device_malloc}.png}
\caption{双游标法分配内存}\label{\detokenize{chapter_backend_and_runtime/memory_allocator:id9}}\label{\detokenize{chapter_backend_and_runtime/memory_allocator:device-malloc}}\end{figure}


\subsection{内存复用}
\label{\detokenize{chapter_backend_and_runtime/memory_allocator:id4}}
\sphinxAtStartPar
在机器学习系统中，内存复用是指分析Tensor的生命周期，将生命周期结束的Tensor的Device内存释放回内存池并用于后续Tensor的内存分配。内存复用的目的是提高内存的利用率，让有限的设备内存容纳更大的模型。
以
\hyperref[\detokenize{chapter_backend_and_runtime/memory_allocator:memory-allocate}]{图\ref{\detokenize{chapter_backend_and_runtime/memory_allocator:memory-allocate}}}为例，当BatchNorm算子计算结束后，output1不再被任何算子使用，则该Tensor的Device内存可以被回收，并且如果output1的内存尺寸大于等于output3的内存尺寸，则从output1回收的地址可以用于output3的内存分配，从而达到复用output1地址的目的。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{combine_memory_reuse_and_no_reuse}.png}
\caption{内存生命周期图}\label{\detokenize{chapter_backend_and_runtime/memory_allocator:id10}}\label{\detokenize{chapter_backend_and_runtime/memory_allocator:combine-memory-reuse-and-no-reuse}}\end{figure}

\sphinxAtStartPar
为了更好地描述内存复用问题，我们通过内存生命周期图来辅助理解。如
\hyperref[\detokenize{chapter_backend_and_runtime/memory_allocator:combine-memory-reuse-and-no-reuse}]{图\ref{\detokenize{chapter_backend_and_runtime/memory_allocator:combine-memory-reuse-and-no-reuse}}}所示，图中横坐标表示Tensor的生命周期，图中纵坐标表示内存大小。在生命周期内，某一个Tensor将一直占用某块Device内存，直至生命周期结束才会释放相应内存块。通过Tensor生命周期和内存大小可以构造出矩形块，而内存分配要求解的目标是在内存生命周期图中容纳更多的矩形块，问题的约束是矩形块之间无碰撞。
\hyperref[\detokenize{chapter_backend_and_runtime/memory_allocator:combine-memory-reuse-and-no-reuse}]{图\ref{\detokenize{chapter_backend_and_runtime/memory_allocator:combine-memory-reuse-and-no-reuse}}}左边是在未使用任何内存复用策略的情况下的内存生命周期图，此时内存同时只能容纳T0、T1、T2、T3四个Tensor。

\sphinxAtStartPar
内存复用策略的求解是一个NP完全的问题。许多深度学习框架通常采用贪心的策略去分配内存，例如采用BestFit算法，每次直接从内存池中选取可以满足条件的最小内存块，然而这种贪心的策略往往会陷入局部最优解，而无法求得全局最优解。为了更好地逼近内存分配策略全局最优解，MindSpore框架提出了一种新的内存分配算法
SOMAS（Safe Optimized Memory Allocation
Solver）。SOMAS将计算图并行流与数据依赖进行聚合分析，得到算子间祖先关系，构建张量全局生命周期互斥约束，使用多种启发式算法求解最优的内存静态规划，实现逼近理论极限的内存复用，从而提升支持的内存大小。

\sphinxAtStartPar
由
\hyperref[\detokenize{chapter_backend_and_runtime/memory_allocator:combine-memory-reuse-and-no-reuse}]{图\ref{\detokenize{chapter_backend_and_runtime/memory_allocator:combine-memory-reuse-and-no-reuse}}}右边所示，经过SOMAS求解之后，同样的内存大小，可支持的Tensor数量达到了7个。


\subsection{常见的内存分配优化手段}
\label{\detokenize{chapter_backend_and_runtime/memory_allocator:id5}}

\subsubsection{内存融合}
\label{\detokenize{chapter_backend_and_runtime/memory_allocator:id6}}
\sphinxAtStartPar
上述内存分配的方式，都是以单个Tensor的维度去分配的，每个Tensor分配到的Device地址往往是离散的。但是对于某些特殊的算子，如AllReduce通信算子，我们需要为它们分配连续的内存。通信算子的执行包含通信等待、数据搬移、计算等步骤，而在大规模分布式集群的场景下，通信的耗时往往是性能瓶颈。针对这种场景，如
\hyperref[\detokenize{chapter_backend_and_runtime/memory_allocator:memory-fusion}]{图\ref{\detokenize{chapter_backend_and_runtime/memory_allocator:memory-fusion}}}所示，我们可以将多个通信算子融合成一个，为通信算子的输入分配连续的内存，从而减少通信的次数。
又比如分布式训练中的神经网络权重初始化，通常将一个训练进程中的权重初始化，然后将该权重广播到其他进程中。当一个网络有较多权重的时候，需要多次进行广播。通常可以为所有权重分配连续的内存地址，然后广播一次，节省大量通信的耗时。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{memory_fusion}.png}
\caption{通信算子内存融合}\label{\detokenize{chapter_backend_and_runtime/memory_allocator:id11}}\label{\detokenize{chapter_backend_and_runtime/memory_allocator:memory-fusion}}\end{figure}


\subsubsection{In\sphinxhyphen{}Place算子}
\label{\detokenize{chapter_backend_and_runtime/memory_allocator:in-place}}
\sphinxAtStartPar
在前面的内存分配流程中，我们会为每个算子的输入和输出都分配不同的内存。然而对很多算子而言，为其分配不同的输入和输出地址，会浪费内存并且影响计算性能。例如优化器算子，其计算的目的就是更新神经网络的权重；例如Python语法中的’+=‘和’*=‘操作符，将计算结果更新到符号左边的变量中；例如’a{[}0{]}=b’语法，将’a{[}0{]}’的值更新为’b’。诸如此类计算有一个特点，都是为了更新输入的值。下面以Tensor的’a{[}0{]}=b’操作为例介绍In\sphinxhyphen{}Place的优点。
\hyperref[\detokenize{chapter_backend_and_runtime/memory_allocator:inplace-op}]{图\ref{\detokenize{chapter_backend_and_runtime/memory_allocator:inplace-op}}}左边是非In\sphinxhyphen{}Place操作的实现，step1将Tensor
a拷贝到Tensor a’，step2将Tensor b赋值给Tensor a’，step3将Tensor
a’拷贝到Tensor a。
\hyperref[\detokenize{chapter_backend_and_runtime/memory_allocator:inplace-op}]{图\ref{\detokenize{chapter_backend_and_runtime/memory_allocator:inplace-op}}}右边是算子In\sphinxhyphen{}Place操作的实现，仅用一个步骤将Tensor
b拷贝到Tensor
a对于的位置上。对比两种实现，可以发现In\sphinxhyphen{}Place操作节省了两次拷贝的耗时，并且省去了Tensor
a’内存的申请。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{inplace-op}.png}
\caption{In\sphinxhyphen{}Place算子内存分配}\label{\detokenize{chapter_backend_and_runtime/memory_allocator:id12}}\label{\detokenize{chapter_backend_and_runtime/memory_allocator:inplace-op}}\end{figure}

\sphinxAtStartPar
这节我们简单介绍了Device内存的概念，内存分配的流程，和一些优化内存分配的方法。内存分配是编译器后端的最重要部分之一，内存的合理分配，不仅关系到相同芯片上能否支持更大的网络模型，也关系到模型在硬件上的执行效率。


\section{计算调度与执行}
\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id1}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute::doc}}
\sphinxAtStartPar
经过算子选择与内存分配之后，计算任务可以通过运行时完成计算的调度与在硬件上的执行。根据是否将算子编译为计算图，计算的调度可以分为单算子调度与计算图调度两种方式，例如在MindSpore中分别提供了PyNative模式和Graph模式。而根据硬件提供的能力差异，计算图的执行方式又可以分为逐算子下发执行的交互式执行以及将整个计算图或者部分子图一次性下发到硬件的下沉式执行两种模式。


\subsection{单算子调度}
\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id2}}
\sphinxAtStartPar
单算子调度是相对于计算图而言，算法或者模型中包含的算子通过Python语言的运行时被逐个调度执行。例如PyTorch的默认执行方式，TensorFlow的eager模式，以及MindSpore的PyNative模式。以如下MindSpore示例代码所示：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{nn} \PYG{k}{as} \PYG{n+nn}{nn}
\PYG{k+kn}{from} \PYG{n+nn}{mindspore} \PYG{k+kn}{import} \PYG{n}{context}
\PYG{k+kn}{from} \PYG{n+nn}{mindspore} \PYG{k+kn}{import} \PYG{n}{ms\PYGZus{}function}

\PYG{c+c1}{\PYGZsh{} 以单算子方式执行后续计算中的算子。}
\PYG{n}{context}\PYG{o}{.}\PYG{n}{set\PYGZus{}context}\PYG{p}{(}\PYG{n}{mode}\PYG{o}{=}\PYG{n}{context}\PYG{o}{.}\PYG{n}{PYNATIVE\PYGZus{}MODE}\PYG{p}{)}

\PYG{k}{class} \PYG{n+nc}{Computation}\PYG{p}{(}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{Cell}\PYG{p}{)}\PYG{p}{:}
   \PYG{k}{def} \PYG{n+nf}{construct}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
     \PYG{n}{m} \PYG{o}{=} \PYG{n}{x} \PYG{o}{*} \PYG{n}{y}
     \PYG{n}{n} \PYG{o}{=} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{y}
     \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{m}\PYG{p}{)}
     \PYG{n}{z} \PYG{o}{=} \PYG{n}{m} \PYG{o}{+} \PYG{n}{n}
     \PYG{k}{return} \PYG{n}{z}

\PYG{n}{compute} \PYG{o}{=} \PYG{n}{Computation}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{c} \PYG{o}{=} \PYG{n}{compute}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{c}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
上述脚本将所有的计算逻辑定义在Computation类的construct方法中，由于在脚本开头的context中预先设置了单算子执行模式，construct中的计算将被Python的运行时逐行调用执行，同时可以在代码中的任意位置添加print命令以便打印中间的计算结果。

\sphinxAtStartPar
单算子执行的调用链路如
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:single-op-exec}]{图\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:single-op-exec}}}所示，算子在Python侧被触发执行后，会经过AI框架初始化，其中需要确定包括算子的精度，输入与输出的类型和大小以及对应的硬件设备等信息，接着框架会为该算子分配计算所需的内存，最后交给具体的硬件计算设备完成计算的执行。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{single_op_exec}.PNG}
\caption{单算子执行}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id6}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:single-op-exec}}\end{figure}

\sphinxAtStartPar
单算子调度方式的好处在于其灵活性，由于算子直接通过Python运行时调度，一方面可以表达任意复杂的计算逻辑，尤其是在需要复杂控制流以及需要Python原生数据结构支持来实现复杂算法的场景；另一方面单算子调度对于程序正确性的调试非常便利，开发人员可以在代码执行过程中打印任意需要调试的变量；最后一点是通过Python运行时驱动算子的方式，可以在计算中与Python庞大而丰富的生态库协同完成计算任务。


\subsection{计算图调度}
\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id3}}
\sphinxAtStartPar
虽然单算子调度具有如上所述的优点，其缺点也很明显。一方面是难于进行计算性能的优化，原因是由于缺乏计算图的全局信息，单算子执行时无法根据上下文完成算子融合，代数化简等优化；另一方面由于缺乏计算的拓扑关系，整个计算只能串行调度执行，即无法通过运行时完成并行计算。例如上述示例代码的计算逻辑可以表达为
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec}]{图\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec}}}所示。由该计算图可以看出，其中乘法和减法之间并没有依赖关系，因此这两个计算可以并行执行，而这样的并行执行信息只有将计算表达为计算图后才能完成分析，这也是计算图调度相对于单算子调度的优势之一。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{graph_exec}.png}
\caption{计算图}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id7}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec}}\end{figure}

\sphinxAtStartPar
下面我们开始介绍计算图的调度方式，在一个典型的异构计算环境中，主要存在CPU、GPU以及NPU等多种计算设备，因此一张计算图可以由运行在不同设备上的算子组成为异构计算图。
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:computation-graph}]{图\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:computation-graph}}}展示了一个典型的由异构硬件共同参与的计算图。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{computation_graph}.png}
\caption{异构硬件计算图}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id8}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:computation-graph}}\end{figure}

\sphinxAtStartPar
所述计算图由如下几类异构硬件对应的算子组成：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{CPU算子}：由C++语言编写实现并在主机上通过CPU执行的算子，CPU计算的性能取决于是否能够充分利用CPU多核心的计算能力。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{GPU算子}：以英伟达GPU芯片为例，通过在主机侧将GPU
Kernel逐个下发到GPU设备上，由GPU芯片执行算子的计算逻辑，由于芯片上具备大量的并行执行单元，可以为高度并行的算法提供强大的加速能力。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{NPU算子}：以华为Ascend芯片为例，
Ascend是一个高度集成的SoC芯片，NPU的优势是支持将部分或整个计算图下沉到芯片中完成计算，计算过程中不与Host发生交互，因此具备较高的计算性能。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Python算子}：在执行模式上与CPU算子类似，都是由主机上的CPU执行计算，区别在于计算逻辑是由Python语言的运行时通过Python解释器解释执行。

\end{itemize}

\sphinxAtStartPar
异构计算图能够被正确表达的首要条件是准确标识算子执行所在的设备，例如异构计算图
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:computation-graph}]{图\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:computation-graph}}}中所标识的CPU、GPU和Ascend
Kernel，以及被标记为被Python语言运行时执行的Python
Kernel。主流框架均提供了指定算子所在运行设备的能力，以MindSpore为例，一段简单的异构计算代码如下所示：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{mindspore} \PYG{k+kn}{import} \PYG{n}{Tensor}
\PYG{k+kn}{import} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{ops}\PYG{n+nn}{.}\PYG{n+nn}{operations} \PYG{k}{as} \PYG{n+nn}{ops}
\PYG{k+kn}{from} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{common}\PYG{n+nn}{.}\PYG{n+nn}{api} \PYG{k+kn}{import} \PYG{n}{ms\PYGZus{}function}

\PYG{c+c1}{\PYGZsh{} 创建算子并指定执行算子的硬件设备}
\PYG{n}{add} \PYG{o}{=} \PYG{n}{ops}\PYG{o}{.}\PYG{n}{Add}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{add\PYGZus{}prim\PYGZus{}attr}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{primitive\PYGZus{}target}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CPU}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{sub} \PYG{o}{=} \PYG{n}{ops}\PYG{o}{.}\PYG{n}{Sub}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{add\PYGZus{}prim\PYGZus{}attr}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{primitive\PYGZus{}target}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{GPU}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} 指定按照静态计算图模式执行函数}
\PYG{n+nd}{@ms\PYGZus{}function}
\PYG{k}{def} \PYG{n+nf}{compute}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{z}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{r} \PYG{o}{=} \PYG{n}{add}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{sub}\PYG{p}{(}\PYG{n}{r}\PYG{p}{,} \PYG{n}{z}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} 创建实参}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{Tensor}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{Tensor}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{Tensor}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} 执行计算}
\PYG{n}{output} \PYG{o}{=} \PYG{n}{compute}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{z}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
上述代码片段完成了x + y \sphinxhyphen{}
z的计算逻辑，其中Add算子被设置为在CPU上执行，Sub算子被设置为在GPU上执行，从而形成了CPU与GPU协同的异构计算，通过类似的标签机制，可以实现任意复杂的多硬件协同的异构计算表达。
另外一类较为特殊的异构是Python算子，Python语言的优势在于表达的灵活性和开发效率，以及丰富的周边生态，因此将Python算子引入到计算图中和其它异构硬件的算子协同计算，对计算的灵活性会产生非常大的帮助。与CPU、GPU分别执行在不同设备上的异构不同，Python算子和C++实现的CPU算子都是通过主机侧的CPU核执行，差异在于Python算子是通过统一的计算图进行描述，因此也需要在计算图的执行引擎中被触发执行。为了在计算图中能够表达Python算子，框架需要提供相应的支持。

\sphinxAtStartPar
完成计算图中算子对应设备的标记以后，计算图已经准备好被调度与执行，根据硬件能力的差异，可以将异构计算图的执行分为三种模式，分别是逐算子交互式执行，整图下沉执行与子图下沉执行。交互式执行主要针对CPU和GPU的场景，计算图中的算子按照输入和输出的依赖关系被逐个调度与执行；而整图下沉执行模式主要是针对NPU芯片而言，这类芯片主要的优势是能够将整个神经网络的计算图一次性下发到设备上，无需借助主机的CPU能力而独立完成计算图中所有算子的调度与执行，减少了主机和芯片的交互次数，借助NPU的Tensor加速能力，提高了计算效率和性能；子图下沉执行模式是前面两种执行模式的结合，由于计算图自身表达的灵活性，对于复杂场景的计算图在NPU芯片上进行整图下沉执行的效率不一定能达到最优，因此可以将对于NPU芯片执行效率低下的部分分离出来，交给CPU或者GPU等执行效率更高的设备处理，而将部分更适合NPU计算的子图下沉到NPU进行计算，这样可以兼顾性能和灵活性两方面。

\sphinxAtStartPar
上述异构计算图可以实现两个目的，一个是异构硬件加速，将特定的计算放置到合适的硬件上执行；第二个是实现算子间的并发执行，从计算图上可以看出，Kernel\_1和Kernel\_2之间没有依赖关系，Kernel\_3和Kernel\_4之间也没有依赖关系，因此这两组CPU和GPU算子在逻辑上可以被框架并发调用，而Kernel\_5依赖Kernel\_3和Kernel\_4的输出作为输入，因此Kernel\_5需要等待Kernel\_3和Kernel\_4执行完成后再被触发执行。

\sphinxAtStartPar
虽然在计算图上可以充分表达算子间的并发关系，在实际代码中会产生由于并发而引起的一些不预期的副作用场景，例如如下代码所示：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{mindspore} \PYG{k}{as} \PYG{n+nn}{ms}
\PYG{k+kn}{from} \PYG{n+nn}{mindspore} \PYG{k+kn}{import} \PYG{n}{Parameter}\PYG{p}{,} \PYG{n}{Tensor}
\PYG{k+kn}{import} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{ops}\PYG{n+nn}{.}\PYG{n+nn}{operations} \PYG{k}{as} \PYG{n+nn}{ops}
\PYG{k+kn}{from} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{common}\PYG{n+nn}{.}\PYG{n+nn}{api} \PYG{k+kn}{import} \PYG{n}{ms\PYGZus{}function}

\PYG{c+c1}{\PYGZsh{} 定义全局变量}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{Parameter}\PYG{p}{(}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ms}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{x}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ms}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ms}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} 指定按照静态计算图模式执行函数}
\PYG{n+nd}{@ms\PYGZus{}function}
\PYG{k}{def} \PYG{n+nf}{compute}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{z}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ops}\PYG{o}{.}\PYG{n}{Assign}\PYG{p}{(}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
    \PYG{n}{ops}\PYG{o}{.}\PYG{n}{Assign}\PYG{p}{(}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{z}\PYG{p}{)}
    \PYG{n}{r} \PYG{o}{=} \PYG{n}{ops}\PYG{o}{.}\PYG{n}{Sub}\PYG{p}{(}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{r}

\PYG{n}{compute}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{z}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
上述代码表达了如下计算逻辑：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
x = y
    x = z
    x = x \PYGZhy{} y
\end{sphinxVerbatim}

\sphinxAtStartPar
这段简单的计算逻辑翻译到计算图上可以表示为：

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{side_effect_1}.png}
\caption{并发算子执行}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id9}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:side-effect-1}}\end{figure}

\sphinxAtStartPar
代码中所示三行计算之间并没有依赖关系，因此这三个算子在计算图的逻辑上可以被并发执行，并发关系如
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:side-effect-1}]{图\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:side-effect-1}}}所示，然而根据代码的语义，显而易见是需要确保程序能够被顺序执行，这里引入的问题被称为副作用，副作用是指函数修改了在函数外部定义的状态变量的行为。由于副作用的引入而导致了错误并发关系的发生，一种解决方案是在计算图编译阶段通过添加算子间的依赖，将并发执行逻辑转换为顺序执行逻辑，转换后的计算图如
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:side-effect-2}]{图\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:side-effect-2}}}所示：

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{side_effect_2}.png}
\caption{消除副作用}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id10}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:side-effect-2}}\end{figure}

\sphinxAtStartPar
图中虚线箭头表达了算子之间的依赖关系，添加依赖关系后，算子会按照Assign\_1、Assign\_2、Sub\_1的顺序串行执行，与代码原本的语义保持一致。


\subsection{交互式执行}
\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id4}}
\sphinxAtStartPar
如上所述，交互式执行模式下，框架的运行时根据计算图中算子的依赖关系，按照某种执行序（例如广度优先序）逐个将算子下发到硬件上执行。为了助于理解和对比，先引入非异构计算图（计算图中的算子都是在同一类设备上）的执行方式，异构计算图的执行是基于非异构计算图基础之上的。

\sphinxAtStartPar
1、非异构计算图的执行方式

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{graph_exec_1}.png}
\caption{非异构计算图}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id11}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-1}}\end{figure}

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-1}]{图\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-1}}}是一张非异构计算图，计算图上全部Kernel均为GPU算子，执行方式一般分为串行执行和并行执行：

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{graph_exec_2}.png}
\caption{串行执行}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id12}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-2}}\end{figure}

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{graph_exec_3}.png}
\caption{并行执行}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id13}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-3}}\end{figure}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{串行执行}：将计算图展开为执行序列，按照执行序逐个串行执行，如
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-2}]{图\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-2}}}所示。其特点为执行顺序固定，单线程执行，对系统资源要求相对较低。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{并行执行}：将计算图按照算子之间的依赖关系展开，有依赖关系的算子通过输入依赖保证执行顺序，没有依赖关系的算子则可以并行执行，如
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-3}]{图\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-3}}}所示，Kernel\_1和Kernel\_2没有依赖可以并行执行，Kernel\_3和Kernel\_4没有依赖可以并行执行。其特点为执行顺序不固定，每轮执行的算子顺序大概率不一样，多线程执行，对系统资源要求相对较高。

\end{itemize}

\sphinxAtStartPar
串行执行和并行执行各有优点和缺点，总结对比见
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:serial-vs-parallel}]{表\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:serial-vs-parallel}}}。


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{串行执行和并行执行之对比}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id14}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:serial-vs-parallel}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
执行方式
&\sphinxstyletheadfamily 
\sphinxAtStartPar
串行执行
&\sphinxstyletheadfamily 
\sphinxAtStartPar
并行执行
\\
\hline
\sphinxAtStartPar
算子执行顺序
&
\sphinxAtStartPar
固定
&
\sphinxAtStartPar
不固定
\\
\hline
\sphinxAtStartPar
算子执行线程
&
\sphinxAtStartPar
单线程
&
\sphinxAtStartPar
多线程
\\
\hline
\sphinxAtStartPar
所需执行资源
&
\sphinxAtStartPar
较低
&
\sphinxAtStartPar
较高
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
2、异构计算图的执行方式

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{graph_exec_4}.png}
\caption{异构计算图}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id15}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-4}}\end{figure}

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-4}]{图\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-4}}}是一张异构计算图，其中Kernel\_1、Kernel\_2、Kernel\_5、Kernel\_9为CPU算子，Kernel\_6为python算子（执行也是在CPU上），Kernel\_3和Kernel\_4为GPU算子，Kernel\_7和Kernel\_8为GPU算子。
一般来说计算图的优化都是基于非异构计算图来实现的，要求计算图中的算子为同一设备上的，方便算子间的融合替换等优化操作，因此需要将一张异构计算图切分为多个非异构计算图，这里切分就比较灵活了，可以定义各种切分规则，一般按照产生尽量少的子图的切分规则来切分，尽量将多的同一设备上的算子放在一张子图中，如
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-5}]{图\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-5}}}所示，最后产生5张子图：Graph\_1\_CPU、Graph\_2\_GPU、Graph\_3\_CPU、Graph\_4\_Ascend、Graph\_5\_CPU。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{graph_exec_5}.png}
\caption{异构计算图切分}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id16}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-5}}\end{figure}

\sphinxAtStartPar
将一张异构计算图切分为多个子计算图后，执行方式一般分为子图拆分执行和子图合并执行：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{子图拆分执行}：将切分后的多个子图分开执行，即一个子图执行完再执行另一个子图，如
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-6}]{图\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-6}}}所示，上一个子图的输出数据会传输给下一个子图的输入数据，并且下一个子图需要将输入数据拷贝为本图的device数据，如Graph\_2\_GPU需要将Graph\_1\_CPU的输出数据从CPU拷贝到GPU，反过来Graph\_3\_CPU需要将Graph2GPU的输出数据从GPU拷贝到CPU，子图之间互相切换执行有一定的开销。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{子图合并执行}：将切分后的多个子图进行合并，合并为一个整体的DAG执行，如
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-7}]{图\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-7}}}所示，通过算子的设备属性来插入拷贝算子以实现不同设备上的算子数据传输，并且拷贝算子也是进入整图中的，从而形成一个大的整图执行，减少子图之间的切换执行开销。

\end{itemize}

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{graph_exec_6}.png}
\caption{子图拆分}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id17}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-6}}\end{figure}

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{graph_exec_7}.png}
\caption{子图合并}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id18}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-7}}\end{figure}

\sphinxAtStartPar
由于子图合并执行能够减少子图之间的切换执行开销，因此一般来说子图合并执行性能较高，总结对比见
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:partitioning-vs-merging}]{表\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:partitioning-vs-merging}}}。


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{子图拆分和子图合并之对比}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id19}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:partitioning-vs-merging}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
执行方式
&\sphinxstyletheadfamily 
\sphinxAtStartPar
子图拆分
&\sphinxstyletheadfamily 
\sphinxAtStartPar
子图合并
\\
\hline
\sphinxAtStartPar
异构数据传输
&
\sphinxAtStartPar
子图之间拷贝
&
\sphinxAtStartPar
算子之间拷贝
\\
\hline
\sphinxAtStartPar
执行额外开销
&
\sphinxAtStartPar
子图切换执行开销
&
\sphinxAtStartPar
无
\\
\hline
\sphinxAtStartPar
执行并发粒度
&
\sphinxAtStartPar
子图并发
&
\sphinxAtStartPar
算子原生并发
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
3、异构计算图的执行加速

\sphinxAtStartPar
前面讲述了非异构计算图的两种执行方式和异构计算图的两种执行方式，其中异构计算图又是在非异构计算图的基础之上，因此异构计算图按照两两组合共有四种执行方式，以MindSpore为例，采用的是子图合并并行执行，示例图如
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-5}]{图\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-5}}}所示，首先是作为一张整图来执行可以避免子图切换的执行开销，然后在整图内并行执行，可以最大粒度的发挥并发执行优势，达到最优的执行性能。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{graph_exec_8}.png}
\caption{异构硬件加速}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id20}}\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-8}}\end{figure}


\subsection{下沉式执行}
\label{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:id5}}
\sphinxAtStartPar
下沉式执行是通过专用芯片的SoC架构，将整个或部分计算图一次性调度到芯片上以完成全量数据的计算。例如对于Ascend芯片，多个Ascend算子组成的计算图可以在执行前被编译成为一个Task，通过Ascend驱动程序提供的接口，将包含多个算子的Task一次性下发到硬件上调度执行。因此上例中可以将Ascend的算子Kernel\_7和Kernel\_8优化为一个子图Graph\_4\_Ascend，再将该子图编译成为一个Task，并下沉到Ascend上执行，如
\hyperref[\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-8}]{图\ref{\detokenize{chapter_backend_and_runtime/compute_schedule_and_execute:graph-exec-8}}}所示。

\sphinxAtStartPar
下沉式执行由于避免了在计算过程中主机侧和设备侧的交互，因此可以获得更好的整体计算性能。然而下沉式执行也存在一些局限，例如在动态shape算子，复杂控制流等场景下会面临较大的技术挑战。


\section{总结}
\label{\detokenize{chapter_backend_and_runtime/summary:id1}}\label{\detokenize{chapter_backend_and_runtime/summary::doc}}\begin{itemize}
\item {} 
\sphinxAtStartPar
编译器后端主要负责计算图优化、算子选择、内存分配这三个任务。

\item {} 
\sphinxAtStartPar
计算图优化是在不影响模型的数值特性的基础上，通过图变换达到减少资源开销、适配硬件的执行能力、提升执行性能的目的。

\item {} 
\sphinxAtStartPar
计算图优化主要分为硬件通用优化和特定硬件优化，例如与硬件无关的算子内存IO优化和为了适配特定硬件指令限制而做的子图变换。

\item {} 
\sphinxAtStartPar
算子选择是为IR图中的每个计算节点选择一个最适合在设备上执行的算子。

\item {} 
\sphinxAtStartPar
数据存在多种存储格式和计算精度，不同的存储格式和计算精度在不同场景下对算子计算性能有较大的影响，所以算子选择需要综合考虑各方面影响选择最优的算子。

\item {} 
\sphinxAtStartPar
经过计算图优化和算子选择之后，得到了最终的IR。基于最终的IR，需要为算子的输入输出Tensor分配内存，然后加载算子到硬件上执行。

\item {} 
\sphinxAtStartPar
内存复用是一个重要的内存分配优化手段，可以让设备上容纳更大的网络模型。

\item {} 
\sphinxAtStartPar
将通信算子的内存进行融合，可以提高通信的效率；合理分配In\sphinxhyphen{}Place算子的内存，可以节省内存使用并且提高计算效率。

\item {} 
\sphinxAtStartPar
运行时对于算子的执行可以分为单算子调度和计算图调度两种模式，而在计算图调度模式中，根据具体硬件的能力又可以分为交互式执行和下沉式执行两种方式，交互式执行具备更多的灵活性，下沉执行可以获得更好的计算性能。

\end{itemize}


\section{扩展阅读}
\label{\detokenize{chapter_backend_and_runtime/summary:id2}}\begin{itemize}
\item {} 
\sphinxAtStartPar
内存分配作为机器学习后端的重要部分，建议阅读 \sphinxhref{https://arxiv.org/abs/1604.06174}{Sublinear Memory
Cost}%
\begin{footnote}[18]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/1604.06174}
%
\end{footnote}、 \sphinxhref{https://arxiv.org/abs/2006.09616}{Dynamic Tensor
Rematerialization}%
\begin{footnote}[19]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/2006.09616}
%
\end{footnote}。

\item {} 
\sphinxAtStartPar
对于运行时的调度以及执行，建议阅读 \sphinxhref{https://arxiv.org/abs/2004.10908}{A Lightweight Parallel and
Heterogeneous Task Graph Computing
System}%
\begin{footnote}[20]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/2004.10908}
%
\end{footnote}、 \sphinxhref{https://arxiv.org/abs/1805.01772}{Dynamic Control
Flow in Large\sphinxhyphen{}Scale Machine
Learning}%
\begin{footnote}[21]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/1805.01772}
%
\end{footnote}、\sphinxhref{https://arxiv.org/abs/1702.02181}{DEEP LEARNING
WITH DYNAMIC COMPUTATION
GRAPHS}%
\begin{footnote}[22]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/1702.02181}
%
\end{footnote}。

\end{itemize}


\chapter{硬件加速器}
\label{\detokenize{chapter_accelerator/index:id1}}\label{\detokenize{chapter_accelerator/index::doc}}
\sphinxAtStartPar
上一章节，我们详细讨论了计算图的基本组成，生成和执行等关键设计。当前主流深度学习模型大多基于神经网络实现，无论是训练还是推理，都会产生海量的计算任务，尤其是涉及矩阵乘法这种高计算任务的算子。另一方面，通用处理器芯片如CPU，在执行这类算子时通常耗时较大，难以满足训练/推理任务的需求。因此工业界和学术界都将目光投向特定领域的加速器芯片设计，希望以此来解决算力资源不足的问题。

\sphinxAtStartPar
本章将会着重介绍加速器的基本组成原理，并且以矩阵乘法为例，介绍在加速器上的编程方式及优化方法。最后，介绍由异构算子组成的异构计算图表达与执行方式。

\sphinxAtStartPar
本章的学习目标包括：
\begin{itemize}
\item {} 
\sphinxAtStartPar
掌握加速器的基本组成

\item {} 
\sphinxAtStartPar
掌握矩阵乘法的常见优化手段

\item {} 
\sphinxAtStartPar
理解编程API的设计理念

\item {} 
\sphinxAtStartPar
理解异构硬件加速的表达与执行

\end{itemize}


\section{概述}
\label{\detokenize{chapter_accelerator/accelerator_introduction:id1}}\label{\detokenize{chapter_accelerator/accelerator_introduction::doc}}

\subsection{硬件加速器设计的意义}
\label{\detokenize{chapter_accelerator/accelerator_introduction:id2}}
\sphinxAtStartPar
未来人工智能发展的三大核心要素是数据、算法和算力。目前，人工智能系统算力大都构建在CPU+GPU之上，主体多是GPU。随着神经网络层数的增多，模型体量的增大，算法复杂度的上升，CPU和GPU很难再满足新型网络对于算力的需求。例如，2015年谷歌的AlphaGo与\sphinxhref{https://baike.baidu.com/item/樊麾}{樊麾}%
\begin{footnote}[23]\sphinxAtStartFootnote
\sphinxnolinkurl{https://baike.baidu.com/item/樊麾}
%
\end{footnote}对弈时，用了1202个CPU和176个GPU，每盘棋需要消耗上千美元的电费，而与之对应的是樊麾的功耗仅为20瓦。

\sphinxAtStartPar
虽然GPU在面向向量、矩阵以及张量的计算上，引入许多新颖的优化设计，但由于GPU需要支持的计算类型复杂，芯片规模大、能耗高，人们开始将更多的精力转移到深度学习硬件加速器的设计上来。和传统CPU和GPU芯片相比，新型深度学习加速器会有更高的性能，以及更低的能耗。未来随着人们真正进入智能时代，智能应用的普及会越来越广泛，到那时每台服务器、每台智能手机、每个智能摄像头，都需要使用加速器。


\subsection{硬件加速器设计的思路}
\label{\detokenize{chapter_accelerator/accelerator_introduction:accelerator-design-title}}\label{\detokenize{chapter_accelerator/accelerator_introduction:id3}}
\sphinxAtStartPar
近些年来，计算机体系结构的研究热点之一就是深度学习硬件加速器的设计。在体系结构的研究中，能效和通用性是两个重要的衡量指标。能效关注单位能耗下基本计算的次数，通用性主要指芯片能够覆盖的任务种类。

\sphinxAtStartPar
以两类特殊的芯片为例:一种是我们较为熟悉的通用处理器(如CPU)，该类芯片理论上可以完成各种计算任务，但是其能效较低大约只有0.1TOPS/W；另一种是专用集成电路(Application
Specific Integrated Circuit,
ASIC)，其能效更高，但是支持的任务相对而言就比较单一。对于通用的处理器而言，为了提升能效，在芯片设计上有许多加速技术的引入，例如：超标量技术、单指令多数据（Single
Instruction Multiple Data，SIMD）技术以及单指令多线程（Single
Instruction Multiple Threads，SIMT）技术等。

\sphinxAtStartPar
对于不同的加速器设计方向，业界也有不同的硬件实现。针对架构的通用性，NVIDIA持续在其GPU芯片上发力，先后推出了Volta,
Turing, Ampere架构，并推出用于加速矩阵计算的张量核（Tensor
Core），以满足深度学习海量算力的需求。

\sphinxAtStartPar
对于偏定制化的硬件架构，面向深度学习计算任务，业界提出了特定领域架构(Domain
Specific Architecture)。
Google公司推出了TPU芯片，专门用于加速深度学习计算任务，其使用脉动阵列(Systolic
Array)来优化矩阵乘法和卷积运算，可以充分地利用数据局部性，降低对内存的访问次数。华为也推出了自研的昇腾AI处理器，旨在为用户提供更高能效的算力和易用的开发、部署体验，其中的CUBE运算单元，就用于加速矩阵乘法的计算。


\section{加速器基本组成原理}
\label{\detokenize{chapter_accelerator/accelerator_architecture:id1}}\label{\detokenize{chapter_accelerator/accelerator_architecture::doc}}
\sphinxAtStartPar
上节主要介绍了加速器的意义以及设计思路，了解到加速器与通用处理器在设计上的区别，因此加速器的硬件结构与CPU的硬件结构有着根本的不同，通常都是由多种片上缓存以及多种运算单元组成。本章节主要通过GPU的Volta架构作为样例进行介绍。


\subsection{硬件加速器的架构}
\label{\detokenize{chapter_accelerator/accelerator_architecture:id2}}
\sphinxAtStartPar
现代GPU在十分有限的面积上实现了极强的计算能力和极高的储存器以及IO带宽。一块高端的GPU中，晶体管数量已经达到主流CPU的两倍，而且显存已经达到了16GB以上,工作频率也达到了1GHz。GPU的体系架构由两部分组成，分别是流处理阵列和存储器系统，两部分通过一个片上互联网络连接。流处理器阵列和存储器系统都可以单独扩展，规格可以根据产品的市场定位单独裁剪。如GV100的组成
\sphinxcite{chapter_reinforcement_learning/summary:nvidia}如 \hyperref[\detokenize{chapter_accelerator/accelerator_architecture:gv100}]{图\ref{\detokenize{chapter_accelerator/accelerator_architecture:gv100}}}所示：

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{V100}.svg}
\caption{Volta GV100}\label{\detokenize{chapter_accelerator/accelerator_architecture:id7}}\label{\detokenize{chapter_accelerator/accelerator_architecture:gv100}}\end{figure}
\begin{itemize}
\item {} 
\sphinxAtStartPar
6个GPU处理集群（GPU Processing Cluster，GPC）, 每个GPC含有：
\begin{itemize}
\item {} 
\sphinxAtStartPar
7个纹理处理集群（Texture Processing Cluster, TPC）
(每个TPC含有两个流多处理器（Streaming Multiprocessor, SM）)

\item {} 
\sphinxAtStartPar
14个SM

\end{itemize}

\item {} 
\sphinxAtStartPar
84个SM, 每个流多处理器含有：
\begin{itemize}
\item {} 
\sphinxAtStartPar
64个32位浮点运算单元

\item {} 
\sphinxAtStartPar
64个32位整数运算单元

\item {} 
\sphinxAtStartPar
32个64位浮点运算单元

\item {} 
\sphinxAtStartPar
8个张量核

\item {} 
\sphinxAtStartPar
4个纹理单元

\end{itemize}

\item {} 
\sphinxAtStartPar
8个512\sphinxhyphen{}bit内存控制器

\end{itemize}

\sphinxAtStartPar
一个完整的GV100
GPU含有84个SM，5376个32位浮点运算单元，5376个32位整型运算单元，2688个64位浮点运算单元，672个张量运算单元和336个纹理单元。一对内存控制器控制一个HBM2
DRAM堆栈。 \hyperref[\detokenize{chapter_accelerator/accelerator_architecture:gv100}]{图\ref{\detokenize{chapter_accelerator/accelerator_architecture:gv100}}}中展示的为带有84个SM的GV100
GPU(不同的厂商可以使用不同的配置)，Tesla V100则含有80个SM。


\subsection{硬件加速器的存储单元}
\label{\detokenize{chapter_accelerator/accelerator_architecture:id4}}
\sphinxAtStartPar
与传统的CPU模型相似，从一个计算机系统主内存DRAM中获取数据的速度相对于处理器的运算速度较慢。对于加速器而言，如果没有缓存进行快速存取，DRAM的带宽非常不足。如果无法快速的在DRAM上获取程序和数据，加速器将因空置而降低利用率。为了缓解DRAM的带宽问题，GPU提供了不同层次的若干区域供程序员存放数据，每块区域的内存都有自己的最大带宽以及延迟。
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{寄存器文件（Register
File）}：片上最快的存储器，但与CPU不同，GPU的每个SM（流多处理器）有上万个寄存器。但当每个线程使用过多的寄存器时，SM中能够调度的线程块数量就会受到限制，可执行的线程总数量会因此受到限制，可执行的线程数量过少会造成硬件无法充分的利用，性能急剧下降。所以要根据算法的需求合理使用寄存器。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{共享内存（Shared
Memory）}：共享内存实际上是用户可控的一级缓存，每个SM（流多处理器）中有128KB的一级缓存,
开发者可根据应用程序需要配置最大96KB的一级缓存作为共享内存。共享内存的访存延迟极低，只有几十个时钟周期。共享内存具有高达1.5TB/s的带宽，远远高于全局内存的峰值带宽900GB/s。所以说，共享内存的使用对于一个高性能计算工程师来说是一个必须要掌握的一个概念。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{全局内存（Global
Memory）}：全局内存之所以称为全局，是因为GPU与CPU都可以对它进行读写操作。全局内存对于GPU中的每个线程都是可见的，都可以直接对全局内存进行读写操作。CPU等其他设备可以通过PCI\sphinxhyphen{}E总线对其进行读写操作。全局内存也是GPU中容量最大的一块内存，可达16GB之多。同时也是延迟最大的内存，通常有高达上百个时钟周期的访存延迟。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{常量内存（Constant
Memory）}：常量内存其实只是全局内存的一种虚拟地址形式，并没有真正的物理硬件内存块。常量内存有两个特性，一个高速缓存，另一个更重要的特性是它支持将某个单个值广播到线程束中的每个线程中。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{纹理内存（Texture
Memory）}：纹理内存是全局内存的一个特殊形态。当全局内存被绑定为纹理内存时，执行读写操作将通过专用的纹理缓存来加速。在早期的GPU上没有缓存，因此每个SM（流多处理器）上的纹理内存为设备提供了唯一真正缓存数据的方法。纹理内存的另外一个特性，也是最有用的特性就是当访问存储单元时，允许GPU实现硬件相关的操作。比如说使用纹理内存，可以通过归一化的地址对数组进行访问，获取的数据可以通过硬件进行自动插值，从而达到快速处理数据的目的。此外对于二维数组和三维数组，支持硬件级的双线性插值与三线性插值。纹理内存另一个实用的特性是可以根据数组的索引自动处理边界条件，不需要对特殊边缘进行处理即可完成数组内元素操作，从而防止线程中分支的产生。

\end{itemize}

\sphinxAtStartPar
由于寄存器的高速读取特性，因此每次计算都离不开寄存器的参与。接着是一级缓存和共享内存，然后是常量内存、纹理内存、全局内存，最后则是主机端内存。根据不同存储器之间的存储速度的数量级的变化规律，选用适当类型的内存以及最大化地利用它们，从而发挥硬件的最大算力，减少计算时间。


\subsection{硬件加速器的计算单元}
\label{\detokenize{chapter_accelerator/accelerator_architecture:compute-unit-title}}\label{\detokenize{chapter_accelerator/accelerator_architecture:id5}}
\sphinxAtStartPar
为了支持不同的神经网络模型，加速器会提供以下几种计算单元，不同的网络层可以根据需要选择使用对应的计算单元。如
\hyperref[\detokenize{chapter_accelerator/accelerator_architecture:compute-unit}]{图\ref{\detokenize{chapter_accelerator/accelerator_architecture:compute-unit}}}所示
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{标量计算单元}：与标准的精简指令运算集（Reduced Instruction Set
Computer，RISC）相似，一次计算一个标量元素。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{一维向量计算单元}：一次可以完成多个元素的计算，与传统的CPU和GPU架构中单指令多数据（SIMD）相似，已广泛应用于高性能计算（High
Performance Computing，HPC）和信号处理中。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{二维向量计算单元}：一次运算可以完成一个矩阵与向量的内积，或向量的外积。利用数据重复使用这一特性，降低数据通信成本与存储空间，更高效的提高矩阵乘法性能。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{三维向量计算单元}：一次完成一个矩阵的乘法，专为神经网络应用设计的计算单元，更充分利用数据重复特性，隐藏数据通信带宽与数据计算的差距。

\end{itemize}

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{compute_unit}.svg}
\caption{多种计算单元}\label{\detokenize{chapter_accelerator/accelerator_architecture:id8}}\label{\detokenize{chapter_accelerator/accelerator_architecture:compute-unit}}\end{figure}

\sphinxAtStartPar
GPU计算单元主要由标量计算单元组成，而在Volta及以后的架构中还加入了三维向量计算单元。如
\hyperref[\detokenize{chapter_accelerator/accelerator_architecture:sm}]{图\ref{\detokenize{chapter_accelerator/accelerator_architecture:sm}}}所示,对于每个SM，其中64个32位浮点运算单元、64个32位整数运算单元、32个64位浮点运算单元均为标量计算单元。而8个张量核则是专为神经网络应用设计的三维向量计算单元。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{SM}.svg}
\caption{Volta GV100 流多处理器（SM）}\label{\detokenize{chapter_accelerator/accelerator_architecture:id9}}\label{\detokenize{chapter_accelerator/accelerator_architecture:sm}}\end{figure}

\sphinxAtStartPar
张量核（Tensor
Core）每个时钟周期完成一次\(4\times4\)的矩阵乘累加计算,如
\hyperref[\detokenize{chapter_accelerator/accelerator_architecture:tensorcore}]{图\ref{\detokenize{chapter_accelerator/accelerator_architecture:tensorcore}}}：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{D}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{A}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{B}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{C}
\end{sphinxVerbatim}

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{tensor_core}.svg}
\caption{Tensor Core \(4\times4\)矩阵乘累加计算}\label{\detokenize{chapter_accelerator/accelerator_architecture:id10}}\label{\detokenize{chapter_accelerator/accelerator_architecture:tensorcore}}\end{figure}

\sphinxAtStartPar
其中A,B,C和D都是\(4\times4\)的矩阵，矩阵乘累加的输入矩阵A和B是FP16的矩阵，累加矩阵C和D可以是FP16也可以是FP32。
V100的张量核是可编程的矩阵乘法和累加计算单元，可以提供多达125 Tensor
TFLOPS(Tera Floating\sphinxhyphen{}point Operations Per
Second)的训练和推理应用。相比于普通的FP32计算单元可以提速10倍以上。


\subsection{DSA芯片架构}
\label{\detokenize{chapter_accelerator/accelerator_architecture:dsa}}
\sphinxAtStartPar
为了满足飞速发展的深度神经网络对芯片算力的需求，业界也纷纷推出了特定领域架构DSA芯片设计。以华为公司昇腾系列AI处理器为例，本质上是一个片上系统（System
on
Chip，SoC），主要应用在图像、视频、语音、文字处理相关的场景。主要的架构组成部件包括特制的计算单元、大容量的存储单元和相应的控制单元。该芯片由以下几个部分构成：芯片系统控制CPU（Control
CPU），AI计算引擎（包括AI Core和AI
CPU），多层级的片上系统缓存（Cache）或缓冲区（Buffer），数字视觉预处理模块（Digital
Vision Pre\sphinxhyphen{}Processing，DVPP）等。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{davinci_architecture}.svg}
\caption{达芬奇架构设计}\label{\detokenize{chapter_accelerator/accelerator_architecture:id11}}\label{\detokenize{chapter_accelerator/accelerator_architecture:davinci-architecture}}\end{figure}

\sphinxAtStartPar
昇腾AI芯片的计算核心主要由AI
Core构成，负责执行标量、向量和张量相关的计算密集型算子。AI
Core采用了达芬奇架构 \sphinxcite{chapter_accelerator/summary:ascend}，基本结构如
\hyperref[\detokenize{chapter_accelerator/accelerator_architecture:davinci-architecture}]{图\ref{\detokenize{chapter_accelerator/accelerator_architecture:davinci-architecture}}}所示，从控制上可以看成是一个相对简化的现代微处理器基本架构。它包括了三种基础计算单元：矩阵计算单元（Cube
Unit）、向量计算单元（Vector Unit）和标量计算单元（Scalar
Unit）。这三种计算单元分别对应了张量、向量和标量三种常见的计算模式，在实际的计算过程中各司其职，形成了三条独立的执行流水线，在系统软件的统一调度下互相配合达到优化计算效率的目的。
同GPU类似，在矩阵乘加速设计上，在AICore中也提供了矩阵计算单元作为昇腾AI芯片的核心计算模块，意图高效解决矩阵计算的瓶颈问题。矩阵计算单元提供强大的并行乘加计算能力，可以用一条指令完成两个\(16\times16\)矩阵的相乘运算，等同于在极短时间内进行了\(16\times16\times16=4096\)个乘加运算，并且可以实现FP16的运算精度。


\section{加速器基本编程原理}
\label{\detokenize{chapter_accelerator/accelerator_programming:accelerator-program-title}}\label{\detokenize{chapter_accelerator/accelerator_programming:id1}}\label{\detokenize{chapter_accelerator/accelerator_programming::doc}}
\sphinxAtStartPar
本章前两节主要介绍了硬件加速器设计的意义、思路以及基本组成原理。软硬件协同优化作为构建高效AI系统的一个重要指导思想，需要软件算法/软件栈和硬件架构在神经网络应用中互相影响、紧密耦合。为了最大限度地发挥加速器的优势，要求能够基于硬件系统架构提供易用、高效的编程方法。因此，在本节中将着重介绍加速器的可编程性，包括编程接口直接调用方式及算子编译器优化方式。


\subsection{硬件加速器的可编程性}
\label{\detokenize{chapter_accelerator/accelerator_programming:accelerator-programable-title}}\label{\detokenize{chapter_accelerator/accelerator_programming:id2}}
\sphinxAtStartPar
\hyperref[\detokenize{chapter_accelerator/accelerator_introduction:accelerator-design-title}]{\ref{\detokenize{chapter_accelerator/accelerator_introduction:accelerator-design-title}}节}节中列出的硬件加速器均具有一定的可编程性，程序员可以通过软件编程，有效的使能上述加速器进行计算加速。现有硬件加速器常见的两类编程方式主要有编程接口调用以及算子编译器优化。


\subsubsection{编程接口使能加速器}
\label{\detokenize{chapter_accelerator/accelerator_programming:id3}}
\sphinxAtStartPar
硬件加速器出于计算效率和易用性等方面考虑，将编程接口使能方式分为不同等级，一般包括：算子库层级，编程原语层级，以及指令层级。为了更具象的解释上述层级的区别，我们以Volta架构的Tensor
Core加速器为例，由高层至底层对比介绍这三种不同编程方式：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{算子库层级}：如cuBLAS基本矩阵与向量运算库，cuDNN深度学习加速库，均通过Host端调用算子库提供的核函数使能TensorCore；

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{编程原语层级}：如基于CUDA的WMMA
API编程接口。同算子库相比，需要用户显式调用计算各流程，如矩阵存取至TensorCore、TensorCore执行矩阵乘累加运算、TensorCore累加矩阵数据初始化操作等；

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{指令层级}：如PTX ISA
MMA指令集，提供更细粒度的mma指令，便于用户组成更多种形状的接口，通过CUDA
Device端内联编程使能TensorCore。

\end{itemize}


\subsubsection{算子编译器使能加速器}
\label{\detokenize{chapter_accelerator/accelerator_programming:id4}}
\sphinxAtStartPar
DSA架构的多维度AI加速器通常提供了更多的指令选择（3D\sphinxhyphen{}Matrix/2D\sphinxhyphen{}Vector/1D\sphinxhyphen{}Scalar），以及更加复杂的数据流处理，通过提供接口调用的方式对程序开发人员带来较大的挑战。此外，由于调度、切分的复杂度增加，直接提供算子库的方式由于缺少根据目标shape调优的能力，往往无法在所有shape下均得到最优的性能。因此，对于DSA加速器，业界通常采用算子编译器的解决方案。

\sphinxAtStartPar
随着深度学习模型的迭代更新及各类AI芯片的层出不穷，基于人工优化算子的方式给算子开发团队带来沉重的负担。因此，开发一种能够将High\sphinxhyphen{}level的算子表示编译成目标硬件可执行代码的算子编译器，逐渐成为学术界及工业界的共识。算子编译器前端通常提供了特定领域描述语言（DSL），用于定义算子的计算范式；类似于传统编译器，算子编译器也会将算子计算表示转换为中间表示，如HalideIR
\sphinxcite{chapter_accelerator/summary:ragan2013halide}、TVM \sphinxcite{chapter_accelerator/summary:chen2018tvm}的TIR、Schedule
Tree
\sphinxcite{chapter_accelerator/summary:verdoolaege2010isl}等，基于模板（手动）、搜索算法或优化求解算法（自动）等方式完成循环变换、循环切分等调度相关优化，以及硬件指令映射、内存分配、指令流水等后端pass优化，最后通过codegen模块将IR转换为DSA加速器可执行的kernel。

\sphinxAtStartPar
当前业界的算子编译器/编译框架主要有TVM/Ansor
\sphinxcite{chapter_accelerator/summary:zheng2020ansor}、MLIR
\sphinxcite{chapter_accelerator/summary:lattner2020mlir}、以及华为Ascend芯片上的TBE/AKG
\sphinxcite{chapter_accelerator/summary:zhao2021akg}等。
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{TVM/Ansor}

\end{itemize}

\sphinxAtStartPar
TVM是陈天奇博士等人开发的开源深度学习编译框架，提供了端到端的编译优化（图优化/算子优化）能力，在工业界应用较广。在架构上，主要包括relay和tir两层。通过relay导入推理模型，进行算子融合等图层优化，通过tir生成融合算子。在算子编译方面，TVM采用了计算和调度分离的技术，为不同的算子提供了不同的模板，同时支持自定义模板，优化特定算子类型调度。为了更进一步优化算子性能，TVM支持对算子进行自动tuning，来生成较优的切分参数。此外，为了简化用户开发模板的工作，TVM在0.8版本后提供了自动调度能力Ansor，通过搜索的方式，为目标算子生成调度及切分参数。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{TVM}.svg}
\caption{TVM}\label{\detokenize{chapter_accelerator/accelerator_programming:id18}}\label{\detokenize{chapter_accelerator/accelerator_programming:tvm}}\end{figure}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{MLIR}

\end{itemize}

\sphinxAtStartPar
前面的章节介绍过，Google开发的MLIR并不是一个单一的算子编译器，而是一套编译器基础设施，提供了工具链的组合与复用能力。基于MLIR，DSA加速器厂商可以快速的搭建其定制化算子编译器。如Google论文
\sphinxcite{chapter_accelerator/summary:vasilache2022composable}中所述，当前的算子编译器大多提供了一整套自顶向下的编译优化pass，包括调度优化、切分优化、窥孔优化、后端优化、指令生成等，彼此之间大多无法复用，导致新的场景中通常又得从头开发。而在MLIR中，将功能相近的IR优化pass封装为方言（Dialect），并且提供了多个代码生成相关的基础方言，如vector、memref、tensor、scf、affine、linalg等。硬件厂商可以基于这些Dialect，快速构建一整套lower优化及codegen流程。如
\hyperref[\detokenize{chapter_accelerator/accelerator_programming:mlir-lowing}]{图\ref{\detokenize{chapter_accelerator/accelerator_programming:mlir-lowing}}}所示，利用scf、affine、linalg等方言，对结构化的计算IR完成循环并行优化、切分、向量化等，最后基于LLVM完成指令映射。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{MLIR-Lowing}.svg}
\caption{MLIR\_Lowing}\label{\detokenize{chapter_accelerator/accelerator_programming:id19}}\label{\detokenize{chapter_accelerator/accelerator_programming:mlir-lowing}}\end{figure}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{华为TBE/AKG}

\end{itemize}

\sphinxAtStartPar
TBE（Tensor Boost Engine）是华为的Ascend芯片及其CANN软件栈基于TVM
开发的一套算子编译优化工具，用于对Ascend芯片进行调度优化、指令映射、及后端pass优化等。不仅提供了一个优化过的神经网络标准算子库，同时还提供了算子开发能力及融合能力。通过TBE提供的API和自定义算子编程开发界面可以完成相应神经网络算子的开发，帮助用户较容易的去使能硬件加速器上的AI\_CORE
相关指令，以实现高性能的神经网络计算。为了简化算子开发流程，TBE还实现了一个Auto
Schedule工具，开放了自定义算子编程DSL，用于自动完成复杂算子的调度生成。此外，TBE还实现了端到端的动态shape算子编译能力。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{TBE}.svg}
\caption{TBE}\label{\detokenize{chapter_accelerator/accelerator_programming:id20}}\label{\detokenize{chapter_accelerator/accelerator_programming:tbe}}\end{figure}

\sphinxAtStartPar
AKG则是MindSpore社区的开源算子编译工具。与上述介绍的算子编译器不同，AKG基于Polyhedral多面体编译技术
\sphinxcite{chapter_accelerator/summary:bastoul2004code}，支持在CPU/GPU/Ascend多硬件上自动生成满足并行性与数据局部性的调度。Polyhedral编译技术的核心思想是将程序中循环的迭代空间映射为高维空间多面体，通过分析语句读写依赖关系，将循环调度优化问题转换为整数规划求解问题。
AKG的编译流程如
\hyperref[\detokenize{chapter_accelerator/accelerator_programming:akg}]{图\ref{\detokenize{chapter_accelerator/accelerator_programming:akg}}}所示，主要包含程序规范化、自动调度优化、指令生成、后端优化几个模块。AKG同样基于TVM实现，支持TVM
compute/Hybrid
DSL编写的算子表示，以及MindSpore图算融合模块优化后的融合子图。通过IR规范化，将DSL/子图IR转换为polyhedral编译的调度树。在polyhedral模块中，利用其提供的调度算法，实现循环的自动融合、自动重排等变换，为融合算子自动生成满足并行性、数据局部性的初始调度。为了能够快速适配不同的硬件后端，我们在poly模块内将优化pass识别为硬件无关的通用优化与硬件相关的特定优化，编译时按照硬件特征拼接组合，实现异构硬件后端的快速适配。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{akg}.png}
\caption{AKG}\label{\detokenize{chapter_accelerator/accelerator_programming:id21}}\label{\detokenize{chapter_accelerator/accelerator_programming:akg}}\end{figure}

\sphinxAtStartPar
在polyhedral模块中，实现了算子的自动调度生成、自动切分以及自动数据搬移。为了进一步提升算子的性能，我们针对不同硬件后端开发了相应的优化pass，如Ascend后端中实现数据对齐、指令映射，GPU后端中实现向量化存取，插入同步指令等，最终生成相应平台代码。


\subsection{硬件加速器的多样化编程方法}
\label{\detokenize{chapter_accelerator/accelerator_programming:diversified-programming-title}}\label{\detokenize{chapter_accelerator/accelerator_programming:id13}}
\sphinxAtStartPar
矩阵乘法运算作为深度学习网络中占比最大的计算，对其进行优化是十分必要的。因此本节将统一以矩阵乘法\(D[M, N] = C[M, N] + A[M, K] \times B[K, N]\)为实例，对比介绍如何通过不同编程方式使能加速器。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{gemm}.svg}
\caption{矩阵乘法GEMM运算}\label{\detokenize{chapter_accelerator/accelerator_programming:id22}}\label{\detokenize{chapter_accelerator/accelerator_programming:id14}}\label{\detokenize{chapter_accelerator/accelerator_programming:gemm-algorith}}\end{figure}


\subsubsection{编程接口使能加速器}
\label{\detokenize{chapter_accelerator/accelerator_programming:id15}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{算子库层级}

\end{itemize}

\sphinxAtStartPar
在上述不同层级的编程方式中，直接调用算子加速库使能加速器无疑是最快捷高效的方式。NVIDIA提供了cuBLAS/cuDNN两类算子计算库，cuBLAS提供了使能Tensor
Core单元的接口，用以加速矩阵乘法(GEMM)运算，cuDNN提供了对应接口加速卷积(CONV)运算等。
以
\hyperref[\detokenize{chapter_accelerator/accelerator_programming:accelerator-programable-title}]{\ref{\detokenize{chapter_accelerator/accelerator_programming:accelerator-programable-title}}节}小节的GEMM运算为例，与常规CUDA调用cuBLAS算子库相似，通过cuBLAS加速库使能Tensor
Core步骤包括： 1. 创建cuBLAS对象句柄且设置对应数学计算模式

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cublasHandle\PYGZus{}t}\PYG{+w}{ }\PYG{n}{handle}\PYG{p}{;}
\PYG{n}{cublasStatus\PYGZus{}t}\PYG{+w}{ }\PYG{n}{cublasStat}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{cublasCreate}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{handle}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cublasStat}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{cublasSetMathMode}\PYG{p}{(}\PYG{n}{handle}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{CUBLAS\PYGZus{}TENSOR\PYGZus{}OP\PYGZus{}MATH}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
分配和初始化矩阵内存空间及内容元素

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kt}{size\PYGZus{}t}\PYG{+w}{ }\PYG{n}{matrixSizeA}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{size\PYGZus{}t}\PYG{p}{)}\PYG{n}{M}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{;}
\PYG{n}{cublasStat}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{cudaMalloc}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{devPtrA}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{matrixSizeA}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{n}{devPtrA}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cublasStat}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{cublasSetMatrix}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{,}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{n}{A}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{A}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{devPtrA}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{2}
\item {} 
\sphinxAtStartPar
调用对应计算函数接口

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cublasStat}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{cublasGemmEx}\PYG{p}{(}\PYG{n}{handle}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{transa}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{transb}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{m}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{n}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{k}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{,}
\PYG{+w}{                          }\PYG{n}{A}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{CUDA\PYGZus{}R\PYGZus{}16F}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{lda}\PYG{p}{,}
\PYG{+w}{                          }\PYG{n}{B}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{CUDA\PYGZus{}R\PYGZus{}16F}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{ldb}\PYG{p}{,}
\PYG{+w}{                          }\PYG{n}{beta}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{C}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{CUDA\PYGZus{}R\PYGZus{}16F}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{ldc}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{CUDA\PYGZus{}R\PYGZus{}32F}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{algo}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{3}
\item {} 
\sphinxAtStartPar
传回结果数据

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cublasStat}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{cublasGetMatrix}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{,}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{n}{D}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{devPtrD}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{D}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{4}
\item {} 
\sphinxAtStartPar
释放内存和对象句柄

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cudaFree}\PYG{p}{(}\PYG{n}{devPtrA}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cudaDestroy}\PYG{p}{(}\PYG{n}{handle}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
当然，由于加速器一般会有矩阵形状、数据类型、排布方式等限制，因此在调用句柄和函数接口时要多加注意。如本例中，cuBLAS计算模式必须设置为\(CUBLAS\_TENSOR\_OP\_MATH\)，步长必须设置为8的倍数，输入数据类型必须为\(CUDA\_R\_16F\)等。按照如上方式即可通过cuBLAS算子库对
\hyperref[\detokenize{chapter_accelerator/accelerator_programming:accelerator-programable-title}]{\ref{\detokenize{chapter_accelerator/accelerator_programming:accelerator-programable-title}}节}实例使能Tensor
Core加速器，通过NVIDIA官方数据可知，该方式对于不同矩阵乘法计算规模，平均有4～10倍的提升，且矩阵规模越大，加速器提升效果越明显。

\sphinxAtStartPar
该方式由于能够隐藏体系结构细节，易用性较好，且一般官方提供的算子库吞吐量较高。但与此同时，这种算子颗粒度的库也存在一些问题，如不足以应对复杂多变的网络模型导致的算子长尾问题（虽然常规形式算子占据绝大多数样本，但仍有源源不断的新增算子，因其出现机会较少，算子库未对其进行有效优化。），以及错失了较多神经网络框架优化（如算子融合）的机会。
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{编程原语层级}

\end{itemize}

\sphinxAtStartPar
第二种加速器编程方式为编程原语使能加速器，如通过在Device端调用CUDA WMMA
(Warp Matrix Multiply Accumulate)
API接口。以线程束（即Warp，是调度的基本单位）为操纵对象，使能多个Tensor
Core单元。该方式在CUDA
9.0中被公开，程序员可通过添加API头文件的引用和命名空间定义来使用上述API接口。基于软硬件协同设计的基本思想，该层级编程API的设计多与架构绑定，如WMMA操纵的总是\(16\times16\)大小的矩阵块，并且操作一次跨两个TensorCore进行处理，本质是与TensorCore如何集成进SM中强相关的。针对Float16输入数据类型，NVIDIA官方提供了三种不同矩阵规模的WMMA乘累加计算接口，分别为\(16\times16\times16\)，\(32\times8\times16\)，\(8\times32\times16\)。
该API接口操纵的基本单位为Fragment，是一种指明了矩阵含义（乘法器/累加器）、矩阵形状（\(WMMA\_M, WMMA\_N, WMMA\_K\)）、数据类型（Half/
Float）、排布方式（\(row\_major/ col\_major\)）等信息的模板类型，包括如下：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{wmma}\PYG{o}{:}\PYG{o}{:}\PYG{n}{fragment}\PYG{o}{\PYGZlt{}}\PYG{n}{wmma}\PYG{o}{:}\PYG{o}{:}\PYG{n}{matrix\PYGZus{}a}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{WMMA\PYGZus{}M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{WMMA\PYGZus{}N}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{WMMA\PYGZus{}K}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{half}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{wmma}\PYG{o}{:}\PYG{o}{:}\PYG{n}{row\PYGZus{}major}\PYG{o}{\PYGZgt{}}\PYG{+w}{ }\PYG{n}{a\PYGZus{}frag}\PYG{p}{;}
\PYG{n}{wmma}\PYG{o}{:}\PYG{o}{:}\PYG{n}{fragment}\PYG{o}{\PYGZlt{}}\PYG{n}{wmma}\PYG{o}{:}\PYG{o}{:}\PYG{n}{matrix\PYGZus{}b}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{WMMA\PYGZus{}M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{WMMA\PYGZus{}N}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{WMMA\PYGZus{}K}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{half}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{wmma}\PYG{o}{:}\PYG{o}{:}\PYG{n}{col\PYGZus{}major}\PYG{o}{\PYGZgt{}}\PYG{+w}{ }\PYG{n}{b\PYGZus{}frag}\PYG{p}{;}
\PYG{n}{wmma}\PYG{o}{:}\PYG{o}{:}\PYG{n}{fragment}\PYG{o}{\PYGZlt{}}\PYG{n}{wmma}\PYG{o}{:}\PYG{o}{:}\PYG{n}{accumulator}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{WMMA\PYGZus{}M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{WMMA\PYGZus{}N}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{WMMA\PYGZus{}K}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{o}{\PYGZgt{}}\PYG{+w}{ }\PYG{n}{acc\PYGZus{}frag}\PYG{p}{;}
\PYG{n}{wmma}\PYG{o}{:}\PYG{o}{:}\PYG{n}{fragment}\PYG{o}{\PYGZlt{}}\PYG{n}{wmma}\PYG{o}{:}\PYG{o}{:}\PYG{n}{accumulator}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{WMMA\PYGZus{}M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{WMMA\PYGZus{}N}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{WMMA\PYGZus{}K}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{o}{\PYGZgt{}}\PYG{+w}{ }\PYG{n}{c\PYGZus{}frag}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
使用时，我们需要将待执行乘法操作矩阵块的数据，作为Fragment，由寄存器加载至TensorCore，在将累加Fragment初始化/清零操作后，通过TensorCore单元执行乘累加运算，最后将运算结果的Fragment存回寄存器或其他内存区域。与上述操作对应的，NVIDIA提供了\(wmma.load\_matrix\_sync(), wmma.store\_matrix\_sync()\)接口用于将参与计算的子矩阵块写入/载出Fragment片段；\(wmma.fill\_fragment()\)接口用于初始化对应Fragment的数据；\(wmma.mma\_sync()\)接口用于对Fragment进行乘累加运算。
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{指令层级}

\end{itemize}

\sphinxAtStartPar
在NVIDIA PTX ISA (Instruction Set
Architecture)中提供了另一个编程接口，如Volta架构中的\(mma.sync.m8n8k4\)指令，它使用\(M=8, N=8, K=4\)的形状配置执行乘累加操作。具体地，它由线程组（黑色椭圆表示）或octet执行
\sphinxcite{chapter_accelerator/summary:modeling}，如
\hyperref[\detokenize{chapter_accelerator/accelerator_programming:ptx}]{图\ref{\detokenize{chapter_accelerator/accelerator_programming:ptx}}}显示了线程和数据的映射关系。每个线程组由四个连续的线程组成，使用不同颜色的圆圈表示。图中还指出了一个octet里面的线程在线程束内的分布，Float16乘法器A或B的四个连续元素（使用具有相同颜色的块表示），以及Float32累加器C或D的八个分散元素（同样使用相同颜色的块表示）。彩色块上的数字代表对应的线程ID。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ptx}.svg}
\caption{mma指令之线程与矩阵元素映射关系}\label{\detokenize{chapter_accelerator/accelerator_programming:id23}}\label{\detokenize{chapter_accelerator/accelerator_programming:ptx}}\end{figure}

\sphinxAtStartPar
作为一个更细粒度的指令，mma可以组成更加多样化形状的Warp范围的WMMA
API接口，可以控制线程束内线程与数据的映射关系，并允许AI编译器自动/手动显式地管理内存层次结构之间的矩阵分解，因此相比于直接应用NVCUDA::WMMA
API具有更好的灵活性。


\subsubsection{算子编译器编程使能加速器}
\label{\detokenize{chapter_accelerator/accelerator_programming:id17}}
\sphinxAtStartPar
基于算子编译器使能加速器实现矩阵乘的流程则对用户更加友好。以在Ascend中使用TBE为例，用户只需基于python定义矩阵乘的tensor信息（数据类型及形状等），调用对应TBE接口即可。如下所示：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a\PYGZus{}shape} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{1024}\PYG{p}{,} \PYG{l+m+mi}{256}\PYG{p}{)}
\PYG{n}{b\PYGZus{}shape} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{l+m+mi}{512}\PYG{p}{)}
\PYG{n}{bias\PYGZus{}shape} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{512}\PYG{p}{,} \PYG{p}{)}
\PYG{n}{in\PYGZus{}dtype} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{float16}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{dst\PYGZus{}dtype} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{float32}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{tensor\PYGZus{}a} \PYG{o}{=} \PYG{n}{tvm}\PYG{o}{.}\PYG{n}{placeholder}\PYG{p}{(}\PYG{n}{a\PYGZus{}shape}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tensor\PYGZus{}a}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{in\PYGZus{}dtype}\PYG{p}{)}
\PYG{n}{tensor\PYGZus{}b} \PYG{o}{=} \PYG{n}{tvm}\PYG{o}{.}\PYG{n}{placeholder}\PYG{p}{(}\PYG{n}{b\PYGZus{}shape}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tensor\PYGZus{}b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{in\PYGZus{}dtype}\PYG{p}{)}
\PYG{n}{tensor\PYGZus{}bias} \PYG{o}{=} \PYG{n}{tvm}\PYG{o}{.}\PYG{n}{placeholder}\PYG{p}{(}\PYG{n}{bias\PYGZus{}shape}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tensor\PYGZus{}bias}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{dst\PYGZus{}dtype}\PYG{p}{)}
\PYG{n}{res} \PYG{o}{=} \PYG{n}{te}\PYG{o}{.}\PYG{n}{lang}\PYG{o}{.}\PYG{n}{cce}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{tensor\PYGZus{}a}\PYG{p}{,} \PYG{n}{tensor\PYGZus{}b}\PYG{p}{,} \PYG{k+kc}{False}\PYG{p}{,} \PYG{k+kc}{False}\PYG{p}{,} \PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{dst\PYGZus{}dtype}\PYG{o}{=}\PYG{n}{dst\PYGZus{}dtype}\PYG{p}{,} \PYG{n}{tensor\PYGZus{}bias}\PYG{o}{=}\PYG{n}{tensor\PYGZus{}bias}\PYG{p}{)}
\end{sphinxVerbatim}


\section{加速器实践}
\label{\detokenize{chapter_accelerator/accelerator_practise:id1}}\label{\detokenize{chapter_accelerator/accelerator_practise::doc}}
\sphinxAtStartPar
在本节中，我们会通过具体的CUDA代码向读者介绍如何编写一个并行计算的广义矩阵乘法程序，通过提高计算强度、使用共享内存、优化内存读取流水线等方法最终取得接近硬件加速器性能峰值的实现。虽然在以上章节介绍了TensorCore相关的内容，但由于篇幅限制，我们在本节中不使用此硬件结构。通过使用更为基本的CUDA代码实现FP32的广义矩阵乘法，与此同时并讲解若干实用优化策略。
\#\#\# 环境

\sphinxAtStartPar
本节的实践有以下的软件环境依赖：
\begin{itemize}
\item {} 
\sphinxAtStartPar
Eigen：Eigen是一个线性代数C++模板库，用户可以只使用几条语句完成多线程线性代数运算。

\item {} 
\sphinxAtStartPar
OpenMP（可选）：OpenMP是用于共享内存并行系统的多处理器程序设计的一套指导性编译处理方案，我们可以使用OpenMP对Eigen的计算进行加速。

\item {} 
\sphinxAtStartPar
CUDA Toolkit：CUDA
Toolkit是英伟达发布的CUDA工具包，其包含了CUDA编译器（NVCC），CUDA线性代数库（cuBLAS）等组件。
本节的实践都是在CPU Intex Xeon E5\sphinxhyphen{}2650 v3，GPU Nvidia Geforce RTX
3080；系统Ubuntu 18.04版本，CUDA Toolkit 11.1进行的。

\end{itemize}


\subsection{安装}
\label{\detokenize{chapter_accelerator/accelerator_practise:id2}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Eigen：Eigen的安装可以通过使用包管理器安装（如使用指令\sphinxcode{\sphinxupquote{apt install libeigen3\sphinxhyphen{}dev}}），也可以从\sphinxhref{https://eigen.tuxfamily.org/index.php?title=Main\_Page}{官网}%
\begin{footnote}[24]\sphinxAtStartFootnote
\sphinxnolinkurl{https://eigen.tuxfamily.org/index.php?title=Main\_Page}
%
\end{footnote}下载。

\item {} 
\sphinxAtStartPar
OpenMP（可选）：通常会被大多数编译器默认支持，如果没有被支持的话可以使用包管理器安装（如使用指令\sphinxcode{\sphinxupquote{apt install libomp\sphinxhyphen{}dev}}）。

\item {} 
\sphinxAtStartPar
CUDA Toolkit：CUDA
Toolkit的安装建议按照\sphinxhref{https://developer.nvidia.com/cuda-downloads}{官方的提示}%
\begin{footnote}[25]\sphinxAtStartFootnote
\sphinxnolinkurl{https://developer.nvidia.com/cuda-downloads}
%
\end{footnote}安装，也可以通过使用包管理器安装（如使用指令\sphinxcode{\sphinxupquote{apt install cuda}}）。

\end{itemize}


\subsubsection{广义矩阵乘法的朴素实现}
\label{\detokenize{chapter_accelerator/accelerator_practise:sec-accelerator-naive}}\label{\detokenize{chapter_accelerator/accelerator_practise:id3}}
\sphinxAtStartPar
广义矩阵乘法指GEMM（General Matrix
Multiplication），即\(C = \alpha A\times B + \beta C\)，其中\(A\in\mathbb{R}^{M\times K}, B\in\mathbb{R}^{K\times N}, C\in\mathbb{R}^{M\times N}\)。

\sphinxAtStartPar
矩阵\(C\)
的第\(m\)行第\(n\)列元素\(C_{m, n}\)
是由矩阵\(A\)中第\(m\)行\(K\)维向量和矩阵\(B\)中第\(n\)列\(K\)维向量的内积与\(C_{m, n}\)原始值加权求和得到的，因此在这种视角下的CPU代码为：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{A}\PYG{p}{[}\PYG{n}{M}\PYG{p}{]}\PYG{p}{[}\PYG{n}{K}\PYG{p}{]}\PYG{p}{;}
\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{B}\PYG{p}{[}\PYG{n}{K}\PYG{p}{]}\PYG{p}{[}\PYG{n}{N}\PYG{p}{]}\PYG{p}{;}
\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{C}\PYG{p}{[}\PYG{n}{M}\PYG{p}{]}\PYG{p}{[}\PYG{n}{N}\PYG{p}{]}\PYG{p}{;}
\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{;}

\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{m}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{m}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{m}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{n}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{n}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{n}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{        }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{c}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{+w}{        }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{k}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{k}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{k}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{            }\PYG{n}{c}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{=}\PYG{+w}{ }\PYG{n}{A}\PYG{p}{[}\PYG{n}{m}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{B}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{[}\PYG{n}{n}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{        }\PYG{p}{\PYGZcb{}}
\PYG{+w}{        }\PYG{n}{C}\PYG{p}{[}\PYG{n}{m}\PYG{p}{]}\PYG{p}{[}\PYG{n}{n}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{alpha}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{c}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{beta}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{C}\PYG{p}{[}\PYG{n}{m}\PYG{p}{]}\PYG{p}{[}\PYG{n}{n}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
可以看到，矩阵\(C\)
中各个元素的计算是独立的。我们可以利用GPU的大量线程去分别计算矩阵\(C\)
中相应的元素，以达到并行计算的目的，GPU核函数将如下所示：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{\PYGZus{}\PYGZus{}global\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{k+kt}{void}\PYG{+w}{ }\PYG{n}{gemmKernel}\PYG{p}{(}\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{A}\PYG{p}{,}
\PYG{+w}{                           }\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{B}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{C}\PYG{p}{,}
\PYG{+w}{                           }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{,}
\PYG{+w}{                           }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{m}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{blockDim}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{blockIdx}\PYG{p}{.}\PYG{n}{x}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{n}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{y}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{blockDim}\PYG{p}{.}\PYG{n}{y}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{blockIdx}\PYG{p}{.}\PYG{n}{y}\PYG{p}{;}
\PYG{+w}{  }\PYG{k}{if}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{\PYGZgt{}}\PYG{o}{=}\PYG{+w}{ }\PYG{n}{M}\PYG{+w}{ }\PYG{o}{|}\PYG{o}{|}\PYG{+w}{ }\PYG{n}{n}\PYG{+w}{ }\PYG{o}{\PYGZgt{}}\PYG{o}{=}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{)}
\PYG{+w}{      }\PYG{k}{return}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{c}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{+w}{  }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{k}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{k}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{k}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n}{c}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{=}\PYG{+w}{ }\PYG{n}{A}\PYG{p}{[}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{K}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{k}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{B}\PYG{p}{[}\PYG{n}{k}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{N}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{n}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{n}{c}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{c}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{result}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{c}\PYG{p}{;}
\PYG{+w}{  }\PYG{k}{if}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{beta}\PYG{+w}{ }\PYG{o}{!}\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n}{result}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{result}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{C}\PYG{p}{[}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{N}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{n}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{n}{C}\PYG{p}{[}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{N}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{n}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{result}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
其可视化结构如
\hyperref[\detokenize{chapter_accelerator/accelerator_practise:cuda-naive-gemm}]{图\ref{\detokenize{chapter_accelerator/accelerator_practise:cuda-naive-gemm}}}所示，矩阵\(C\)中每一个元素由一个线程计算，在GPU
Kernel的第5和6行计算该线程对应矩阵\(C\)中的元素行号\(m\)及列号\(n\)，然后在第9到11行该线程利用行号与列号读取矩阵\(A\)和矩阵\(B\)中相应的行列向量元素并计算向量内积，最后在第17行将结果写回\(C\)矩阵。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{naive}.svg}
\caption{矩阵乘法的朴素实现}\label{\detokenize{chapter_accelerator/accelerator_practise:id24}}\label{\detokenize{chapter_accelerator/accelerator_practise:cuda-naive-gemm}}\end{figure}

\sphinxAtStartPar
使用以下代码启动核函数：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kt}{void}\PYG{+w}{ }\PYG{n+nf}{gemmNaive}\PYG{p}{(}\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{A}\PYG{p}{,}\PYG{+w}{ }\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{B}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{C}\PYG{p}{,}
\PYG{+w}{               }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}
\PYG{+w}{               }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n}{dim3}\PYG{+w}{ }\PYG{n}{block}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{16}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{dim3}\PYG{+w}{ }\PYG{n}{grid}\PYG{p}{(}\PYG{p}{(}\PYG{n}{M}\PYG{+w}{ }\PYG{o}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{block}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{N}\PYG{+w}{ }\PYG{o}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{block}\PYG{p}{.}\PYG{n}{y}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{;}

\PYG{+w}{  }\PYG{n}{gemmKernel}\PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}}\PYG{n}{grid}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{block}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{B}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{C}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
在这里我们令每个线程块处理矩阵\(C\)中\(16\times16\)个元素，因此我们开启\((M - 1) / 16 + 1 \times (N - 1) / 16 + 1\)个线程块用于计算整个矩阵\(C\)。

\sphinxAtStartPar
接下来我们生成数据并执行：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include}\PYG{+w}{ }\PYG{c+cpf}{\PYGZlt{}Eigen/Core\PYGZgt{}}

\PYG{k}{using}\PYG{+w}{ }\PYG{k}{namespace}\PYG{+w}{ }\PYG{n+nn}{Eigen}\PYG{p}{;}
\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n+nf}{main}\PYG{p}{(}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{M}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{2048}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{N}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{2048}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{K}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{1024}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{alpha}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mf}{1.}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{beta}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mf}{1.}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{deviceAPrt}\PYG{p}{,}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{deviceBPtr}\PYG{p}{,}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{deviceCPtr}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{Matrix}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{Dynamic}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{Dynamic}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{RowMajor}\PYG{o}{\PYGZgt{}}\PYG{+w}{ }\PYG{n}{A}\PYG{p}{\PYGZob{}}\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{\PYGZcb{}}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{B}\PYG{p}{\PYGZob{}}\PYG{n}{K}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{\PYGZcb{}}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{C}\PYG{p}{\PYGZob{}}\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{A}\PYG{p}{.}\PYG{n}{setRandom}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{B}\PYG{p}{.}\PYG{n}{setRandom}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{C}\PYG{p}{.}\PYG{n}{setRandom}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cudaMalloc}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{deviceAPrt}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{K}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{k+kt}{float}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cudaMemcpy}\PYG{p}{(}\PYG{n}{deviceAPrt}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{A}\PYG{p}{.}\PYG{n}{data}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{K}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{k+kt}{float}\PYG{p}{)}\PYG{p}{,}
\PYG{+w}{             }\PYG{n}{cudaMemcpyHostToDevice}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cudaMalloc}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{deviceBPtr}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{K}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{N}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{k+kt}{float}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cudaMemcpy}\PYG{p}{(}\PYG{n}{deviceBPtr}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{B}\PYG{p}{.}\PYG{n}{data}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{K}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{N}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{k+kt}{float}\PYG{p}{)}\PYG{p}{,}
\PYG{+w}{             }\PYG{n}{cudaMemcpyHostToDevice}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cudaMalloc}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{deviceCPtr}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{N}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{k+kt}{float}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cudaMemcpy}\PYG{p}{(}\PYG{n}{deviceCPtr}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{C}\PYG{p}{.}\PYG{n}{data}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{N}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{k+kt}{float}\PYG{p}{)}\PYG{p}{,}
\PYG{+w}{             }\PYG{n}{cudaMemcpyHostToDevice}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{gemmNaive}\PYG{p}{(}\PYG{n}{deviceAPrt}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{deviceBPtr}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{deviceCPtr}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cudaDeviceSynchronize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
我们在代码的第8到11行利用Eigen构建并初始化矩阵\(A, B, C\)。在代码的第12到20行分配GPU内存并将CPU数据拷贝到GPU端。最后我们在第21行执行函数并在22行等待该函数结束。

\sphinxAtStartPar
接下来我们需要对我们实现的GPU代码测速并验证其数值正确性。对GPU代码的测速我们使用\sphinxcode{\sphinxupquote{cudaEvent}}，\sphinxcode{\sphinxupquote{cudaEvent}}可以记录GPU端程序事务并用来计算两个事务之间的耗时，使用方法如下段代码：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cudaEvent\PYGZus{}t}\PYG{+w}{ }\PYG{n}{startEvent}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{stopEvent}\PYG{p}{;}
\PYG{n}{cudaEventCreate}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{startEvent}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cudaEventCreate}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{stopEvent}\PYG{p}{)}\PYG{p}{;}

\PYG{n}{cudaEventRecord}\PYG{p}{(}\PYG{n}{startEvent}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{gemmNaive}\PYG{p}{(}\PYG{n}{deviceAPrt}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{deviceBPtr}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{deviceCPtr}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cudaEventRecord}\PYG{p}{(}\PYG{n}{stopEvent}\PYG{p}{)}\PYG{p}{;}

\PYG{n}{cudaEventSynchronize}\PYG{p}{(}\PYG{n}{stopEvent}\PYG{p}{)}\PYG{p}{;}
\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{milliseconds}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{n}{cudaEventElapsedTime}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{milliseconds}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{startEvent}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{stopEvent}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Average Time: \PYGZpc{}.3f ms}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{\PYGZdq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{milliseconds}\PYG{p}{)}\PYG{p}{;}

\PYG{n}{cudaEventDestroy}\PYG{p}{(}\PYG{n}{stopEvent}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cudaEventDestroy}\PYG{p}{(}\PYG{n}{startEvent}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
具体地，我们首先声明类型为\sphinxcode{\sphinxupquote{cudaEvent\_t}}的变量，然后使用第2及第3行所示代码创建GPU事务，在待测的GPU代码起始时使用第5行的代码记录起始事务并在GPU代码结束后使用第7行的代码记录结束事务。通过使用第9到第12行代码计算两次事务间的时间差并打印，最后使用第12及第13行代码销毁GPU事务。

\sphinxAtStartPar
执行这段代码，得到输出结果：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Average} \PYG{n}{Time}\PYG{p}{:} \PYG{l+m+mf}{46.354} \PYG{n}{ms}
\end{sphinxVerbatim}

\sphinxAtStartPar
接下来我们实现数值验证的相关代码并计算CPU耗时：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include}\PYG{+w}{ }\PYG{c+cpf}{\PYGZlt{}omp.h\PYGZgt{}}

\PYG{c+c1}{// ...}

\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n+nf}{main}\PYG{p}{(}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{c+c1}{// ...}
\PYG{+w}{  }\PYG{n}{Matrix}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{Dynamic}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{Dynamic}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{RowMajor}\PYG{o}{\PYGZgt{}}\PYG{+w}{ }\PYG{n}{hostResult}\PYG{p}{\PYGZob{}}\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n}{deviceResult}\PYG{p}{\PYGZob{}}\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{omp\PYGZus{}set\PYGZus{}num\PYGZus{}threads}\PYG{p}{(}\PYG{n}{omp\PYGZus{}get\PYGZus{}num\PYGZus{}procs}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{clock\PYGZus{}t}\PYG{+w}{ }\PYG{n}{begin}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{end}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{begin}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{clock}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{hostResult}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{alpha}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{A}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{B}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{beta}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{C}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{end}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{clock}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Average Time: \PYGZpc{}.3f ms}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{\PYGZdq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{double}\PYG{p}{(}\PYG{n}{end}\PYG{+w}{ }\PYG{o}{\PYGZhy{}}\PYG{+w}{ }\PYG{n}{begin}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{CLOCKS\PYGZus{}PER\PYGZus{}SEC}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{l+m+mf}{1e3}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cudaMemcpy}\PYG{p}{(}\PYG{n}{deviceResult}\PYG{p}{.}\PYG{n}{data}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{deviceCPtr}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{N}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{k+kt}{float}\PYG{p}{)}\PYG{p}{,}
\PYG{+w}{             }\PYG{n}{cudaMemcpyDeviceToHost}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cudaDeviceSynchronize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

\PYG{+w}{  }\PYG{n}{Eigen}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Array}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{float}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{Eigen}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Dynamic}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{Eigen}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Dynamic}\PYG{o}{\PYGZgt{}}\PYG{+w}{ }\PYG{n}{diffArray}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{      }\PYG{p}{(}\PYG{n}{hostResult}\PYG{+w}{ }\PYG{o}{\PYGZhy{}}\PYG{+w}{ }\PYG{n}{deviceResult}\PYG{p}{)}\PYG{p}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{)}\PYG{p}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Max Error: \PYGZpc{}f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{\PYGZdq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{diffArray}\PYG{p}{.}\PYG{n}{maxCoeff}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
我们在第9到14行使用CPU计算结果并计时，在第15行将GPU计算的结果拷贝到CPU内存中，在第19到21行计算误差并打印。值得注意的是我们在第9行使用了OpenMP以启用CPU多线程计算一般矩阵乘法。

\sphinxAtStartPar
完整代码在\sphinxhref{https://github.com/openmlsys/openmlsys-cuda/blob/main/first\_attempt.cu}{first\_attempt.cu}%
\begin{footnote}[26]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/openmlsys/openmlsys-cuda/blob/main/first\_attempt.cu}
%
\end{footnote}中，编译及执行指令如下：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mkdir build \PYG{o}{\PYGZam{}\PYGZam{}} \PYG{n+nb}{cd} build
cmake ..
make first\PYGZus{}attempt
./first\PYGZus{}attempt
\end{sphinxVerbatim}

\sphinxAtStartPar
输出结果为：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Average} \PYG{n}{Time}\PYG{p}{:} \PYG{l+m+mf}{48.961} \PYG{n}{ms}
\PYG{n}{Max} \PYG{n}{Error}\PYG{p}{:} \PYG{l+m+mf}{0.000092}
\end{sphinxVerbatim}

\sphinxAtStartPar
我们可以使用以下公式粗略的计算GPU的峰值吞吐量：2\(\times\)频率\(\times\)单精度计算单元数量
，其中单精度计算单元数量等于GPU中流多处理器（SM）数量乘每个流多处理器中单精度计算单元数量；并用以下公式粗略的计算我们代码的吞吐量：2\(\times\)数据量\(\div\)时间：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n+nf}{main}\PYG{p}{(}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{c+c1}{// ...}
\PYG{+w}{  }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{gpu\PYGZus{}rank}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cudaDeviceProp}\PYG{+w}{ }\PYG{n}{deviceProp}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cudaGetDeviceProperties}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{deviceProp}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{gpu\PYGZus{}rank}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cudaSetDevice}\PYG{p}{(}\PYG{n}{gpu\PYGZus{}rank}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{double}\PYG{+w}{ }\PYG{n}{boostFrequency}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{deviceProp}\PYG{p}{.}\PYG{n}{clockRate}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{l+m+mf}{1e6}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{int1}\PYG{+w}{ }\PYG{n}{fp32CoresNum}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{128}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{double}\PYG{+w}{ }\PYG{n}{peakPerformance}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{boostFrequency}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{fp32CoresNum}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{l+m+mi}{2}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{FP32 peak throughput \PYGZpc{}.3f GFLOPS}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{\PYGZdq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{peakPerformance}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{double}\PYG{+w}{ }\PYG{n}{GFLOPS}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{2}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{l+m+mf}{1e\PYGZhy{}9}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{M}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{N}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{K}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{milliseconds}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Average Throughput: \PYGZpc{}.3f GFLOPS}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{\PYGZdq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{GFLOPS}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
执行可以输出：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{FP32} \PYG{n}{peak} \PYG{n}{throughput} \PYG{l+m+mf}{29767.680} \PYG{n}{GFLOPS}
\PYG{n}{Average} \PYG{n}{Throughput}\PYG{p}{:} \PYG{l+m+mf}{185.313} \PYG{n}{GFLOPS}
\end{sphinxVerbatim}

\sphinxAtStartPar
可以发现目前的代码距离设备峰值性能仍有较大的差距。在整个计算过程中计算密集最大的过程为矩阵乘法\(A\times B\)，其时间复杂度为\(O(M*N*K)\)，而整个计算过程时间复杂度为\(O(M*N*K+2*M*N)\)，因此对矩阵乘法的优化是提升性能的关键。


\subsection{使用封装结构代替指针}
\label{\detokenize{chapter_accelerator/accelerator_practise:id4}}
\sphinxAtStartPar
在上面的实现中，由于二维矩阵的数据是使用一维数组进行存储，所以在访问数据时需要使用行坐标与二维矩阵宽度的乘积和列坐标的和来索引到具体位置的元素，这样的访问方式并不直观且在后续逐渐复杂的实现中容易出错。因此，我们可以自定义一个结构体，通过重载
\sphinxcode{\sphinxupquote{()}} 运算符，实现对矩阵元素的二维索引，同时我们提供 \sphinxcode{\sphinxupquote{addOffset}}
方法用于加入一个固定的偏移，具体实现如下：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{template}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{k}{typename} \PYG{n+nc}{T}\PYG{o}{\PYGZgt{}}
\PYG{k}{struct} \PYG{n+nc}{\PYGZus{}\PYGZus{}device\PYGZus{}builtin\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{Tensor2D}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n}{T}\PYG{+w}{ }\PYG{o}{*}\PYG{k}{const}\PYG{+w}{ }\PYG{n}{\PYGZus{}\PYGZus{}restrict\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{ptr}\PYG{p}{;}
\PYG{+w}{  }\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{rows}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{cols}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{\PYGZus{}rowOffset}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{\PYGZcb{}}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{\PYGZus{}colOffset}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{+w}{  }\PYG{k}{template}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{k}{typename} \PYG{n+nc}{t}\PYG{o}{\PYGZgt{}}
\PYG{+w}{  }\PYG{n}{\PYGZus{}\PYGZus{}host\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{\PYGZus{}\PYGZus{}device\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{Tensor2D}\PYG{p}{(}\PYG{n}{t}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{o}{\PYGZam{}}\PYG{n}{ptr}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{rows}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{cols}\PYG{p}{)}
\PYG{+w}{      }\PYG{o}{:}\PYG{+w}{ }\PYG{n}{ptr}\PYG{p}{\PYGZob{}}\PYG{k}{reinterpret\PYGZus{}cast}\PYG{o}{\PYGZlt{}}\PYG{n}{T}\PYG{+w}{ }\PYG{o}{*}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{n}{ptr}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{rows}\PYG{p}{\PYGZob{}}\PYG{n}{rows}\PYG{p}{\PYGZcb{}}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{cols}\PYG{p}{\PYGZob{}}\PYG{n}{cols}\PYG{p}{\PYGZcb{}}\PYG{+w}{ }\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{+w}{  }\PYG{n}{\PYGZus{}\PYGZus{}host\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{\PYGZus{}\PYGZus{}device\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{T}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{k}{operator}\PYG{p}{(}\PYG{p}{)}\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{row}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{col}\PYG{p}{)}\PYG{+w}{ }\PYG{k}{const}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{k}{return}\PYG{+w}{ }\PYG{n}{ptr}\PYG{p}{[}\PYG{n}{\PYGZus{}colOffset}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{col}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{row}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{\PYGZus{}rowOffset}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{cols}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}

\PYG{+w}{  }\PYG{k}{template}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{k}{typename} \PYG{n+nc}{t}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{T}\PYG{o}{\PYGZgt{}}
\PYG{+w}{  }\PYG{n}{\PYGZus{}\PYGZus{}host\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{\PYGZus{}\PYGZus{}device\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{k+kt}{void}\PYG{+w}{ }\PYG{n}{addOffset}\PYG{p}{(}\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{rowOffset}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{colOffset}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n}{\PYGZus{}rowOffset}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{=}\PYG{+w}{ }\PYG{n}{rowOffset}\PYG{p}{;}
\PYG{+w}{    }\PYG{n}{\PYGZus{}colOffset}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{=}\PYG{+w}{ }\PYG{n}{colOffset}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{n}{t}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{n}{T}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
另外，我们在读取数据数据之前需要对偏移判断是否越界，因此我们增加以下方法：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{template}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{k}{typename} \PYG{n+nc}{T}\PYG{o}{\PYGZgt{}}
\PYG{k}{struct} \PYG{n+nc}{\PYGZus{}\PYGZus{}device\PYGZus{}builtin\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{Tensor2D}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{c+c1}{// ...}
\PYG{+w}{  }\PYG{n}{\PYGZus{}\PYGZus{}host\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{\PYGZus{}\PYGZus{}device\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{k+kt}{bool}\PYG{+w}{ }\PYG{n}{validRowOffset}\PYG{p}{(}\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{rowOffset}\PYG{p}{)}\PYG{+w}{ }\PYG{k}{const}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{k}{return}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{\PYGZus{}rowOffset}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{rowOffset}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{rows}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}

\PYG{+w}{  }\PYG{n}{\PYGZus{}\PYGZus{}host\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{\PYGZus{}\PYGZus{}device\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{k+kt}{bool}\PYG{+w}{ }\PYG{n}{validColOffset}\PYG{p}{(}\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{colOffset}\PYG{p}{)}\PYG{+w}{ }\PYG{k}{const}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{k}{return}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{\PYGZus{}colOffset}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{colOffset}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{cols}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}

\PYG{+w}{  }\PYG{n}{\PYGZus{}\PYGZus{}host\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{\PYGZus{}\PYGZus{}device\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{k+kt}{bool}\PYG{+w}{ }\PYG{n}{validOffset}\PYG{p}{(}\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{rowOffset}\PYG{p}{,}
\PYG{+w}{                                       }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{colOffset}\PYG{p}{)}\PYG{+w}{ }\PYG{k}{const}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{k}{return}\PYG{+w}{ }\PYG{n}{validRowOffset}\PYG{p}{(}\PYG{n}{rowOffset}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{o}{\PYGZam{}}\PYG{+w}{ }\PYG{n}{validColOffset}\PYG{p}{(}\PYG{n}{colOffset}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
完整代码在\sphinxhref{https://github.com/openmlsys/openmlsys-cuda/blob/main/util.cuh}{util.cuh}%
\begin{footnote}[27]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/openmlsys/openmlsys-cuda/blob/main/util.cuh}
%
\end{footnote}。
最终，我们可以将GPU核函数改写为以下形式：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{\PYGZus{}\PYGZus{}global\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{k+kt}{void}\PYG{+w}{ }\PYG{n}{gemmKernel}\PYG{p}{(}\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{A}\PYG{p}{,}
\PYG{+w}{                           }\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{B}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{C}\PYG{p}{,}
\PYG{+w}{                           }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{,}
\PYG{+w}{                           }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{m}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{blockDim}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{blockIdx}\PYG{p}{.}\PYG{n}{x}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{n}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{y}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{blockDim}\PYG{p}{.}\PYG{n}{y}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{blockIdx}\PYG{p}{.}\PYG{n}{y}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{Tensor2D}\PYG{o}{\PYGZlt{}}\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{o}{\PYGZgt{}}\PYG{+w}{ }\PYG{n}{tensorA}\PYG{p}{\PYGZob{}}\PYG{n}{A}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{Tensor2D}\PYG{o}{\PYGZlt{}}\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{o}{\PYGZgt{}}\PYG{+w}{ }\PYG{n}{tensorB}\PYG{p}{\PYGZob{}}\PYG{n}{B}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{Tensor2D}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{float}\PYG{o}{\PYGZgt{}}\PYG{+w}{ }\PYG{n}{tensorC}\PYG{p}{\PYGZob{}}\PYG{n}{C}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{+w}{  }\PYG{k}{if}\PYG{+w}{ }\PYG{p}{(}\PYG{o}{!}\PYG{n}{tensorC}\PYG{p}{.}\PYG{n}{validOffset}\PYG{p}{(}\PYG{n}{m}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{n}\PYG{p}{)}\PYG{p}{)}\PYG{+w}{ }\PYG{k}{return}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{c}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{+w}{  }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{k}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{k}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{k}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n}{c}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{=}\PYG{+w}{ }\PYG{n}{tensorA}\PYG{p}{(}\PYG{n}{m}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{k}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tensorB}\PYG{p}{(}\PYG{n}{k}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{n}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{n}{c}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{c}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{result}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{c}\PYG{p}{;}
\PYG{+w}{  }\PYG{k}{if}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{beta}\PYG{+w}{ }\PYG{o}{!}\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n}{result}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{result}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{tensorC}\PYG{p}{(}\PYG{n}{m}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{n}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{n}{tensorC}\PYG{p}{(}\PYG{n}{m}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{n}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{result}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsubsection{提高计算强度}
\label{\detokenize{chapter_accelerator/accelerator_practise:id5}}
\sphinxAtStartPar
计算强度（Compute
Intensity）指计算指令数量与访存指令数量的比值，在现代GPU中往往有大量计算单元但只有有限的访存带宽，程序很容易出现计算单元等待数据读取的问题，因此提高计算强度是提升程序性能的一条切实有限的指导思路。对于之前实现的GPU核函数，我们可以粗略计算其计算强度：在\(K\)次循环的内积计算中，对矩阵\(A\)与矩阵\(B\)的每次读取会计算一次浮点乘法与浮点加法，因此计算强度为1——两次浮点运算除以两次数据读取。之前的版本是每个线程负责处理矩阵\(C\)的一个元素——计算矩阵{}{}\(A\)的一行与矩阵\(B\)的一列的内积，我们可以通过使每个线程计算\(C\)更多的元素——计算矩阵\(A\)的多行与矩阵\(B\)的多列的内积——从而提升计算强度。具体地，如果在\(K\)次循环的内积计算中一次读取矩阵\(A\)中的\(m\)个元素和矩阵\(B\)中的\(n\)个元素，那么访存指令为\(m+n\)条，而计算指令为\(2mn\)条，所以计算强度为\(\frac{2mn}{m+n}\)，因此可以很容易发现提高\(m\)和\(n\)会带来计算强度的提升。

\sphinxAtStartPar
我们在上一个代码例子中对全局内存的访问与存储都是借助 \sphinxcode{\sphinxupquote{float}}
指针完成的，具体到硬件指令集上实际是使用指令 \sphinxcode{\sphinxupquote{LDG.E}} 与 \sphinxcode{\sphinxupquote{STG.E}}
完成的。我们可以使用128位宽指令\sphinxcode{\sphinxupquote{LDG.E.128}} 与 \sphinxcode{\sphinxupquote{STG.E.128}}
一次读取多个 \sphinxcode{\sphinxupquote{float}}
数。使用宽指令的好处是一方面简化了指令序列，使用一个宽指令代替四个标准指令可以节省十几个指令的发射周期，这可以为计算指令的发射争取到额外的时间；另一方面128比特正好等于一个cache
line的长度，使用宽指令也有助于提高cache
line的命中率。但我们并不提倡在一切代码中过度追求宽指令的使用，开发者应当将更多的时间关注并行性设计和局部数据复用等更直接的优化手段。

\sphinxAtStartPar
具体的实现如下，由于每个 \sphinxcode{\sphinxupquote{float}} 类型大小为32个比特，我们可以将4个
\sphinxcode{\sphinxupquote{float}} 堆叠在一起构成一个128比特的 \sphinxcode{\sphinxupquote{float4}} 类，对 \sphinxcode{\sphinxupquote{float4}}
的访存将会是使用宽指令完成。虽然CUDA Toolkit已经有实现的 \sphinxcode{\sphinxupquote{float4}}
类，但是为了代码抽象我们将自行实现我们自己的 \sphinxcode{\sphinxupquote{float4}} 类。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{struct} \PYG{n+nc}{\PYGZus{}\PYGZus{}device\PYGZus{}builtin\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{\PYGZus{}\PYGZus{}builtin\PYGZus{}align\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{)}\PYG{+w}{ }\PYG{n}{float4}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{;}

\PYG{+w}{  }\PYG{n}{\PYGZus{}\PYGZus{}host\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{\PYGZus{}\PYGZus{}device\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{k}{operator}\PYG{p}{[}\PYG{p}{]}\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{idx}\PYG{p}{)}\PYG{+w}{ }\PYG{k}{const}\PYG{+w}{ }\PYG{p}{\PYGZob{}}\PYG{+w}{ }\PYG{k}{return}\PYG{+w}{ }\PYG{n}{data}\PYG{p}{[}\PYG{n}{idx}\PYG{p}{]}\PYG{p}{;}\PYG{+w}{ }\PYG{p}{\PYGZcb{}}

\PYG{+w}{  }\PYG{n}{\PYGZus{}\PYGZus{}host\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{\PYGZus{}\PYGZus{}device\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{k}{operator}\PYG{p}{[}\PYG{p}{]}\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{idx}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}\PYG{+w}{ }\PYG{k}{return}\PYG{+w}{ }\PYG{n}{data}\PYG{p}{[}\PYG{n}{idx}\PYG{p}{]}\PYG{p}{;}\PYG{+w}{ }\PYG{p}{\PYGZcb{}}

\PYG{+w}{  }\PYG{n}{\PYGZus{}\PYGZus{}host\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{\PYGZus{}\PYGZus{}device\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{float4}\PYG{+w}{ }\PYG{k}{operator}\PYG{o}{*}\PYG{p}{(}\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{other}\PYG{p}{)}\PYG{+w}{ }\PYG{k}{const}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{k}{return}\PYG{+w}{ }\PYG{n}{float4}\PYG{p}{\PYGZob{}}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{other}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{other}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{other}\PYG{p}{,}
\PYG{+w}{                  }\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{other}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}

\PYG{+w}{  }\PYG{n}{\PYGZus{}\PYGZus{}host\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{\PYGZus{}\PYGZus{}device\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{float4}\PYG{+w}{ }\PYG{k}{operator}\PYG{o}{+}\PYG{p}{(}\PYG{k}{const}\PYG{+w}{ }\PYG{n}{float4}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{n}{other}\PYG{p}{)}\PYG{+w}{ }\PYG{k}{const}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{k}{return}\PYG{+w}{ }\PYG{n}{float4}\PYG{p}{\PYGZob{}}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{other}\PYG{p}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{other}\PYG{p}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
\PYG{+w}{                  }\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{other}\PYG{p}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{other}\PYG{p}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
我们重载了\sphinxcode{\sphinxupquote{{[}{]}}}运算符，从而可以通过索引访问\sphinxcode{\sphinxupquote{float4}}
内部元素。此外我们定义了 \sphinxcode{\sphinxupquote{float4}} 与 \sphinxcode{\sphinxupquote{float}} 乘法及 \sphinxcode{\sphinxupquote{float4}} 与
\sphinxcode{\sphinxupquote{float4}}
加法的实现，方便对后续代码抽象。完整代码在\sphinxhref{https://github.com/openmlsys/openmlsys-cuda/blob/main/util.cuh}{util.cuh}%
\begin{footnote}[28]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/openmlsys/openmlsys-cuda/blob/main/util.cuh}
%
\end{footnote}中。

\sphinxAtStartPar
在实现GPU核函数过程中要注意，每个线程需要从原本各读取矩阵\(A\)和矩阵\(B\)中一个
\sphinxcode{\sphinxupquote{float}} 数据变为各读取4个 \sphinxcode{\sphinxupquote{float}}
数据，这就要求现在每个线程负责处理矩阵\(C\)中\(4\times 4\)的矩阵块，我们称之为
\sphinxcode{\sphinxupquote{thread tile}} 。相应的GPU核函数应为以下形式：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{\PYGZus{}\PYGZus{}global\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{k+kt}{void}\PYG{+w}{ }\PYG{n}{gemmKernel}\PYG{p}{(}\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{\PYGZus{}\PYGZus{}restrict\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{A}\PYG{p}{,}
\PYG{+w}{                           }\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{\PYGZus{}\PYGZus{}restrict\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{B}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{\PYGZus{}\PYGZus{}restrict\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{C}\PYG{p}{,}
\PYG{+w}{                           }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{,}
\PYG{+w}{                           }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{kCount}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{n}{float4}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{k+kt}{float}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{m}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{blockDim}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{blockIdx}\PYG{p}{.}\PYG{n}{x}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{n}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{y}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{blockDim}\PYG{p}{.}\PYG{n}{y}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{blockIdx}\PYG{p}{.}\PYG{n}{y}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{Tensor2D}\PYG{o}{\PYGZlt{}}\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{o}{\PYGZgt{}}\PYG{+w}{ }\PYG{n}{tensorA}\PYG{p}{\PYGZob{}}\PYG{n}{A}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{tensorA}\PYG{p}{.}\PYG{n}{addOffset}\PYG{p}{(}\PYG{n}{m}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{Tensor2D}\PYG{o}{\PYGZlt{}}\PYG{k}{const}\PYG{+w}{ }\PYG{n}{float4}\PYG{o}{\PYGZgt{}}\PYG{+w}{ }\PYG{n}{tensorB}\PYG{p}{\PYGZob{}}\PYG{n}{B}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{N}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{tensorB}\PYG{p}{.}\PYG{n}{addOffset}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{n}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{Tensor2D}\PYG{o}{\PYGZlt{}}\PYG{n}{float4}\PYG{o}{\PYGZgt{}}\PYG{+w}{ }\PYG{n}{tensorC}\PYG{p}{\PYGZob{}}\PYG{n}{C}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{N}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{tensorC}\PYG{p}{.}\PYG{n}{addOffset}\PYG{p}{(}\PYG{n}{m}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{n}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{k}{if}\PYG{+w}{ }\PYG{p}{(}\PYG{o}{!}\PYG{n}{tensorC}\PYG{p}{.}\PYG{n}{validOffset}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{)}\PYG{+w}{ }\PYG{k}{return}\PYG{p}{;}

\PYG{+w}{  }\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{c}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{memset}\PYG{p}{(}\PYG{n}{c}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{n}{c}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{k}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{k}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{k}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{fragmentA}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{i}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{      }\PYG{n}{fragmentA}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{tensorA}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{k}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{+w}{    }\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{fragmentB}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{tensorB}\PYG{p}{(}\PYG{n}{k}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{;}

\PYG{+w}{    }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{i}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{      }\PYG{n}{c}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{c}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{fragmentB}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{fragmentA}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}

\PYG{+w}{  }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k}{auto}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{n+nl}{term} \PYG{p}{:}\PYG{+w}{ }\PYG{n}{c}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n}{term}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{term}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}

\PYG{+w}{  }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{i}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{result}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{c}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{    }\PYG{k}{if}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{beta}\PYG{+w}{ }\PYG{o}{!}\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{      }\PYG{n}{result}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{c}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{tensorC}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{;}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{+w}{    }\PYG{n}{tensorC}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{result}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
我们首先在第6到14行计算每个线程需要处理的数据块在矩阵中的起始行列坐标\sphinxcode{\sphinxupquote{m,n}}，即
\hyperref[\detokenize{chapter_accelerator/accelerator_practise:use-float4}]{图\ref{\detokenize{chapter_accelerator/accelerator_practise:use-float4}}}
中矩阵\(C\)中浅绿色数据块的左上角坐标，然后使用\sphinxcode{\sphinxupquote{Tensor2D}}中的\sphinxcode{\sphinxupquote{addOffset}}方法，为每个线程定位到它要处理的数据块的起始位置上，并且利用\sphinxcode{\sphinxupquote{validOffset}}方法判断线程是否越界。然后就可以沿着K方向循环，在第18到23行每个线程分别读取矩阵\(A\)中连续的4行和矩阵\(B\)中连续的四列组成两个
\sphinxcode{\sphinxupquote{float4}} ，即 \hyperref[\detokenize{chapter_accelerator/accelerator_practise:use-float4}]{图\ref{\detokenize{chapter_accelerator/accelerator_practise:use-float4}}}
中粉色与黄色的四个元素。之后在第25到27行计算线程负责处理矩阵\(C\)中的\(4 \times 4\)个元素。最后在第30到40行对结果使用参数
\sphinxcode{\sphinxupquote{alpha}} 和 \sphinxcode{\sphinxupquote{beta}} 进行放缩并写回矩阵\(C\)的内存。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{use_float4}.svg}
\caption{提高计算强度}\label{\detokenize{chapter_accelerator/accelerator_practise:id25}}\label{\detokenize{chapter_accelerator/accelerator_practise:use-float4}}\end{figure}

\sphinxAtStartPar
为了将尺寸数据在编译期确定，减少执行期的额外数据读取开销，我们引入一个新的模板类
\sphinxcode{\sphinxupquote{Layout}} ，这个类保存各种尺寸数据。其实现代码如下：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{template}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{\PYGZus{}m}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{\PYGZus{}n}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{\PYGZus{}k}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{o}{\PYGZgt{}}
\PYG{k}{struct} \PYG{n+nc}{Layout}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{ }\PYG{k}{static}\PYG{+w}{ }\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{m}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{\PYGZus{}m}\PYG{p}{;}
\PYG{+w}{ }\PYG{k}{static}\PYG{+w}{ }\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{n}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{\PYGZus{}n}\PYG{p}{;}
\PYG{+w}{ }\PYG{k}{static}\PYG{+w}{ }\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{k}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{\PYGZus{}k}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
对上述代码稍加修改便可使用这个新的特性：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{template}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{k}{typename} \PYG{n+nc}{LayoutTile}\PYG{o}{\PYGZgt{}}
\PYG{n}{\PYGZus{}\PYGZus{}global\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{k+kt}{void}\PYG{+w}{ }\PYG{n}{gemmKernel}\PYG{p}{(}\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{\PYGZus{}\PYGZus{}restrict\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{A}\PYG{p}{,}
\PYG{+w}{                           }\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{\PYGZus{}\PYGZus{}restrict\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{B}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{\PYGZus{}\PYGZus{}restrict\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{C}\PYG{p}{,}
\PYG{+w}{                           }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{,}
\PYG{+w}{                           }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{kCount}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{n}{float4}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{k}{sizeof}\PYG{p}{(}\PYG{k+kt}{float}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{m}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{blockIdx}\PYG{p}{.}\PYG{n}{x}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{;}
\PYG{+w}{  }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{n}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{y}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{blockIdx}\PYG{p}{.}\PYG{n}{y}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{;}
\PYG{+w}{  }\PYG{c+c1}{// ...}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
同时在启动时使用新修改的模板函数来启动GPU核函数：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{using}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{Layout}\PYG{o}{\PYGZlt{}}\PYG{l+m+mi}{16}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{16}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{l+m+mi}{4}\PYG{o}{\PYGZgt{}}\PYG{p}{;}

\PYG{n}{gemmKernel}\PYG{o}{\PYGZlt{}}\PYG{n}{LayoutTile}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}}\PYG{n}{grid}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{block}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{n}{deviceAPtr}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{deviceBPtr}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{deviceCPtr}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{,}
\PYG{+w}{                             }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
完整代码见\sphinxhref{https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm\_use\_128.cu}{gemm\_use\_128.cu}%
\begin{footnote}[29]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm\_use\_128.cu}
%
\end{footnote}。


\subsection{测试及分析}
\label{\detokenize{chapter_accelerator/accelerator_practise:id6}}
\sphinxAtStartPar
测试得到以下结果：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Max} \PYG{n}{Error}\PYG{p}{:} \PYG{l+m+mf}{0.000092}
\PYG{n}{Average} \PYG{n}{Time}\PYG{p}{:} \PYG{l+m+mf}{6.232} \PYG{n}{ms}\PYG{p}{,} \PYG{n}{Average} \PYG{n}{Throughput}\PYG{p}{:} \PYG{l+m+mf}{1378.317} \PYG{n}{GFLOPS}
\end{sphinxVerbatim}

\sphinxAtStartPar
接下来我们使用分析工具Nsight Compute分析取得性能提升的具体原因。Nsight
Compute是英伟达发布的主要针对GPU核函数的性能分析工具，它通过劫持驱动的方式对GPU底层数据采样和输出。可以使用以下指令进行性能分析：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ncu \PYGZhy{}\PYGZhy{}set full \PYGZhy{}o \PYGZlt{}profile\PYGZus{}output\PYGZus{}file\PYGZgt{} \PYGZlt{}profile\PYGZus{}process\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}set full}} 代表采样所有数据， \sphinxcode{\sphinxupquote{\sphinxhyphen{}o}} 代表以文件的形式输出结果；
\sphinxcode{\sphinxupquote{<profile\_output\_file>}} 填输出文件名但注意不要加后缀名，
\sphinxcode{\sphinxupquote{<profile\_process>}} 填待分析的可执行文件及其参数。 比如我们需要分析
\sphinxcode{\sphinxupquote{first\_attempt}} ，将输出结果命名为 \sphinxcode{\sphinxupquote{first\_attepmt\_prof\_result}}
我们可以使用以下指令：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ncu}\PYG{+w}{ }\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{set}\PYG{+w}{ }\PYG{n}{full}\PYG{+w}{ }\PYG{o}{\PYGZhy{}}\PYG{n}{o}\PYG{+w}{ }\PYG{n}{first\PYGZus{}attepmt\PYGZus{}prof\PYGZus{}result}\PYG{+w}{ }\PYG{p}{.}\PYG{o}{/}\PYG{n}{first\PYGZus{}attempt}
\end{sphinxVerbatim}

\sphinxAtStartPar
如果提示权限不足可以使在指令前加\sphinxcode{\sphinxupquote{sudo}} 。
在得到输出文件之后，我们可以使用 \sphinxcode{\sphinxupquote{nv\sphinxhyphen{}nsight\sphinxhyphen{}cu}}
查看文件。我们对我们改动的GPU核函数与上一版本的GPU核函数进行对比分析，发现：

\sphinxAtStartPar
首先 \sphinxcode{\sphinxupquote{LDG}} 指令数量下降了84\%，且指标 \sphinxcode{\sphinxupquote{Stall LG Throttle}}
下降33\%，说明使用宽指令增加计算密度确实可以通过减少全局内存访问的指令数目而减少发射等待时间。最后指标
\sphinxcode{\sphinxupquote{Arithmetic Intensity}} 的提升也和我们之前的关于计算强度的分析相吻合。


\subsubsection{进一步提升计算强度}
\label{\detokenize{chapter_accelerator/accelerator_practise:id7}}
\sphinxAtStartPar
我们可以通过使每个线程负责处理更多的矩阵\(C\)中的数据块从而实现更高的计算强度，即如
\hyperref[\detokenize{chapter_accelerator/accelerator_practise:use-tile}]{图\ref{\detokenize{chapter_accelerator/accelerator_practise:use-tile}}} 右侧所示，使 \sphinxcode{\sphinxupquote{thread tile}}
扩大为4个\(4 \times 4\)矩阵的规模。我们对核函数进行以下修改，首先我们用\sphinxcode{\sphinxupquote{LayoutTile}}
来描述每个线程块处理数据 \sphinxcode{\sphinxupquote{tile}}的布局 ，其中 \sphinxcode{\sphinxupquote{LayoutTile::m}} 和
\sphinxcode{\sphinxupquote{LayoutTile::n}} 等于 \hyperref[\detokenize{chapter_accelerator/accelerator_practise:use-tile}]{图\ref{\detokenize{chapter_accelerator/accelerator_practise:use-tile}}}
左图中浅绿色矩阵块的高度和宽度， \sphinxcode{\sphinxupquote{LayoutTile::k}}
等于1；其次我们用\sphinxcode{\sphinxupquote{LayoutBlock}}
来描述一个线程块中线程的布局；同时我们用\sphinxcode{\sphinxupquote{LayoutThread}} 来描述
\sphinxcode{\sphinxupquote{thread tile}} 中子矩阵的布局 ，其中\sphinxcode{\sphinxupquote{LayoutThread::m}} 和
\sphinxcode{\sphinxupquote{LayoutThread::n}} 等于 \hyperref[\detokenize{chapter_accelerator/accelerator_practise:use-tile}]{图\ref{\detokenize{chapter_accelerator/accelerator_practise:use-tile}}}
右图中深绿色矩阵块的高度和宽度 。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{use_tile}.svg}
\caption{通过提高线程所处理矩阵块的数量来进一步提高计算强度}\label{\detokenize{chapter_accelerator/accelerator_practise:id26}}\label{\detokenize{chapter_accelerator/accelerator_practise:id8}}\label{\detokenize{chapter_accelerator/accelerator_practise:use-tile}}\end{figure}

\sphinxAtStartPar
首先修改核函数签名：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{template}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{k}{typename} \PYG{n+nc}{LayoutTile}\PYG{p}{,}\PYG{+w}{ }\PYG{k}{typename} \PYG{n+nc}{LayoutBlock}\PYG{p}{,}\PYG{+w}{ }\PYG{k}{typename} \PYG{n+nc}{LayoutThread}\PYG{o}{\PYGZgt{}}
\PYG{n}{\PYGZus{}\PYGZus{}global\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{k+kt}{void}\PYG{+w}{ }\PYG{n}{gemmKernel}\PYG{p}{(}\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{\PYGZus{}\PYGZus{}restrict\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{A}\PYG{p}{,}
\PYG{+w}{                          }\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{\PYGZus{}\PYGZus{}restrict\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{B}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{\PYGZus{}\PYGZus{}restrict\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{C}\PYG{p}{,}
\PYG{+w}{                          }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{,}
\PYG{+w}{                          }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{c+c1}{// ...}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
然后改写线程负责处理数据的偏移量，即图3左图中的行列偏移值 \sphinxcode{\sphinxupquote{m}} 和 \sphinxcode{\sphinxupquote{n}}
，其代码实现如下 ：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{m}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{blockIdx}\PYG{p}{.}\PYG{n}{x}\PYG{p}{;}
\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{n}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{y}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{blockIdx}\PYG{p}{.}\PYG{n}{y}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
由于每个线程从原来的处理一个数据块变为多个数据块，我们需要以下几个变量：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{itekCountnA}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{LayoutBlock}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{;}
\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{itekCountnB}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{LayoutBlock}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{p}{;}
\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{intervalA}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{itekCountnA}\PYG{p}{;}
\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{intervalB}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{itekCountnB}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{itekCountnA}} 是每个线程处理 \sphinxcode{\sphinxupquote{thread tile}}
在行方向上迭代的次数。\sphinxcode{\sphinxupquote{intervalA}} 是 \sphinxcode{\sphinxupquote{thread tile}}
子矩阵在行方向的间隔。同理 \sphinxcode{\sphinxupquote{itekCountnB}} 与 \sphinxcode{\sphinxupquote{intervalB}}
是在列方向上数据块的数量与数据块的间隔。 因为 \sphinxcode{\sphinxupquote{thread tile}}
扩大为若干个矩阵块，我们使用以下代码用来记录每个矩阵块是否越界：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kt}{bool}\PYG{+w}{ }\PYG{n}{validLoadTileA}\PYG{p}{[}\PYG{n}{itekCountnA}\PYG{p}{]}\PYG{p}{;}
\PYG{k+kt}{bool}\PYG{+w}{ }\PYG{n}{validLoadTileB}\PYG{p}{[}\PYG{n}{itekCountnB}\PYG{p}{]}\PYG{p}{;}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{itekCountnA}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{i}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n}{validLoadTileA}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{pA}\PYG{p}{.}\PYG{n}{validRowOffset}\PYG{p}{(}\PYG{n}{i}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{intervalA}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{itekCountnB}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{i}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n}{validLoadTileB}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{pB}\PYG{p}{.}\PYG{n}{validColOffset}\PYG{p}{(}\PYG{n}{i}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{intervalB}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
对于数据的读取和累加计算相应的需要增加循环：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{constexpr}\PYG{+w}{ }\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{float4Zero}\PYG{p}{\PYGZob{}}\PYG{l+m+mf}{0.f}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{0.f}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{0.f}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{0.f}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{k}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{k}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{k}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{  }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{iterA}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{iterA}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{itekCountnA}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{iterA}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{fragmentA}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{+w}{    }\PYG{n}{validLoadTileA}\PYG{p}{[}\PYG{n}{iterA}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{o}{=}\PYG{+w}{ }\PYG{n}{pA}\PYG{p}{.}\PYG{n}{validColOffset}\PYG{p}{(}\PYG{n}{k}\PYG{p}{)}\PYG{p}{;}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{    }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{i}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{      }\PYG{n}{fragmentA}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{validLoadTileA}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{?}\PYG{+w}{ }\PYG{n}{pA}\PYG{p}{(}\PYG{n}{i}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{iterA}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{intervalA}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{k}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{:}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{    }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{iterB}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{iterB}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{itekCountnB}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{iterB}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{      }\PYG{n}{validLoadTileB}\PYG{p}{[}\PYG{n}{iterB}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{o}{=}\PYG{+w}{ }\PYG{n}{pB}\PYG{p}{.}\PYG{n}{validRowOffset}\PYG{p}{(}\PYG{n}{k}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{      }\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{fragmentB}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{validLoadTileB}\PYG{p}{[}\PYG{n}{iterB}\PYG{p}{]}
\PYG{+w}{                             }\PYG{o}{?}\PYG{+w}{ }\PYG{n}{pB}\PYG{p}{(}\PYG{n}{k}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{iterB}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{intervalB}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{)}
\PYG{+w}{                             }\PYG{o}{:}\PYG{+w}{ }\PYG{n}{float4Zero}\PYG{p}{;}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{      }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{i}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{        }\PYG{n}{c}\PYG{p}{[}\PYG{n}{iterA}\PYG{p}{]}\PYG{p}{[}\PYG{n}{iterB}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{c}\PYG{p}{[}\PYG{n}{iterA}\PYG{p}{]}\PYG{p}{[}\PYG{n}{iterB}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{fragmentB}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{fragmentA}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{      }\PYG{p}{\PYGZcb{}}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
注意到我们此时使用了编译器指令 \sphinxcode{\sphinxupquote{\#pragma unroll}}
用于将循环展开，即如果循环次数是可以在编译时确定的话，编译器将会把带有判断和跳转的循环代码展开成串行代码。这样做的好处主要是减少了判断语句，此外还有利于编译器发现数据依赖从而更好地分配寄存器。缺点是可能会增加寄存器的使用，有潜在的降低GPU占用率的风险。
最后对于结果使用 \sphinxcode{\sphinxupquote{alpha}} 和 \sphinxcode{\sphinxupquote{beta}}
的放缩以及写回也相应的加上数据块的循环：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k}{auto}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{n+nl}{termA} \PYG{p}{:}\PYG{+w}{ }\PYG{n}{c}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{ }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k}{auto}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{n+nl}{termB} \PYG{p}{:}\PYG{+w}{ }\PYG{n}{termA}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{   }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k}{auto}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{n+nl}{term} \PYG{p}{:}\PYG{+w}{ }\PYG{n}{termB}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{     }\PYG{n}{term}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{term}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{;}
\PYG{+w}{   }\PYG{p}{\PYGZcb{}}
\PYG{+w}{ }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{iterA}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{iterA}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{itekCountnA}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{iterA}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{ }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{iterB}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{iterB}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{itekCountnB}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{iterB}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{   }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{i}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{     }\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{result}\PYG{p}{\PYGZob{}}\PYG{n}{c}\PYG{p}{[}\PYG{n}{iterA}\PYG{p}{]}\PYG{p}{[}\PYG{n}{iterB}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{+w}{     }\PYG{k}{if}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{beta}\PYG{+w}{ }\PYG{o}{!}\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{       }\PYG{n}{result}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{result}\PYG{+w}{ }\PYG{o}{+}
\PYG{+w}{                }\PYG{n}{pC}\PYG{p}{(}\PYG{n}{i}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{iterA}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{intervalA}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{iterB}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{intervalB}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{;}
\PYG{+w}{     }\PYG{p}{\PYGZcb{}}
\PYG{+w}{     }\PYG{n}{pC}\PYG{p}{(}\PYG{n}{i}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{iterA}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{intervalA}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{iterB}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{intervalB}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{result}\PYG{p}{;}
\PYG{+w}{   }\PYG{p}{\PYGZcb{}}
\PYG{+w}{ }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
完整代码见\sphinxhref{https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm\_use\_tile.cu}{gemm\_use\_tile.cu}%
\begin{footnote}[30]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm\_use\_tile.cu}
%
\end{footnote}。


\subsection{测试及分析}
\label{\detokenize{chapter_accelerator/accelerator_practise:id9}}
\sphinxAtStartPar
测试得到以下结果：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Max} \PYG{n}{Error}\PYG{p}{:} \PYG{l+m+mf}{0.000092}
\PYG{n}{Average} \PYG{n}{Time}\PYG{p}{:} \PYG{l+m+mf}{3.188} \PYG{n}{ms}\PYG{p}{,} \PYG{n}{Average} \PYG{n}{Throughput}\PYG{p}{:} \PYG{l+m+mf}{2694.440} \PYG{n}{GFLOPS}
\end{sphinxVerbatim}

\sphinxAtStartPar
使用Nsight Compute分析发现：类似地，本次优化在\sphinxcode{\sphinxupquote{Stall LG Throttle}}
等指标上取得了进一步的提升。


\subsubsection{使用共享内存缓存复用数据}
\label{\detokenize{chapter_accelerator/accelerator_practise:sec-accelerator-use-smem}}\label{\detokenize{chapter_accelerator/accelerator_practise:id10}}
\sphinxAtStartPar
虽然令一个线程一次读取更多的数据能取得计算强度的提升进而带来性能的提升，但是这种令单个线程处理数据增多的设计会导致开启总的线程数量减少，进而导致并行度下降，因此我们需要使用其他硬件特性在尽可能不影响并行度的前提下取得性能提升。在之前的代码中，我们开启若干个线程块，每个线程块处理矩阵\(C\)中的一个或多个矩阵块。在
\hyperref[\detokenize{chapter_accelerator/accelerator_practise:duplicated-data}]{图\ref{\detokenize{chapter_accelerator/accelerator_practise:duplicated-data}}}
中，我们可以观察到，处理矩阵\(C\)同一行的线程\(x, y\)会读取矩阵\(A\)中相同的数据，我们可以借助共享内存让同一个线程块中不同的线程读取不重复的数据而提升程序吞吐量。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{duplicated_data}.svg}
\caption{线程间重复读取数据}\label{\detokenize{chapter_accelerator/accelerator_practise:id27}}\label{\detokenize{chapter_accelerator/accelerator_practise:duplicated-data}}\end{figure}

\sphinxAtStartPar
具体地，我们需要对代码进行如下改造：首先此前代码在计算内积过程是进行\(K\)次循环读取数据并累加计算，在此设定下每次循环中处理矩阵\(C\)中相同行的线程会读取相同的矩阵\(A\)的数据，处理矩阵\(C\)中相同列的线程会读取相同的矩阵\(B\)的数据。我们可以通过将此\(K\)次循环拆解成两层循环，外层循环\(\frac{K}{tileK}\)次，每次外循环的迭代读取一整块数据，内层循环\(tileK\)次进行累加数据。直观来看，外层循环如
\hyperref[\detokenize{chapter_accelerator/accelerator_practise:use-smem-store}]{图\ref{\detokenize{chapter_accelerator/accelerator_practise:use-smem-store}}}
所示，每次循环将矩阵\(A\)和矩阵\(B\)中一整个 \sphinxcode{\sphinxupquote{tile}}
读取到共享内存中；内层循环如 \hyperref[\detokenize{chapter_accelerator/accelerator_practise:use-smem-load}]{图\ref{\detokenize{chapter_accelerator/accelerator_practise:use-smem-load}}}
所示，每次循环从共享内存读取数据并计算。这种设计带来的好处是，我们可以让每个线程不必独自从全局内存读取所有需要的数据，整个线程块将共同需要的数据从全局内存中读取并写入到共享内存中，此后每个线程在计算过程中只需要从共享内存中读取所需要的数据即可。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{use_smem_store}.svg}
\caption{向共享内存中写入数据}\label{\detokenize{chapter_accelerator/accelerator_practise:id28}}\label{\detokenize{chapter_accelerator/accelerator_practise:use-smem-store}}\end{figure}

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{use_smem_load}.svg}
\caption{从共享内存中读取数据}\label{\detokenize{chapter_accelerator/accelerator_practise:id29}}\label{\detokenize{chapter_accelerator/accelerator_practise:id11}}\label{\detokenize{chapter_accelerator/accelerator_practise:use-smem-load}}\end{figure}

\sphinxAtStartPar
下面我们将实现使用共享内存的GPU核函数。首先，我们定义每个线程块在外层循环的每次迭代中从矩阵\(A\)中读取大小为\(tileM \times tileK\)的数据块，在矩阵\(B\)中读取大小为\(tileK \times tileN\)的数据块。假设每个线程块中一共含有\(blockSize\)个线程，那么就可以使用这\(blockSize\)个线程，每个线程循环\(\frac{tileM * tileK}{blockSize * 4}\)次将矩阵\(A\)中的矩阵块
\sphinxcode{\sphinxupquote{tileA}}
读取进共享内存中，同理每个线程循环\(\frac{tileM * tileK}{blockSize * 4}\)次将矩阵\(B\)中的矩阵块
\sphinxcode{\sphinxupquote{tileB}} 读取进共享内存中。

\sphinxAtStartPar
首先需要定义若干变量：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{using}\PYG{+w}{ }\PYG{n}{LayoutTileT}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{     }\PYG{n}{Layout}\PYG{o}{\PYGZlt{}}\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{,}
\PYG{+w}{                               }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{o}{\PYGZgt{}}\PYG{p}{;}
\PYG{+w}{ }\PYG{k}{using}\PYG{+w}{ }\PYG{n}{LayoutThreadT}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{     }\PYG{n}{Layout}\PYG{o}{\PYGZlt{}}\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{o}{\PYGZgt{}}\PYG{p}{;}

\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{blockSize}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{LayoutBlock}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{LayoutBlock}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{p}{;}

\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{nInTileC}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{\PYGZpc{}}\PYG{+w}{ }\PYG{n}{LayoutBlock}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{;}
\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{mInTileC}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{LayoutBlock}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{;}

\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{tileSizeA}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{p}{;}
\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{tileIterationsA}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{tileSizeA}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{blockSize}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{;}
\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalA}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{blockSize}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{p}{;}
\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{tileComputeIterationsA}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{LayoutBlock}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{;}
\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{tileSharedIntervalA}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{tileComputeIterationsA}\PYG{p}{;}
\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{kInTileA}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{\PYGZpc{}}\PYG{+w}{ }\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{p}{;}
\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{mInTileA}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{p}{;}

\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{tileSizeB}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{p}{;}
\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{tileIterationsB}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{tileSizeB}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{blockSize}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{;}
\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalB}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{blockSize}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{p}{;}
\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{tileComputeIterationsB}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{LayoutBlock}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{p}{;}
\PYG{k}{constexpr}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{tileSharedIntervalBT}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{tileComputeIterationsB}\PYG{p}{;}
\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{nInTileB}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{\PYGZpc{}}\PYG{+w}{ }\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{p}{;}
\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{kinTileB}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{threadIdx}\PYG{p}{.}\PYG{n}{x}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
因为 \sphinxcode{\sphinxupquote{LayoutTile}} 与 \sphinxcode{\sphinxupquote{LayoutThread}} 是表示的 \sphinxcode{\sphinxupquote{float}}
数据的布局，我们有时将其看为 \sphinxcode{\sphinxupquote{float4}} 的数据储存，因此我们需要加入变量
\sphinxcode{\sphinxupquote{LayoutTileT}} 与 \sphinxcode{\sphinxupquote{LayoutThreadT}} 。 \sphinxcode{\sphinxupquote{blockSize}}
指一个线程块内的线程数量。
我们在此版本使用一维线程块的布局模拟二维布局，所以我们需要计算在二维布局下的坐标：用
\sphinxcode{\sphinxupquote{mInTileC}} 与 \sphinxcode{\sphinxupquote{nInTileC}} 表示在给定 \sphinxcode{\sphinxupquote{LayoutBlock}}
布局下的二维线程坐标。由于 \sphinxcode{\sphinxupquote{tileA}}
是\(tileM \times timeK\)的尺寸，因此我们可以确定其中数据数量\sphinxcode{\sphinxupquote{tileSizeA}}
，由于一个线程块内有 \sphinxcode{\sphinxupquote{blockSize}} 个线程且每个线程一次读取 \sphinxcode{\sphinxupquote{kCount}}
个 \sphinxcode{\sphinxupquote{float}} 数，因此整个 \sphinxcode{\sphinxupquote{tileA}} 需要用
\sphinxcode{\sphinxupquote{tileIterationsA = tileSizeA / blockSize / kCount}}
次读取。每个线程在最开始时负责读取的 \sphinxcode{\sphinxupquote{tileA}} 的位置使用变量
\sphinxcode{\sphinxupquote{kInTileA}} 和 \sphinxcode{\sphinxupquote{mInTileA}} 表示。因为需要用\sphinxcode{\sphinxupquote{tileIterationsA}}
次读取 \sphinxcode{\sphinxupquote{tileA}}
，每次向下滑动的距离我们使用变量\sphinxcode{\sphinxupquote{tileGlobalIntervalA}}表示。同时因为需要用每个线程需要处理
\sphinxcode{\sphinxupquote{thread tile}} 中多个子矩阵块，其中每个线程处理 \sphinxcode{\sphinxupquote{thread tile}}
时在行方向上迭代的次数 定义为\sphinxcode{\sphinxupquote{tileComputeIterationsA}}
。这些子矩阵块在 \sphinxcode{\sphinxupquote{m}} 方向的间隔我们用\sphinxcode{\sphinxupquote{tileSharedIntervalA}}
表示。类似地，我们定义与 \sphinxcode{\sphinxupquote{tileB}} 的若干变量。

\sphinxAtStartPar
此外我们需要声明共享内存 \sphinxcode{\sphinxupquote{tile}} 和从全局内存读取的数据 \sphinxcode{\sphinxupquote{buffer}} ：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{\PYGZus{}\PYGZus{}shared\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{tileA}\PYG{p}{[}\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{]}\PYG{p}{[}\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{p}{]}\PYG{p}{;}
\PYG{n}{\PYGZus{}\PYGZus{}shared\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{tileB}\PYG{p}{[}\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{p}{]}\PYG{p}{[}\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{p}{]}\PYG{p}{;}
\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{bufferA}\PYG{p}{[}\PYG{n}{tileIterationsA}\PYG{p}{]}\PYG{p}{;}
\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{bufferB}\PYG{p}{[}\PYG{n}{tileIterationsB}\PYG{p}{]}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
我们使用以下代码将数据从全局内存中读出：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{j}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{j}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileIterationsA}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{j}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{ }\PYG{n}{validLoadTileA}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{validLoadTileA}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{o}{\PYGZam{}}\PYG{+w}{ }\PYG{n}{pA}\PYG{p}{.}\PYG{n}{validColOffset}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{ }\PYG{n}{bufferA}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{     }\PYG{n}{validLoadTileA}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{?}\PYG{+w}{ }\PYG{n}{pA}\PYG{p}{(}\PYG{n}{j}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalA}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{:}\PYG{+w}{ }\PYG{n}{float4Zero}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{j}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{j}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileIterationsB}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{j}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{ }\PYG{n}{validLoadTileB}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{     }\PYG{n}{validLoadTileB}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{o}{\PYGZam{}}\PYG{+w}{ }\PYG{n}{pB}\PYG{p}{.}\PYG{n}{validRowOffset}\PYG{p}{(}\PYG{n}{j}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalB}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{ }\PYG{n}{bufferB}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{     }\PYG{n}{validLoadTileB}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{?}\PYG{+w}{ }\PYG{n}{pB}\PYG{p}{(}\PYG{n}{j}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalB}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{:}\PYG{+w}{ }\PYG{n}{float4Zero}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
从全局内存将数据读入 \sphinxcode{\sphinxupquote{buffer}} 之后我们使用以下代码将数据写入共享内存：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{\PYGZus{}\PYGZus{}syncthreads}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileIterationsA}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{a}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{ }\PYG{n}{tileA}\PYG{p}{[}\PYG{n}{mInTileA}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalA}\PYG{p}{]}\PYG{p}{[}\PYG{n}{kInTileA}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{bufferA}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileIterationsB}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{a}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{ }\PYG{n}{tileB}\PYG{p}{[}\PYG{n}{kinTileB}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalB}\PYG{p}{]}\PYG{p}{[}\PYG{n}{nInTileB}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{bufferB}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\PYG{n}{\PYGZus{}\PYGZus{}syncthreads}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
不要忘记写入前和写入后进行一次同步避免数据竞争。
此后我们使用以下代码执行内层循环：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{j}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{j}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{j}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{ }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileComputeIterationsA}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{a}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{   }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{b}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{b}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{b}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{     }\PYG{n}{fragmentA}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{p}{[}\PYG{n}{b}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{         }\PYG{n}{tileA}\PYG{p}{[}\PYG{n}{a}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileSharedIntervalA}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{mInTileC}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{b}\PYG{p}{]}
\PYG{+w}{              }\PYG{p}{[}\PYG{n}{j}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{+w}{ }\PYG{o}{\PYGZpc{}}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{   }\PYG{p}{\PYGZcb{}}
\PYG{+w}{ }\PYG{p}{\PYGZcb{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{ }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileComputeIterationsB}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{a}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{   }\PYG{n}{fragmentB}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{tileB}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileSharedIntervalBT}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{nInTileC}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{ }\PYG{p}{\PYGZcb{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{ }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{d}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{d}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileComputeIterationsA}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{d}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{   }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{e}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{e}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileComputeIterationsB}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{LayoutThreadT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{e}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{     }\PYG{n}{c}\PYG{p}{[}\PYG{n}{d}\PYG{p}{]}\PYG{p}{[}\PYG{n}{e}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{         }\PYG{n}{c}\PYG{p}{[}\PYG{n}{d}\PYG{p}{]}\PYG{p}{[}\PYG{n}{e}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{fragmentB}\PYG{p}{[}\PYG{n}{e}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{*}
\PYG{+w}{                       }\PYG{n}{fragmentA}\PYG{p}{[}\PYG{n}{d}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{]}\PYG{p}{[}\PYG{n}{d}\PYG{+w}{ }\PYG{o}{\PYGZpc{}}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{   }\PYG{p}{\PYGZcb{}}
\PYG{+w}{ }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
内层循环的流程包括从共享内存中读取数据到 \sphinxcode{\sphinxupquote{fragment}} ，使用
\sphinxcode{\sphinxupquote{fragment}} 的数据进行计算。
在内层循环结束后对全局内存增加偏移量后执行下一次外层循环：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pA}\PYG{p}{.}\PYG{n}{addOffset}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{pB}\PYG{p}{.}\PYG{n}{addOffset}\PYG{p}{(}\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
其他计算放缩等代码与上一个版本基本一致，写回代码如下：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileComputeIterationsA}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{i}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{ }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{a}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{   }\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{bool}\PYG{+w}{ }\PYG{n}{mValid}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{pC}\PYG{p}{.}\PYG{n}{validRowOffset}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}\PYG{p}{;}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{   }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{b}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{b}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileComputeIterationsB}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{b}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{     }\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{bool}\PYG{+w}{ }\PYG{n}{nValid}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{pC}\PYG{p}{.}\PYG{n}{validColOffset}\PYG{p}{(}\PYG{n}{b}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileSharedIntervalBT}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{     }\PYG{k}{if}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{mValid}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{o}{\PYGZam{}}\PYG{+w}{ }\PYG{n}{nValid}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{       }\PYG{n}{openmlsys}\PYG{o}{:}\PYG{o}{:}\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{result}\PYG{p}{\PYGZob{}}\PYG{n}{c}\PYG{p}{[}\PYG{n}{a}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{]}\PYG{p}{[}\PYG{n}{b}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{+w}{       }\PYG{k}{if}\PYG{+w}{ }\PYG{p}{(}\PYG{n}{beta}\PYG{+w}{ }\PYG{o}{!}\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{         }\PYG{n}{result}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{result}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{pC}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{b}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileSharedIntervalBT}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{;}
\PYG{+w}{       }\PYG{p}{\PYGZcb{}}
\PYG{+w}{       }\PYG{n}{pC}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{b}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileSharedIntervalBT}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{result}\PYG{p}{;}
\PYG{+w}{     }\PYG{p}{\PYGZcb{}}
\PYG{+w}{   }\PYG{p}{\PYGZcb{}}
\PYG{+w}{ }\PYG{p}{\PYGZcb{}}
\PYG{+w}{ }\PYG{n}{pC}\PYG{p}{.}\PYG{n}{addOffset}\PYG{p}{(}\PYG{n}{tileSharedIntervalA}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
完整代码见\sphinxhref{https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm\_use\_smem.cu}{gemm\_use\_smem.cu}%
\begin{footnote}[31]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm\_use\_smem.cu}
%
\end{footnote}。


\subsection{测试及分析}
\label{\detokenize{chapter_accelerator/accelerator_practise:id12}}
\sphinxAtStartPar
测试得到以下结果：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Max} \PYG{n}{Error}\PYG{p}{:} \PYG{l+m+mf}{0.000092}
\PYG{n}{Average} \PYG{n}{Time}\PYG{p}{:} \PYG{l+m+mf}{0.617} \PYG{n}{ms}\PYG{p}{,} \PYG{n}{Average} \PYG{n}{Throughput}\PYG{p}{:} \PYG{l+m+mf}{13925.168} \PYG{n}{GFLOPS}
\end{sphinxVerbatim}

\sphinxAtStartPar
我们使用Nsight
Compute对核函数分析并与上一个核函数进行对比，我们观察到主要的变化有：首先
\sphinxcode{\sphinxupquote{LDG}} 指令数量下降了97\%，与我们的此前设计相吻合。同时观察到
\sphinxcode{\sphinxupquote{SM Utilization}}
提升了218\%也可以侧面证实我们使用共享内存减少了内存访问延迟从而提升了利用率，此外我们观察到各项指标如
\sphinxcode{\sphinxupquote{Pipe Fma Cycles Active}}
等都有显著提升，这都能充分解释了我们使用共享内存的改进是合理且有效的。


\subsubsection{减少寄存器使用}
\label{\detokenize{chapter_accelerator/accelerator_practise:id13}}\label{\detokenize{chapter_accelerator/accelerator_practise:id14}}
\sphinxAtStartPar
我们注意到在我们向共享内存中存储矩阵\(A\)的数据块是按照行优先的数据排布进行的，而我们对此共享内存的读取是按列逐行读取的。我们可以将矩阵\(A\)的数据块在共享内存中数据按照列优先的形式排布，这样我们可以减少循环及循环变量从而带来寄存器使用数量减少进而带来性能提升。

\sphinxAtStartPar
需要对代码做如下修改，首先将 \sphinxcode{\sphinxupquote{tileA}} 修改为列优先矩阵：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{\PYGZus{}\PYGZus{}shared\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{tileA}\PYG{p}{[}\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{p}{]}\PYG{p}{[}\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{]}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
其次需要将写入 \sphinxcode{\sphinxupquote{tileA}} 的过程按照列优先调整：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{ }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileIterationsA}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{a}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{   }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{j}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{j}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{j}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{     }\PYG{n}{tileA}\PYG{p}{[}\PYG{n}{kInTileA}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{kCount}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{j}\PYG{p}{]}
\PYG{+w}{          }\PYG{p}{[}\PYG{p}{(}\PYG{n}{a}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalA}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{mInTileA}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{]}
\PYG{+w}{          }\PYG{p}{[}\PYG{p}{(}\PYG{n}{a}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalA}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{mInTileA}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{\PYGZpc{}}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{bufferA}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{   }\PYG{p}{\PYGZcb{}}
\PYG{+w}{ }\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
最后修改从 \sphinxcode{\sphinxupquote{tileA}} 读取的过程：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{ }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileComputeIterationsA}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{a}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{   }\PYG{n}{fragmentA}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{tileA}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileSharedIntervalAT}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{mInTileC}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{ }\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
完整代码见\sphinxhref{https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm\_transpose\_smem.cu}{gemm\_transpose\_smem.cu}%
\begin{footnote}[32]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm\_transpose\_smem.cu}
%
\end{footnote}。


\subsection{测试及分析}
\label{\detokenize{chapter_accelerator/accelerator_practise:id15}}
\sphinxAtStartPar
测试得到以下结果：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Max} \PYG{n}{Error}\PYG{p}{:} \PYG{l+m+mf}{0.000092}
\PYG{n}{Average} \PYG{n}{Time}\PYG{p}{:} \PYG{l+m+mf}{0.610} \PYG{n}{ms}\PYG{p}{,} \PYG{n}{Average} \PYG{n}{Throughput}\PYG{p}{:} \PYG{l+m+mf}{14083.116} \PYG{n}{GFLOPS}
\end{sphinxVerbatim}

\sphinxAtStartPar
使用Nsight Compute分析有以下观察发现主要的变化： \sphinxcode{\sphinxupquote{Occupancy}}
提升1.3\%，而带来此提升的原因是寄存器使用111个，相比上一个GPU核函数使用128个寄存器减少了17个，从而带来了性能提升。但这个变化会因为GPU架构不同导致有不同的变化，同时我们观察到
\sphinxcode{\sphinxupquote{STS}} 指令数量提升且带来一些 \sphinxcode{\sphinxupquote{bank confilct}}
，因此在其他GPU架构上此改动可能不会带来正面影响。


\subsubsection{隐藏共享内存读取延迟}
\label{\detokenize{chapter_accelerator/accelerator_practise:id16}}
\sphinxAtStartPar
在GPU中使用指令 \sphinxcode{\sphinxupquote{LDS}}
读取共享内存中的数据，在这条指令发出后并不会等待数据读取到寄存器后再执行下一条语句，只有执行到依赖
\sphinxcode{\sphinxupquote{LDS}}
指令读取的数据的指令时才会等待读取的完成。而在上一小节中，我们在内层\(tileK\)次循环中，每次发射完读取共享内存的指令之后就会立即执行依赖于读取数据的数学运算，这样就会导致计算单元等待数据从共享内存的读取，如
\hyperref[\detokenize{chapter_accelerator/accelerator_practise:use-smem-pipeline}]{图\ref{\detokenize{chapter_accelerator/accelerator_practise:use-smem-pipeline}}}
所示。事实上，对共享内存的访问周期能多达几十个时钟周期，而计算指令的执行往往只有几个时钟周期，因此通过一定方式隐藏对共享内存的访问会取得不小的收益。我们可以重新优化流水线隐藏一定的数据读取延迟。具体地，我们可以在内层的\(tileK\)次循环中每次循环开始时读取发射下一次内层循环数据的读取指令。由于在执行本次运算时计算指令并不依赖于下一次循环的数据，因此计算过程不会等待之前发出的读取下一次内层循环数据的指令，具体见
\hyperref[\detokenize{chapter_accelerator/accelerator_practise:hide-smem-latency}]{图\ref{\detokenize{chapter_accelerator/accelerator_practise:hide-smem-latency}}} 。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{use_smem_pipeline}.svg}
\caption{上一个GPU核函数的流水线}\label{\detokenize{chapter_accelerator/accelerator_practise:id30}}\label{\detokenize{chapter_accelerator/accelerator_practise:use-smem-pipeline}}\end{figure}

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{hide_smem_latency}.svg}
\caption{隐藏共享内存读取延迟的流水线}\label{\detokenize{chapter_accelerator/accelerator_practise:id31}}\label{\detokenize{chapter_accelerator/accelerator_practise:id17}}\label{\detokenize{chapter_accelerator/accelerator_practise:hide-smem-latency}}\end{figure}

\sphinxAtStartPar
我们对代码需要做如下修改，首先需要将\sphinxcode{\sphinxupquote{fragment}}
的数量加倍用于存储下一次内循环读取的数据：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{fragmentA}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{n}{tileComputeIterationsA}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{LayoutThreadT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{]}\PYG{p}{;}
\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{fragmentB}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{n}{tileComputeIterationsB}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{LayoutThreadT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{p}{]}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
其后要在内层循环开始前从 \sphinxcode{\sphinxupquote{tile}} 中向 \sphinxcode{\sphinxupquote{fragment}} 传输数据：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileComputeIterationsA}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{a}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n}{fragmentA}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{tileA}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileSharedIntervalAT}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{mInTileC}\PYG{p}{]}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileComputeIterationsB}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{a}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n}{fragmentB}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{tileB}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileSharedIntervalBT}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{nInTileC}\PYG{p}{]}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
同时在内层循环每次迭代的开始时读取下一次内层循环需要的 \sphinxcode{\sphinxupquote{tile}}
中的数据：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileComputeIterationsA}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{a}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n}{fragmentA}\PYG{p}{[}\PYG{p}{(}\PYG{n}{j}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{\PYGZpc{}}\PYG{+w}{ }\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{      }\PYG{n}{tileA}\PYG{p}{[}\PYG{n}{j}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileSharedIntervalAT}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{mInTileC}\PYG{p}{]}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileComputeIterationsB}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{a}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n}{fragmentB}\PYG{p}{[}\PYG{p}{(}\PYG{n}{j}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{\PYGZpc{}}\PYG{+w}{ }\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{      }\PYG{n}{tileB}\PYG{p}{[}\PYG{n}{j}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileSharedIntervalBT}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{nInTileC}\PYG{p}{]}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
其中 \sphinxcode{\sphinxupquote{j}} 为内存循环的次数。 最后我们修改计算过程的代码 ：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{d}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{d}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileComputeIterationsA}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{d}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{  }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{e}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{e}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileComputeIterationsA}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{LayoutThreadT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{e}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n}{c}\PYG{p}{[}\PYG{n}{d}\PYG{p}{]}\PYG{p}{[}\PYG{n}{e}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{        }\PYG{n}{c}\PYG{p}{[}\PYG{n}{d}\PYG{p}{]}\PYG{p}{[}\PYG{n}{e}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{+}
\PYG{+w}{        }\PYG{n}{fragmentB}\PYG{p}{[}\PYG{n}{j}\PYG{+w}{ }\PYG{o}{\PYGZpc{}}\PYG{+w}{ }\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{n}{e}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{*}
\PYG{+w}{            }\PYG{n}{fragmentA}\PYG{p}{[}\PYG{n}{j}\PYG{+w}{ }\PYG{o}{\PYGZpc{}}\PYG{+w}{ }\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{n}{d}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{]}\PYG{p}{[}\PYG{n}{d}\PYG{+w}{ }\PYG{o}{\PYGZpc{}}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
其中 \sphinxcode{\sphinxupquote{j}} 为内层循环的次数。
完整代码见\sphinxhref{https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm\_hide\_smem\_latency.cu}{gemm\_hide\_smem\_latency.cu}%
\begin{footnote}[33]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm\_hide\_smem\_latency.cu}
%
\end{footnote}。


\subsection{测试及分析}
\label{\detokenize{chapter_accelerator/accelerator_practise:id18}}
\sphinxAtStartPar
测试得到以下结果：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Max} \PYG{n}{Error}\PYG{p}{:} \PYG{l+m+mf}{0.000092}
\PYG{n}{Average} \PYG{n}{Time}\PYG{p}{:} \PYG{l+m+mf}{0.585} \PYG{n}{ms}\PYG{p}{,} \PYG{n}{Average} \PYG{n}{Throughput}\PYG{p}{:} \PYG{l+m+mf}{14686.179} \PYG{n}{GFLOPS}
\end{sphinxVerbatim}

\sphinxAtStartPar
使用Nsight Compute观察发现：相比上一个GPU核函数，指标
\sphinxcode{\sphinxupquote{Stall Short Scoreboard}}
减少了67\%。而此前提过GPU内存读写指令发出后并不会等待数据读取到寄存器后再执行下一条语句，但是会在Scoreboard设置符号并在完成读取后置回符号，等到之后有数据依赖的指令执行前会等待Scoreboard中符号的置回。所以这里\sphinxcode{\sphinxupquote{Stall Short Scoreboard}}
的减少充分说明了内存延迟是有效的。


\subsubsection{隐藏全局内存读取延迟}
\label{\detokenize{chapter_accelerator/accelerator_practise:id19}}
\sphinxAtStartPar
上一小节中我们介绍了对共享内存读取流水线优化的方法，事实上，GPU再读取全局内存中使用的指令
\sphinxcode{\sphinxupquote{LDG}} 也有与共享内存读取指令 \sphinxcode{\sphinxupquote{LDS}}
类似的行为特性。因此我们类似的在\(\frac{K}{tileK}\)次外层循环中每次循环开始时发出下一次外层循环需要的矩阵\(A\)中的数据块的读取指令，而本次外循环的整个内层循环过程中不依赖下一次外循环的数据，因此本次外循环的内循环过程中不会等待对下一次外层循环需要的矩阵\(A\)中的数据块的读取指令完成，从而实现隐藏全局内存读取延迟的目的。具体流水线可视化见
\hyperref[\detokenize{chapter_accelerator/accelerator_practise:hide-global-latency}]{图\ref{\detokenize{chapter_accelerator/accelerator_practise:hide-global-latency}}} 。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{hide_global_latency}.svg}
\caption{隐藏全局内存读取延迟的流水线}\label{\detokenize{chapter_accelerator/accelerator_practise:id32}}\label{\detokenize{chapter_accelerator/accelerator_practise:id20}}\label{\detokenize{chapter_accelerator/accelerator_practise:hide-global-latency}}\end{figure}

\sphinxAtStartPar
我们将对代码进行以下修改，首先需要将 \sphinxcode{\sphinxupquote{tile}} 加倍并加入一个决定向哪个
\sphinxcode{\sphinxupquote{tile}} 写入的符号 \sphinxcode{\sphinxupquote{writeStageIdx}} ：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{\PYGZus{}\PYGZus{}shared\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{tileA}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{p}{]}\PYG{p}{[}\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{]}\PYG{p}{;}
\PYG{n}{\PYGZus{}\PYGZus{}shared\PYGZus{}\PYGZus{}}\PYG{+w}{ }\PYG{n}{float4}\PYG{+w}{ }\PYG{n}{tileB}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{p}{]}\PYG{p}{[}\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{n}\PYG{p}{]}\PYG{p}{;}
\PYG{k+kt}{bool}\PYG{+w}{ }\PYG{n}{writeStageIdx}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nb}{false}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
紧接着我们将从 \sphinxcode{\sphinxupquote{buffer}} 向 \sphinxcode{\sphinxupquote{tile}} 写入的过程相应的依照加倍后的
\sphinxcode{\sphinxupquote{tile}} 修改 ：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileIterationsA}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{i}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{  }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{j}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{j}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{j}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n}{tileA}\PYG{p}{[}\PYG{n}{writeStageIdx}\PYG{p}{]}\PYG{p}{[}\PYG{n}{kInTileA}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{kCount}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{j}\PYG{p}{]}
\PYG{+w}{         }\PYG{p}{[}\PYG{p}{(}\PYG{n}{i}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalA}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{mInTileA}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{]}
\PYG{+w}{         }\PYG{p}{[}\PYG{p}{(}\PYG{n}{i}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalA}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{mInTileA}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{\PYGZpc{}}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{bufferA}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileIterationsB}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{i}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n}{tileB}\PYG{p}{[}\PYG{n}{writeStageIdx}\PYG{p}{]}\PYG{p}{[}\PYG{n}{kinTileB}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalB}\PYG{p}{]}\PYG{p}{[}\PYG{n}{nInTileB}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{      }\PYG{n}{bufferB}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
其后相应修改从 \sphinxcode{\sphinxupquote{tile}} 向 \sphinxcode{\sphinxupquote{fragment}} 读取数据的相关代码，并将符号
\sphinxcode{\sphinxupquote{writeStageIdx}} 翻转：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileComputeIterationsA}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{i}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n}{fragmentA}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{      }\PYG{n}{tileA}\PYG{p}{[}\PYG{n}{writeStageIdx}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileSharedIntervalAT}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{mInTileC}\PYG{p}{]}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{i}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileComputeIterationsB}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{i}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n}{fragmentB}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{      }\PYG{n}{tileB}\PYG{p}{[}\PYG{n}{writeStageIdx}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileSharedIntervalBT}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{nInTileC}\PYG{p}{]}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\PYG{n}{writeStageIdx}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{o}{!}\PYG{n}{writeStageIdx}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
接下来我们在每次外层循环开始时从全局内存读取下一次计算需要的 \sphinxcode{\sphinxupquote{buffer}}
：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tensorA}\PYG{p}{.}\PYG{n}{addOffset}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{LayoutTileT}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{tensorB}\PYG{p}{.}\PYG{n}{addOffset}\PYG{p}{(}\PYG{n}{LayoutTile}\PYG{o}{:}\PYG{o}{:}\PYG{n}{k}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{;}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{j}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{j}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileIterationsA}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{j}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n}{validLoadTileA}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{validLoadTileA}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{o}{\PYGZam{}}\PYG{+w}{ }\PYG{n}{tensorA}\PYG{p}{.}\PYG{n}{validColOffset}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{bufferA}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{      }\PYG{n}{validLoadTileA}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{?}\PYG{+w}{ }\PYG{n}{tensorA}\PYG{p}{(}\PYG{n}{j}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalA}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{:}\PYG{+w}{ }\PYG{n}{float4Zero}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{j}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{j}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileIterationsB}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{j}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n}{validLoadTileB}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{      }\PYG{n}{validLoadTileB}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{o}{\PYGZam{}}\PYG{+w}{ }\PYG{n}{tensorB}\PYG{p}{.}\PYG{n}{validRowOffset}\PYG{p}{(}\PYG{n}{j}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalB}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{bufferB}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{      }\PYG{n}{validLoadTileB}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{?}\PYG{+w}{ }\PYG{n}{tensorB}\PYG{p}{(}\PYG{n}{j}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalB}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{:}\PYG{+w}{ }\PYG{n}{float4Zero}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
最后我们在内层循环结束后将预先读取的 \sphinxcode{\sphinxupquote{buffer}} 写入到 \sphinxcode{\sphinxupquote{tile}}
中并翻转符号位 \sphinxcode{\sphinxupquote{writeStageIdx}} ：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{d}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{d}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileIterationsA}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{d}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{+w}{  }\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{e}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{e}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{LayoutThread}\PYG{o}{:}\PYG{o}{:}\PYG{n}{m}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{e}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n}{tileA}\PYG{p}{[}\PYG{n}{writeStageIdx}\PYG{p}{]}\PYG{p}{[}\PYG{n}{kInTileA}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{kCount}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{e}\PYG{p}{]}
\PYG{+w}{         }\PYG{p}{[}\PYG{p}{(}\PYG{n}{d}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalA}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{mInTileA}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{]}
\PYG{+w}{         }\PYG{p}{[}\PYG{p}{(}\PYG{n}{d}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalA}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{mInTileA}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{\PYGZpc{}}\PYG{+w}{ }\PYG{n}{kCount}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{bufferA}\PYG{p}{[}\PYG{n}{d}\PYG{p}{]}\PYG{p}{[}\PYG{n}{e}\PYG{p}{]}\PYG{p}{;}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma unroll}
\PYG{k}{for}\PYG{+w}{ }\PYG{p}{(}\PYG{k+kt}{unsigned}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{;}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{\PYGZlt{}}\PYG{+w}{ }\PYG{n}{tileIterationsB}\PYG{p}{;}\PYG{+w}{ }\PYG{o}{+}\PYG{o}{+}\PYG{n}{a}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n}{tileB}\PYG{p}{[}\PYG{n}{writeStageIdx}\PYG{p}{]}\PYG{p}{[}\PYG{n}{kinTileB}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{n}{a}\PYG{+w}{ }\PYG{o}{*}\PYG{+w}{ }\PYG{n}{tileGlobalIntervalB}\PYG{p}{]}\PYG{p}{[}\PYG{n}{nInTileB}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{      }\PYG{n}{bufferB}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\PYG{n}{writeStageIdx}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{o}{!}\PYG{n}{writeStageIdx}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
事实上，我们可以让内层循环先执行\(tileK - 1\)次，在最后一次执行前将
\sphinxcode{\sphinxupquote{buffer}} 中的数据写入 \sphinxcode{\sphinxupquote{tile}}
，其后再执行内层循环的最后一次迭代，这样能更进一步隐藏向 \sphinxcode{\sphinxupquote{tile}}
写入的内存延迟。

\sphinxAtStartPar
完整代码见\sphinxhref{https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm\_final.cu}{gemm\_final.cu}%
\begin{footnote}[34]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm\_final.cu}
%
\end{footnote}。


\subsection{测试及分析}
\label{\detokenize{chapter_accelerator/accelerator_practise:id21}}
\sphinxAtStartPar
测试得到以下结果：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Max} \PYG{n}{Error}\PYG{p}{:} \PYG{l+m+mf}{0.000092}
\PYG{n}{Average} \PYG{n}{Time}\PYG{p}{:} \PYG{l+m+mf}{0.542} \PYG{n}{ms}\PYG{p}{,} \PYG{n}{Average} \PYG{n}{Throughput}\PYG{p}{:} \PYG{l+m+mf}{15838.302} \PYG{n}{GFLOPS}
\end{sphinxVerbatim}

\sphinxAtStartPar
使用Nsight Compute分析我们观察到指标 \sphinxcode{\sphinxupquote{Stall Long Scoreboard}}
减少了67\%，与上一小结的 \sphinxcode{\sphinxupquote{Stall Short Scoreboard}}
概念相对应，\sphinxcode{\sphinxupquote{Stall Long Scoreboard}}
主要是针对全局内存的指标。该指标的显著减少充分说明我们可以在一定程度上隐藏全局内存的读取。


\subsubsection{与cuBLAS对比}
\label{\detokenize{chapter_accelerator/accelerator_practise:cublas}}\label{\detokenize{chapter_accelerator/accelerator_practise:id22}}
\sphinxAtStartPar
前一节中介绍了cuBLAS的接口，我们可以很容易地写出以下代码使用cuBLAS完成矩阵乘法：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kt}{void}\PYG{+w}{ }\PYG{n+nf}{cublasGemm}\PYG{p}{(}\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{A}\PYG{p}{,}\PYG{+w}{ }\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{B}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{C}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{alf}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{n}{bet}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{,}\PYG{+w}{ }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{k+kt}{int}\PYG{+w}{ }\PYG{n}{lda}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{ldb}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{ldc}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{;}
\PYG{+w}{  }\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{alpha}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{n}{alf}\PYG{p}{;}
\PYG{+w}{  }\PYG{k}{const}\PYG{+w}{ }\PYG{k+kt}{float}\PYG{+w}{ }\PYG{o}{*}\PYG{n}{beta}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{o}{\PYGZam{}}\PYG{n}{bet}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cublasHandle\PYGZus{}t}\PYG{+w}{ }\PYG{n}{handle}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cublasCreate}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{handle}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cublasSgemm}\PYG{p}{(}\PYG{n}{handle}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{CUBLAS\PYGZus{}OP\PYGZus{}N}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{CUBLAS\PYGZus{}OP\PYGZus{}N}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{N}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{M}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{K}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{alpha}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{B}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{lda}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{A}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{ldb}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{beta}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{C}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{ldc}\PYG{p}{)}\PYG{p}{;}
\PYG{+w}{  }\PYG{n}{cublasDestroy}\PYG{p}{(}\PYG{n}{handle}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
需要注意的是cuBLAS默认矩阵在GPU中是按列优先存储的，而我们的矩阵是按行优先存储的，而两者可以通过转置相互转换，所以\(A\times B = (B^T\times A^T)^T\)，因此在输入时需要调整矩阵的顺序，即可保证输出结果仍是行优先矩阵。


\subsection{测试及分析}
\label{\detokenize{chapter_accelerator/accelerator_practise:id23}}
\sphinxAtStartPar
测试得到以下结果：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Max} \PYG{n}{Error}\PYG{p}{:} \PYG{l+m+mf}{0.000092}
\PYG{n}{Average} \PYG{n}{Time}\PYG{p}{:} \PYG{l+m+mf}{0.613} \PYG{n}{ms}\PYG{p}{,} \PYG{n}{Throughput}\PYG{p}{:} \PYG{l+m+mf}{14002.600} \PYG{n}{GFLOPS}
\end{sphinxVerbatim}

\sphinxAtStartPar
使用Nsight Compute分析发现 \sphinxcode{\sphinxupquote{LDG}} 和 \sphinxcode{\sphinxupquote{STS}}
等指令使用较多，导致指令发射压力较大，具体体现在 \sphinxcode{\sphinxupquote{Stall Wait}} 与
\sphinxcode{\sphinxupquote{Stall Dispatch Stall}} 指标相比我们较差。但其他指标诸如
\sphinxcode{\sphinxupquote{Stall Long Scoreboard}} 等优于我们，但总体上我们略胜一筹。
尽管我们的代码相比cuBLAS已经取得了一定的性能提升，但是需要强调的是cuBLAS内部为各种不同的矩阵尺寸以及不同的设备实现了若干不同的GPU核函数，我们实现的核函数在其他尺寸或其他设备设备上性能可能无法取得此加速比。


\section{总结}
\label{\detokenize{chapter_accelerator/summary:id1}}\label{\detokenize{chapter_accelerator/summary::doc}}\begin{itemize}
\item {} 
\sphinxAtStartPar
面向深度学习计算任务，加速器通常都是由多种片上缓存以及多种运算单元组成来提升性能。

\item {} 
\sphinxAtStartPar
未来性能增长需要依赖架构上的改变，即需要利用可编程的硬件加速器来实现性能突破。

\item {} 
\sphinxAtStartPar
出于计算效率和易用性等原因，加速器一般会具有多个等级的编程方式，包括：算子库层级，编程原语层级和指令层级。

\item {} 
\sphinxAtStartPar
越底层的编程方式越能够灵活地控制加速器，但同时对程序员的能力要求也越高。

\end{itemize}


\section{扩展阅读}
\label{\detokenize{chapter_accelerator/summary:id2}}\begin{itemize}
\item {} 
\sphinxAtStartPar
CUDA编程指导
\sphinxhref{https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html}{CUDA}%
\begin{footnote}[35]\sphinxAtStartFootnote
\sphinxnolinkurl{https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
昇腾社区 \sphinxhref{https://gitee.com/ascend}{Ascend}%
\begin{footnote}[36]\sphinxAtStartFootnote
\sphinxnolinkurl{https://gitee.com/ascend}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
MLIR应用进展 \sphinxhref{https://mlir.llvm.org/talks}{MLIR}%
\begin{footnote}[37]\sphinxAtStartFootnote
\sphinxnolinkurl{https://mlir.llvm.org/talks}
%
\end{footnote}

\end{itemize}


\section{参考文献}
\label{\detokenize{chapter_accelerator/summary:id3}}
\sphinxAtStartPar



\chapter{数据处理框架}
\label{\detokenize{chapter_data_processing/index:id1}}\label{\detokenize{chapter_data_processing/index::doc}}
\sphinxAtStartPar
在前两个章节中，我们介绍了编译器前后端的相关内容，详细地阐述了源程序到目标程序的转换优化过程。除了让芯片在训练/推理过程中高性能地运行，我们还需要将数据高效地发送给芯片，以实现全流程的性能最优。机器学习模型训练和推理需要从存储设备（如本地磁盘和内存、远端的存储系统等）中加载数据集，对数据集进行一系列处理变换，将处理结果发送到GPU或者华为昇腾Ascend等加速器中完成模型计算，该流程的任何一个步骤出现性能问题都会对训练和推理的吞吐率造成负面影响。本章我们将核心介绍如何设计、并实现一个面向机器学习场景的数据系统，以帮助用户轻松构建各种复杂的数据处理流水线(Data
Pipeline)，同时我们的数据系统要有足够高的执行性能，以确保数据预处理步骤不会成为模型训练和推理的性能瓶颈。

\sphinxAtStartPar
本章主要从易用性、高效性和保序性三个维度展开介绍机器学习系统中的数据模块。在前两个小节中，我们首先讨论如何构建一个易用的数据模块。包括如何设计编程抽象，使得用户通过短短几行代码便可以描述一个复杂的预处理过程；以及如何做到既内置丰富算子提升易用性，又可以灵活支持用户使用自定义算子覆盖长尾需求。用户构建好数据处理流程后，数据模块需要负责高效的调度执行数据流水线，以达到最优的数据处理吞吐率。高效的执行数据流水线是一个具有挑战性的任务，我们既要面临数据读取部分的I/O性能问题，又要解决数据处理部分的计算性能问题。针对上述挑战，我们将分别介绍面向高吞吐率读取性能的数据文件格式设计，以及能够充分发挥多核CPU算力的并行架构设计。不仅如此，和常规数据并行计算任务不同的是，大部分机器学习场景对于数据的输入输出顺序有着特殊的\sphinxcode{\sphinxupquote{保序性}}的要求，我们将会使用一节的内容来介绍什么是保序性，以及如何在数据模块的并行架构中设计相应组件计来满足该特性需求。学习了上述的内容后，读者将会对如何构建一个面向机器学习场景高效易用的数据模块有深刻的理解。最后，作为拓展内容，我们将以目前学术界和业界的一些实践经验来介绍当单机处理性能达不到要求时，该如何去扩展我们的数据处理模块以满足训练性能需求。本章学习目标包括：
\begin{itemize}
\item {} 
\sphinxAtStartPar
了解机器学习数据模块架构中的关键组件及其功能

\item {} 
\sphinxAtStartPar
了解不同数据模块用户编程接口的设计

\item {} 
\sphinxAtStartPar
掌握面向高性能数据读取的数据文件格式设计

\item {} 
\sphinxAtStartPar
掌握机器学习系统数据模块并行架构

\item {} 
\sphinxAtStartPar
掌握机器学习系统数据模块数据保序性含义及其解决方案

\item {} 
\sphinxAtStartPar
了解两种单机数据处理性能扩展方案

\end{itemize}


\section{概述}
\label{\detokenize{chapter_data_processing/requirements:id1}}\label{\detokenize{chapter_data_processing/requirements::doc}}
\sphinxAtStartPar
机器学习场景中的数据处理是一个典型的ETL（Extract, Transform,
Load）过程，第一个阶段（Extract）需要从存储设备中加载数据集，第二个阶段（Transform）完成对数据集的变换处理。虽然不同的机器学习系统在构建数据模块时采用了不同的技术方案，但其核心都会包含数据加载、数据混洗、数据变换、数据mini\sphinxhyphen{}batch组装以及数据发送等关键组件。其中每个组件的功能介绍如下所示：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{数据加载组件(Load)}：负责从存储设备中加载读取数据集，需要同时考虑存储设备的多样性（如本地磁盘/内存，远端磁盘和内存等）和数据集格式的多样性（如csv格式，txt格式等）。根据机器学习任务的特点，AI框架也提出了统一的数据存储格式（如谷歌TFRecord,
华为MindRecord等）以提供更高性能的数据读取。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{数据混洗组件(Shuffle)}：负责将输入数据的顺序按照用户指定方式随机打乱，以提升模型的鲁棒性。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{数据变换组件(Map)}：负责完成数据的变换处理，内置面向各种数据类型的常见预处理算子，如图像中的尺寸缩放和翻转，音频中的随机加噪和变调、文本处理中的停词去除和随机遮盖(Mask)等。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{数据组装组件(Batch)}：负责组装构造一个批次(mini\sphinxhyphen{}batch)的数据发送给训练/推理。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{数据发送组件(Send)}：负责将处理后的数据发送到GPU/华为昇腾Ascend等加速器中以进行后续的模型计算和更新。高性能的数据模块往往选择将数据向设备的搬运与加速器中的计算异步执行，以提升整个训练的吞吐率。

\end{itemize}

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{pipeline}.png}
\caption{数据模块的核心组件}\label{\detokenize{chapter_data_processing/requirements:id5}}\label{\detokenize{chapter_data_processing/requirements:pipeline}}\end{figure}

\sphinxAtStartPar
实现上述的组件只是数据模块的基础，我们还要对如下方面进行重点设计：


\subsection{易用性}
\label{\detokenize{chapter_data_processing/requirements:id2}}
\sphinxAtStartPar
AI模型训练/推理过程中涉及到的数据处理非常灵活：一方面，不同的应用场景中数据集类型千差万别，特点各异，在加载数据集时，数据模块要支持图像、文本、音频、视频等多种类型的特定存储格式，还要支持内存、本地磁盘、分布式文件系统以及对象存储系统等多种存储设备类型，模块需要对上述复杂情况下数据加载中的IO差异进行抽象统一，减少用户的学习成本。另一方面，不同的数据类型往往也有着不同的数据处理需求。现有常见机器学习任务中，图像任务常常对图像进行缩放、翻转、模糊化等处理，文本任务需要对文本进行切分、向量化等操作，而语音任务需要对语音进行快速傅立叶变换、混响增强、变频等预处理。为帮助用户解决绝大部分场景下的数据处理需求，数据模块需要支持足够丰富的面向各种类型的数据预处理算子。然而新的算法和数据处理需求在不断快速涌现，我们需要支持用户在数据模块中方便的使用自定义处理算子，以应对数据模块未覆盖到的场景，达到灵活性和高效性的最佳平衡。


\subsection{高效性}
\label{\detokenize{chapter_data_processing/requirements:id3}}
\sphinxAtStartPar
由于GPU/华为昇腾Ascend等常见AI加速器主要面向Tensor数据类型计算，并不具备通用的数据处理能力，现有主流机器学习系统数据模块通常选择使用CPU进行数据流水线的执行。理想情况下，在每个训练迭代步开始之前，数据模块都需要将数据准备好、以减少加速器因为等待数据而阻塞的时间消耗。然而数据流水线中的数据加载和数据预处理常常面临着具有挑战性的I/O性能和CPU计算性能问题，数据模块需要设计具备支持随机读取且具备高读取吞吐率的文件格式来解决数据读取瓶颈问题，同时还需要设计合理的并行架构来高效的执行数据流水线，以解决计算性能问题。为达到高性能的训练吞吐率，主流机器学习系统均采用数据处理与模型计算进行异步执行，以掩盖数据预处理的延迟。


\subsection{保序性}
\label{\detokenize{chapter_data_processing/requirements:id4}}
\sphinxAtStartPar
和常规的数据并行计算任务所不同的是，机器学习模型训练对数据输入顺序敏感。使用随机梯度下降算法训练模型时，通常在每一轮需要按照一种伪随机顺序向模型输入数据，并且在多轮训练(Epoch)中每一轮按照不同的随机顺序向模型输入数据。由于模型最终的参数对输入数据的顺序敏感，为了帮助用户更好的调试和确保不同次实验的可复现性，我们需要在系统中设计相应机制使得数据最终送入模型的顺序由数据混洗组件的数据输出顺序唯一确定，不会由于并行数据变换而带来最终数据模块的数据输出顺序不确定。我们将在后文中对于保序性的要求和具体实现细节展开探讨。


\section{易用性设计}
\label{\detokenize{chapter_data_processing/program_model:id1}}\label{\detokenize{chapter_data_processing/program_model::doc}}
\sphinxAtStartPar
本节我们主要介绍如何设计一个易用的机器学习系统数据模块。正如前文所言，易用性既要求数据模块提供好的编程抽象和接口使得用户可以方便的构建一个数据处理流水，同时还要支持用户灵活地在数据流水中注册使用自定义算子以满足丰富多变的特殊需求，接下来我们将从编程接口抽象和自定义算子注册机制两个方面来展开探讨。


\subsection{编程抽象与接口}
\label{\detokenize{chapter_data_processing/program_model:id2}}
\sphinxAtStartPar
\hyperref[\detokenize{chapter_data_processing/program_model:image-process-pipeline}]{图\ref{\detokenize{chapter_data_processing/program_model:image-process-pipeline}}}
我们展示的是一个训练图片分类模型的经典数据预处理流水线。我们从存储设备中加载数据集后，对数据集中的图片数据进行解码、缩放、旋转、正规化、通道变换等一系列操作，对数据集的标签也进行特定的预处理操作，最终将处理好的数据发送到芯片上进行模型的计算。我们希望数据模块提供的编程抽象具备足够高的层次，以使得用户可以通过短短几行代码就能描述清楚数据处理的逻辑，不需要陷入过度的、重复的数据处理实现细节当中。同时又要确保这一套高层次的抽象具备足够通用性，以满足多样的数据预处理需求。在我们得到一个好的编程抽象后，我们将会以基于MindSpore的数据模块提供的编程接口实现下图所描述的数据预处理流水线的代码片段为例子来展示一个优秀的编程抽象对用户编程负担的减轻是有多么大的作用。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{image_process_pipeline}.png}
\caption{数据预处理示例}\label{\detokenize{chapter_data_processing/program_model:id10}}\label{\detokenize{chapter_data_processing/program_model:image-process-pipeline}}\end{figure}

\sphinxAtStartPar
事实上，面向数据计算的编程抽象早已在通用数据并行计算系统领域中被广泛的研究并取得了相对统一的共识——那就是提供类LINQ式
\DUrole{bibtex}{{[}meijer2006linq{]}}
的编程抽象，其最大的特点是让用户专注于描述基于数据集的生成与变换，而将这些操作的高效实现与调度执行交由数据系统的运行时负责。一些优秀的系统如Naiad
\DUrole{bibtex}{{[}murray2013naiad{]}}, Spark \DUrole{bibtex}{{[}zaharia2010spark{]}}, DryadLINQ
\DUrole{bibtex}{{[}fetterly2009dryadlinq{]}}等都采用了这种编程模型。我们以Spark为例子进行简要介绍。

\sphinxAtStartPar
Spark向用户提供了基于弹性分布式数据集(Resilient Distributed Dataset,
RDD)概念的编程模型。一个RDD是一个只读的分布式数据集合，用户通过Spark的编程接口来主要描述RDD的创建及变换过程，我们以一个Spark示例进行展开讨论。下面展示了一段在一个日志文件中统计包含ERROR字段的行数的Spark代码，我们首先通过文件读取创建一个分布式的数据集file（前文提到RDD表示数据的集合，这里的file实际上是日志行的数据集合）。
我们对这个file数据集进行filter(过滤)运算得到新的只保留包含ERROR字段的日志行的数据集errs，接着我们对errs中的每一个数据进行map(映射)操作得到数据集ones，最后我们对ones数据集进行reduce操作得到了我们最终想要的统计结果，即file数据集中包含ERROR字段的日志行数。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{val} \PYG{n}{file} \PYG{o}{=} \PYG{n}{spark}\PYG{p}{.}\PYG{n+na}{textFile}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{hdfs://...}\PYG{l+s}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{val} \PYG{n}{errs} \PYG{o}{=} \PYG{n}{file}\PYG{p}{.}\PYG{n+na}{filter}\PYG{p}{(}\PYG{n}{\PYGZus{}}\PYG{p}{.}\PYG{n+na}{contains}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{ERROR}\PYG{l+s}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{val} \PYG{n}{ones} \PYG{o}{=} \PYG{n}{errs}\PYG{p}{.}\PYG{n+na}{map}\PYG{p}{(}\PYG{n}{\PYGZus{}} \PYG{o}{=}\PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{val} \PYG{n}{count} \PYG{o}{=} \PYG{n}{ones}\PYG{p}{.}\PYG{n+na}{reduce}\PYG{p}{(}\PYG{n}{\PYGZus{}}\PYG{o}{+}\PYG{n}{\PYGZus{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
我们发现用户只需要四行代码就完成了在这样一个分布式的数据集中统计特定字段行数的复杂任务，这得益于Spark核心的RDD编程抽象，从
\hyperref[\detokenize{chapter_data_processing/program_model:rdd-transformation-example}]{图\ref{\detokenize{chapter_data_processing/program_model:rdd-transformation-example}}}的计算流程可视化中我们也可以清晰的看到用户在创建数据集后，只需要描述在数据集上的作用算子即可，至于算子的执行和实现则由系统的运行时负责。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{RDD}.png}
\caption{Spark编程核心——RDD变换}\label{\detokenize{chapter_data_processing/program_model:id11}}\label{\detokenize{chapter_data_processing/program_model:rdd-transformation-example}}\end{figure}

\sphinxAtStartPar
主流机器学习系统中的数据模块同样也采用了类似的编程抽象，如TensorFlow的数据模块tf.data
\DUrole{bibtex}{{[}murray2021tf{]}},
以及MindSpore的数据模块MindData等。接下来我们以MindData的接口设计为例子来介绍如何面向机器学习这个场景设计好的编程抽象来帮助用户方便的构建模型训练中多种多样的数据处理流水线。

\sphinxAtStartPar
MindData是机器学习系统MindSpore的数据模块，主要负责完成机器学习模型训练中的数据预处理任务，MindData向用户提供的核心编程抽象为基于Dataset（数据集）的变换处理。这里的Dataset是一个数据帧的概念(Data
Frame)，即一个Dataset为一个多行多列，且每一列都有列名的关系数据表。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{dataset_table}.png}
\caption{MindSpore Dataset示例}\label{\detokenize{chapter_data_processing/program_model:id12}}\label{\detokenize{chapter_data_processing/program_model:mindspore-dataset-example}}\end{figure}

\sphinxAtStartPar
基于这样一个编程模型，结合我们在第一节中介绍的机器学习数据流程中的关键处理流程，MindData为用户提供了对数据集进行shuffle、map、batch等变换操作的数据集操作算子，这些算子接收一个Dataset作为输入，并以一个新处理生成的Dataset作为结果输出，我们列举典型的数据集变换接口如下：


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{MindSpore支持的数据集操作接口}\label{\detokenize{chapter_data_processing/program_model:id13}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
数据集操作
&\sphinxstyletheadfamily 
\sphinxAtStartPar
含义解释
\\
\hline
\sphinxAtStartPar
batch
&
\sphinxAtStartPar
将数据集中的多行数据项组成一个mini\sphinxhyphen{}batch
\\
\hline
\sphinxAtStartPar
map
&
\sphinxAtStartPar
对数据集中的每行数据进行变换操作
\\
\hline
\sphinxAtStartPar
shuffle
&
\sphinxAtStartPar
随机打乱数据集中的数据行的顺序
\\
\hline
\sphinxAtStartPar
filter
&
\sphinxAtStartPar
对数据集的数
据行进行过滤操作，只保留通过过滤条件的数据行
\\
\hline
\sphinxAtStartPar
prefetch
&
\sphinxAtStartPar
从存储介质中预取数据集
\\
\hline
\sphinxAtStartPar
project
&
\sphinxAtStartPar
从Dataset数据表中选择一些列用于接下来的处理
\\
\hline
\sphinxAtStartPar
zip
&
\sphinxAtStartPar
将多个数据集合并为一个数据集
\\
\hline
\sphinxAtStartPar
repeat
&
\sphinxAtStartPar
多轮次训练中，重复整个数据流水多次
\\
\hline
\sphinxAtStartPar
create\_dict\_iterator
&
\sphinxAtStartPar
对数据集创建一个返回字典类型数据的迭代器
\\
\hline
\sphinxAtStartPar
…
&
\sphinxAtStartPar
…
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
上述描述了数据集的接口抽象，而对数据集的具体操作实际上是由具体的数据算子函数定义。为了方便用户使用，MindData对机器学习领域常见的数据类型及其常见数据处理需求都内置实现了丰富的数据算子库。针对视觉领域，MindData提供了常见的如Decode(解码)、Resize（缩放）、RandomRotation（随机旋转）、Normalize(正规化)以及HWC2CHW（通道转置）等算子；针对文本领域，MindData提供了Ngram、NormalizeUTF8、BertTokenizer等算子；针对语音领域，MindData提供了TimeMasking（时域掩盖）、LowpassBiquad（双二阶滤波器）、ComplexNorm（归一化）等算子；这些常用算子能覆盖用户的绝大部分需求。

\sphinxAtStartPar
除了支持灵活的Dataset变换，针对数据集种类繁多、格式与组织各异的难题，MindData还提供了灵活的Dataset创建，主要分为如下三类：
\begin{itemize}
\item {} 
\sphinxAtStartPar
通过内置数据集直接创建：MindData内置丰富的经典数据集，如CelebADataset、Cifar10Dataset、CocoDataset、ImageFolderDataset、MnistDataset、VOCDataset等。如果用户需要使用这些常用数据集，可通过一行代码即可实现数据集的开箱使用。同时MindData对这些数据集的加载进行了高效的实现，以确保用户能够享受到最好的读取性能。

\item {} 
\sphinxAtStartPar
从MindRecord中加载创建：MindRecord为MindData设计的一种高性能通用数据存储文件格式，用户可将数据集转换为MindRecord后借助MindSpore的相关API进行高效的读取。

\item {} 
\sphinxAtStartPar
从Python类创建：如果用户已经有自己数据集的Python读取类，那么可以通过MindData的GeneratorDataset接口调用该Python类实现Dataset的创建，这给用户提供了极大的自由度。

\end{itemize}

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{dataset}.png}
\caption{MindSpore Dataset多种生成方式}\label{\detokenize{chapter_data_processing/program_model:id14}}\end{figure}

\sphinxAtStartPar
最后我们以一个基于MindData实现我们本节开篇所描述的数据处理流水线为例子来展示以Dataset为核心概念的数据编程抽象是多么的用户友好。我们只需要短短10余行代码即可完成我们所期望的复杂数据处理，同时在整个过程中，我们只专注于逻辑的描述，而将算子的实现和算子执行流程交由数据模块负责，这极大的减轻了用户的编程负担。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{dataset} \PYG{k}{as} \PYG{n+nn}{ds}
\PYG{k+kn}{import} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{dataset}\PYG{n+nn}{.}\PYG{n+nn}{transforms}\PYG{n+nn}{.}\PYG{n+nn}{c\PYGZus{}transforms} \PYG{k}{as} \PYG{n+nn}{c\PYGZus{}transforms}
\PYG{k+kn}{import} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{dataset}\PYG{n+nn}{.}\PYG{n+nn}{transforms}\PYG{n+nn}{.}\PYG{n+nn}{vision}\PYG{n+nn}{.}\PYG{n+nn}{c\PYGZus{}transforms} \PYG{k}{as} \PYG{n+nn}{vision}
\PYG{n}{dataset\PYGZus{}dir} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{path/to/imagefolder\PYGZus{}directory}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} create a dataset that reads all files in dataset\PYGZus{}dir with 8 threads}
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{ds}\PYG{o}{.}\PYG{n}{ImageFolderDatasetV2}\PYG{p}{(}\PYG{n}{dataset\PYGZus{}dir}\PYG{p}{,} \PYG{n}{num\PYGZus{}parallel\PYGZus{}workers}\PYG{o}{=}\PYG{l+m+mi}{8}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}create a list of transformations to be applied to the image data}
\PYG{n}{transforms\PYGZus{}list} \PYG{o}{=} \PYG{p}{[}\PYG{n}{vision}\PYG{o}{.}\PYG{n}{Decode}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
                    \PYG{n}{vision}\PYG{o}{.}\PYG{n}{Resize}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{l+m+mi}{256}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
                    \PYG{n}{vision}\PYG{o}{.}\PYG{n}{RandomRotation}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
                    \PYG{n}{vision}\PYG{o}{.}\PYG{n}{Normalize}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,}  \PYG{l+m+mf}{115.0}\PYG{p}{,} \PYG{l+m+mf}{121.0}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mf}{71.0}\PYG{p}{,} \PYG{l+m+mf}{68.0}\PYG{p}{,} \PYG{l+m+mf}{70.0}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
                    \PYG{n}{vision}\PYG{o}{.}\PYG{n}{HWC2CHW}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{onehot\PYGZus{}op} \PYG{o}{=} \PYG{n}{c\PYGZus{}transforms}\PYG{o}{.}\PYG{n}{OneHot}\PYG{p}{(}\PYG{n}{num\PYGZus{}classes}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} apply the transform to the dataset through dataset.map()}
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{input\PYGZus{}columns}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{image}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{operations}\PYG{o}{=}\PYG{n}{transforms\PYGZus{}list}\PYG{p}{)}
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{input\PYGZus{}columns}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{label}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{operations}\PYG{o}{=}\PYG{n}{onehot\PYGZus{}op}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{自定义算子支持}
\label{\detokenize{chapter_data_processing/program_model:id8}}
\sphinxAtStartPar
有了基于数据集变换的编程抽象、以及针对机器学习各种数据类型的丰富变换算子支持，我们可以覆盖用户绝大部分的数据处理需求。然而由于机器学习领域本身进展快速，新的数据处理需求不断涌现，可能会有用户想要使用的数据变换算子没有被数据模块覆盖支持到的情况发生。为此我们需要设计良好的用户自定义算子注册机制，使得用户可以方便在构建数据处理流水线时使用自定义的算子。

\sphinxAtStartPar
机器学习场景中，用户的开发编程语言以Python为主，所以我们可以认为用户的自定义算子更多情况下实际上是一个Python函数或者Python类。数据模块支持自定义算子的难度主要与数据模块对计算的调度实现方式有关系，比如Pytorch的dataloader的计算调度主要在Python层面实现，得益于Python语言的灵活性，在dataloader的数据流水中插入自定义的算子相对来说比较容易；而像TensorFlow的tf.data以及MindSpore的MindData的计算调度主要在C++层面实现，这使得数据模块想要灵活的在数据流中插入用户定义的Python算子变得较为有挑战性。接下来我们以MindData中的自定义算子注册使用实现为例子展开讨论这部分内容。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{operation}.png}
\caption{MindData的C层算子和Python层算子}\label{\detokenize{chapter_data_processing/program_model:id15}}\label{\detokenize{chapter_data_processing/program_model:mindspore-operator-example}}\end{figure}

\sphinxAtStartPar
MindData中的数据预处理算子可以分为C层算子以及Python层算子，C层算子能提供较高的执行性能而Python层算子可以很方便借助丰富的第三方Python包进行开发。为了灵活地覆盖更多场景，MindData支持用户使用Python开发自定义算子，如果用户追求更高的性能，MindData也支持用户将开发的C层算子编译后以插件的形式注册到MindSpore的数据处理中进行调用。

\sphinxAtStartPar
对于用户传入map、filter等数据集变换算子中的自定义数据处理算子，MindData的Pipeline启动后会通过创建的Python运行时来执行。需要指出的是自定义的Python算子需要保证输入、输出均是numpy.ndarray类型。具体执行过程中，当MindData的Pipeline的数据集变换中执行用户自定义的PyFunc算子时，会将输入数据以numpy.ndarray的类型传递给用户的PyFunc，自定义算子执行完毕后再以numpy.ndarray返回给MindData，在此期间，正在执行的数据集变换算子（如map、filter等）负责该PyFunc的运行时生命周期及异常判断。如果用户追求更高的性能，MindData也支持用户自定义C算子。dataset\sphinxhyphen{}plugin仓（插件仓）
\DUrole{bibtex}{{[}minddata{]}}
为MindData的算子插件仓，囊括了为特定领域（遥感，医疗，气象等）量身制作的算子，该仓承载MindData的插件能力扩展，为用户编写MindData的新算子提供了便捷易用的入口，用户通过编写算子、编译、安装插件步骤，然后就可以在MindData
Pipeline的map操作中使用新开发的算子。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{dataset-plugin}.png}
\caption{MindSpore自定义算子注册}\label{\detokenize{chapter_data_processing/program_model:id16}}\label{\detokenize{chapter_data_processing/program_model:mindspore-user-defined-operator}}\end{figure}


\section{高效性设计}
\label{\detokenize{chapter_data_processing/performance:id1}}\label{\detokenize{chapter_data_processing/performance::doc}}
\sphinxAtStartPar
在上一节中我们重点介绍了数据模块的编程抽象以及编程接口设计，确保用户可以方便的基于我们提供的API描述数据处理流程而不需要过多关注实现和执行细节。那么本节我们将进一步探究数据加载以及流水线调度执行等数据模块关键部分设计细节以确保用户能够拥有最优的数据处理性能。同时在本节内容中，我们也会贯穿现有主要机器学习系统的实践经验以帮助读者加深对这些关键设计方案的理解。

\sphinxAtStartPar
如 \hyperref[\detokenize{chapter_data_processing/performance:async-data-process}]{图\ref{\detokenize{chapter_data_processing/performance:async-data-process}}}
所示，深度学习模型训练需要借助数据模块首先从存储设备中加载数据集，在内存中进行一系列的预处理变换，最终将处理好的数据集发送到加速器芯片上执行模型的计算，目前有大量的工作都着重于研究如何通过设计新的硬件或者应用算子编译等技术加速芯片上的模型计算，而在数据处理流水的性能问题上鲜有涉及。但事实上很多情况下，数据预处理的执行时间往往在整个训练任务中占据着相当大的比例，导致GPU/华为昇腾Ascend等加速器无法被充分利用。研究数据表明，企业内数据中心的计算任务大约有30\%的计算时间花费在数据预处理步骤
\DUrole{bibtex}{{[}murray2021tf{]}}，也有研究发现在一些公开数据集上的模型训练任务有65\%的时间都花费在了数据预处理上
\DUrole{bibtex}{{[}mohan2020analyzing{]}}，由此可以看出数据模块的性能对于整体训练吞吐率有着决定性的影响。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{async_data_process}.png}
\caption{数据加载、预处理、模型计算异步并行执行}\label{\detokenize{chapter_data_processing/performance:id12}}\label{\detokenize{chapter_data_processing/performance:async-data-process}}\end{figure}

\sphinxAtStartPar
为了追求最高的训练吞吐率，现有系统一般选择将数据读取、数据预处理计算、以及芯片上的模型计算三个步骤异步并行执行。这三步构成了典型的数据生产者和数据消费者的上下游关系，我们将数据从存储设备中的读取速率用F表示，数据预处理速率用P表示，芯片上的数据消费速率用G表示。理想情况下我们希望G
< min(F,
P)，此时加速芯片不会因为等待数据而阻塞。然而现实情况下，我们常常要么因为数据加载速率F过低(称为I/O
Bound)，要么因为数据预处理速率P过低(称为CPU Bound)导致G>min(F,
P)而使得芯片无法被充分利用。针对上述关键性能问题，我们将在本节重点探究两个内容：
\begin{itemize}
\item {} 
\sphinxAtStartPar
如何针对机器学习场景的特定I/O需求来设计相应文件格式及加载方式，以优化数据读取速率F。

\item {} 
\sphinxAtStartPar
如何设计并行架构来充分发挥现代多核CPU的计算能力，以提升数据处理速率P。

\end{itemize}

\sphinxAtStartPar
在本节的最后我们还会研究一个具有挑战性的问题，即如何利用我们在前几章学到的计算图的编译技术来优化用户的数据处理计算流图，以进一步达到最优的数据处理吞吐率性能。那么接下来，请读者和我们一起开启本节的头脑风暴旅程。


\subsection{数据读取的高效性}
\label{\detokenize{chapter_data_processing/performance:id4}}
\sphinxAtStartPar
首先我们来研究如何解决数据读取的性能挑战。我们面临的第一个问题是数据类型繁多，存储格式不统一带来的I/O差异，如文本数据可能存储成txt数据格式，图像数据可能存储成原始格式或者如JPEG等压缩格式。我们显然无法去针对每一种存储情况都设计其最优的数据读取方案。但是我们可以通过提出一种统一的存储格式(我们称之为Unirecord格式)以屏蔽不同数据类型的I/O差异，并基于这种数据格式进行数据加载方案的设计与优化，而实际使用中用户只需要将其原始数据集转换存储为我们的统一数据格式便可以享受到高效的读取效率。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{uni_record}.png}
\caption{统一数据格式}\label{\detokenize{chapter_data_processing/performance:id13}}\label{\detokenize{chapter_data_processing/performance:unified-record-format}}\end{figure}

\sphinxAtStartPar
那么我们的Unirecord除了统一用户存储格式之外还需要具备哪些特性呢？机器学习模型训练中对数据的访问具有如下特点：
\begin{itemize}
\item {} 
\sphinxAtStartPar
每一个Epoch内以一种随机顺序遍历所有的数据且每个数据只被遍历一次

\item {} 
\sphinxAtStartPar
所有Epoch需要以不同的随机顺序遍历访问所有数据

\end{itemize}

\sphinxAtStartPar
上述的访问特性要求我们的Unirecord存储格式能够支持高效的随机读取。当我们的数据集能够全部存储在RAM中时，对Unirecord的随机读取并不会成为大的问题。但是当数据集大到必须存储在本地磁盘或者分布式文件系统中时，我们就需要设计特定的方案。一个直观的想法是将一个Unirecord文件分为索引块和数据块，索引块中记录每个数据在文件中的大小、偏移以及一些校验值等元信息，数据块存储每个数据的主体数据。当我们需要对一个Unirecord格式的文件进行随机读取时，我们首先在内存中加载该文件的索引块(通常远远小于整个文件大小)并在内存中建立文件内数据的索引表，接着当我们需要随机读取数据时，我们首先在索引表中查询该数据在文件中的偏移、大小等信息并基于该信息从磁盘上进行读取。这样的读取方式可以满足我们在磁盘上的随机读取需求。接下来我们以MindSpore提出的MindRecord的实践经验为例子介绍统一文件格式的设计，以帮助大家加深对这部分内容的理解

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{file_indexing}.png}
\caption{支持随机读取的文件格式设计}\label{\detokenize{chapter_data_processing/performance:id14}}\label{\detokenize{chapter_data_processing/performance:file-random-access}}\end{figure}


\subsubsection{MindRecord介绍}
\label{\detokenize{chapter_data_processing/performance:mindrecord}}
\sphinxAtStartPar
MindRecord是MindSpore推出的统一数据格式，目标是归一化用户的数据集，优化训练数据的读取过程。该文件格式具备如下特征：
\begin{itemize}
\item {} 
\sphinxAtStartPar
实现多变的用户数据统一存储、访问，训练数据读取更加简便。

\item {} 
\sphinxAtStartPar
数据聚合存储，高效读取，且方便管理、移动。

\item {} 
\sphinxAtStartPar
高效的数据编解码操作，对用户透明、无感知。

\item {} 
\sphinxAtStartPar
可以灵活控制分区的大小，实现分布式训练。

\end{itemize}

\sphinxAtStartPar
和我们前文设计的Unirecord思路相似，一个MindRecord文件也由数据文件和索引文件组成，数据文件包含文件头、标量数据页、块数据页，用于存储用户归一化后的训练数据，索引文件包含基于标量数据（如图像Label、图像文件名等）生成的索引信息，用于方便的检索、统计数据集信息。为确保对一个MindRecord文件的随机读取性能，MindSpore建议单个MindRecord文件小于20G，若数据集超过20G，用户可在MindRecord数据集生成时指定相应参数将原始数据集分片存储为多个MindRecord文件。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{MindRecord_format}.png}
\caption{MindRecord文件格式组成}\label{\detokenize{chapter_data_processing/performance:id15}}\label{\detokenize{chapter_data_processing/performance:mindrecord-format}}\end{figure}

\sphinxAtStartPar
一个MindRecord文件中的数据文件部分具体的关键部分的详细信息如下：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{文件头}
文件头主要用来存储文件头大小、标量数据页大小、块数据页大小、Schema信息、索引字段、统计信息、文件分区信息、标量数据与块数据对应关系等，是MindRecord文件的元信息。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{标量数据页}
标量数据页主要用来存储整型、字符串、浮点型数据，如图像的Label、图像的文件名、图像的长宽等信息，即适合用标量来存储的信息会保存在这里。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{块数据页}
块数据页主要用来存储二进制串、Numpy数组等数据，如二进制图像文件本身、文本转换成的字典等。

\end{itemize}

\sphinxAtStartPar
用户训练时，MindRecord的读取器能基于索引文件快速的定位找到数据所在的位置，并将其读取解码出来。另外MindRecord具备一定的检索能力，用户可以通过指定查询条件筛选获取符合期望的数据样本。

\sphinxAtStartPar
对于分布式训练场景，MindRecord会基于数据文件中Header及索引文件进行元数据的加载，得到所有样本的ID及样本在数据文件中的偏移信息，然后根据用户输入的num\_shards（训练节点数）和shard\_id（当前节点号）进行数据的partition，得到当前节点的num\_shards分之一的数据，即：分布式训练时，多个节点只读取数据集的num\_shards分之一，借由计算侧的AllReduce实现整个数据集训练的效果。进一步，如果用户开启shuffle操作，那么每epoch保证所有节点shuffle
seed保持一致，那么对所有样本的ID
shuffle结果是一致的，那么数据partition的结果就是正确的。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{partition}.png}
\caption{MindRecord Partition策略}\label{\detokenize{chapter_data_processing/performance:id16}}\label{\detokenize{chapter_data_processing/performance:mindrecord-partition}}\end{figure}


\subsection{数据计算的高效性}
\label{\detokenize{chapter_data_processing/performance:id5}}
\sphinxAtStartPar
解决了数据读取性能问题后，我们继续来研究数据计算的性能提升(即最大化上文中的数据处理速率P)。我们以上文提及的数据预处理流水为例子、来研究如何设计数据模块对用户计算图的调度执行以达到最优的性能。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{single_pipeline}.png}
\caption{数据预处理流程串行顺序执行示意图}\label{\detokenize{chapter_data_processing/performance:id17}}\label{\detokenize{chapter_data_processing/performance:serialized-data-process}}\end{figure}

\sphinxAtStartPar
由于深度学习芯片如GPU/华为昇腾Ascend等并不具备通用数据处理的能力，
我们目前还是主要依赖CPU来完成预处理计算。主流的AI服务器大多具备多个多核CPU，数据模块需要设计合理的并行架构充分发挥多核算力，以提升数据预处理性能达到尽可能减少加速器由于等待数据而阻塞的目的。本节中我们将介绍流水线粒度并行以及算子粒度并行两种常见的并行架构。流水线并行的方式结构清晰，易于理解和实现，主要被Pytorch这样基于Python实现数据模块的机器学习系统所采用。受到经典数据并行系统调度执行架构设计的影响，其他如Google的TensorFlow以及华为的MindSpore等系统主要采用算子粒度并行做精细CPU算力分配以达到充分利用多核算力的目的。然而精细的分配意味着我们需要对所有数据处理流程中涉及的算子设置合理的并行参数，这对用户而言是一个较大的挑战。于是MindSpore等框架又提供数据流图中关键参数自动调优的功能，通过运行时的动态分析自动搜索得到最优的算子并行度等参数，极大的减少了用户的编程负担。接下来我们一一展开讨论。


\subsubsection{流水线并行}
\label{\detokenize{chapter_data_processing/performance:id6}}
\sphinxAtStartPar
第一种常见的并行方案为流水线粒度的并行，即我们把用户构建的计算流水在一个线程/进程内顺序串行执行，同时启动多个线程/进程并行执行多个流水线。假设用户总共需要处理N个数据样本，那么当流水线并行度为M时，每个进程/线程只需要执行处理(N/M)个样本。流水线并行架构结构简单，易于实现。整个并行架构中各个执行进程/线程只需要在数据执行的开始和结束进行跨进程/线程的通信即可，数据模块将待处理的数据任务分配给各个流水线进程/线程，并在最终进行结果汇总发送到芯片上进行模型计算。从用户的角度而言使用也相对方便，只需要指定关键的并行度参数即可。接下来我们以Pytorch为例子进行详细展开。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{pipeline_parallisim}.png}
\caption{流水线级别并行执行示意图}\label{\detokenize{chapter_data_processing/performance:id18}}\label{\detokenize{chapter_data_processing/performance:pipeline-parallisim}}\end{figure}

\sphinxAtStartPar
在Pytorch中，用户只需要实现一个Dataset的Python类编写数据处理过程，Dataloader通过用户指定的并行度参数num\_workers来启动相应个数的Python进程调用用户自定义的Dataset类进行数据预处理。Dataloader中的进程有两类角色：worker进程以及主进程，以及两类进程间通信队列：index\_queue以及worker\_result\_queue。训练过程中，主进程负责将待处理数据任务列表通过index\_queue发送给各个worker进程，每个woker进程执行用户编写的Dataset类的数据预处理逻辑并将处理后的结果通过worker\_result\_queue返回给主进程。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{pytorch_dataloader}.png}
\caption{Pytorch Dataloader并行执行架构}\label{\detokenize{chapter_data_processing/performance:id19}}\label{\detokenize{chapter_data_processing/performance:pytorch-dataloader}}\end{figure}

\sphinxAtStartPar
接下来我们展示一段用户使用Pytorch的Dataloader进行并行数据预处理的代码片段，可以发现我们只需要实现Dataset类描述数据预处理逻辑，并指定num\_workers即可实现流水线粒度的并行数据预处理。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} 描述数据预处理流程}
\PYG{k}{class} \PYG{n+nc}{TensorDataset}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{inps}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{sef}\PYG{o}{.}\PYG{n}{inps} \PYG{o}{=} \PYG{n}{inps}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}getitem\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{idx}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{data} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{inps}\PYG{p}{[}\PYG{n}{idx}\PYG{p}{]}
        \PYG{n}{data} \PYG{o}{=} \PYG{n}{data} \PYG{o}{+} \PYG{l+m+mi}{1}
        \PYG{k}{return} \PYG{n}{data}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}len\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{inps}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}

\PYG{n}{inps} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{10} \PYG{o}{*} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{TensorDataset}\PYG{p}{(}\PYG{n}{inps}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} 指定并行度为3}
\PYG{n}{loader} \PYG{o}{=} \PYG{n}{DataLoader}\PYG{p}{(}\PYG{n}{dataset}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{num\PYGZus{}workers}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{sample} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{loader}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
最后需要指出的是, Pytorch
Dataloader的执行过程中涉及大量进程间通信，虽然为了加速这一步骤，Pytorch对Tensor类数据实现了基于共享内存的进程间通信机制。然而当通信数据量较大时，跨进程通信仍然会较大地影响端到端的数据预处理吞吐率性能。当然，这不是流水线并行自身的架构问题，而是由于CPython的全局解释器锁(Global
Interpreter Lock,
GIL)导致在Python层面实现流水线并行时只能采用进程并行。为了解决这个问题，目前Pytorch团队也在尝试通过移除CPython中的GIL来达到基于多线程实现流水线并行以提升通信效率的目的
\DUrole{bibtex}{{[}rmpygil{]}}，感兴趣的读者可以选择继续深入了解。


\subsubsection{算子并行}
\label{\detokenize{chapter_data_processing/performance:id8}}
\sphinxAtStartPar
流水线并行中算力(CPU核心)的分配以流水线为粒度，相对而言，以算子为计算资源分配粒度的算子并行是一种追求更精细算力分配的并行方案。我们期望对计算耗时高的算子分配更高的并行度，计算耗时低的算子分配更低的并行度，以达到更加高效合理的CPU算力使用。算子并行想法和经典的数据并行计算系统的并行方式一脉相承，我们以经典的MapReduce计算执行为例子，我们发现这也可以认为是一种算子并行(map算子和reduce算子),其中map算子的并行度和reduce算子的并行度根据各个算子阶段的计算耗时而决定。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{map_reduce}.png}
\caption{MapReduce经典并行执行架构}\label{\detokenize{chapter_data_processing/performance:id20}}\label{\detokenize{chapter_data_processing/performance:mapreduce}}\end{figure}

\sphinxAtStartPar
下图中我们给出本节开头数据预处理流程的算子并行架构示意图，我们根据各个算子的计算耗时设置图片解码算子并行度为3，图片缩放并行度为2，图片随机旋转算子并行度为4，图片归一化算子并行度为3，以及图像通道转置算子并行度为1。我们期望通过给不同耗时的算子精准的分配算力，以达到算力高效充分的利用。具体实现中算子并行一般采用线程级并行，所有的算子使用线程间队列等方法进行共享内存通信。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{operator_parallisim}.png}
\caption{算子并行执行架构}\label{\detokenize{chapter_data_processing/performance:id21}}\label{\detokenize{chapter_data_processing/performance:operator-parallisim}}\end{figure}

\sphinxAtStartPar
现有机器学习系统的数据模块中，tf.data以及MindData均采用了算子并行的方案。由于对算力的利用更加充分、以及基于C++的高效数据流调度实现，算子并行的方案往往展示出更好的性能，tf.data的性能评测表明其相比较Pytorch的Dataloader有近两倍的性能优势
\DUrole{bibtex}{{[}murray2021tf{]}}。
接下来我们以一段基于MindSpore实现本节开篇的数据预处理流程来展示如何在一个算子并行的数据流水线中设置各个算子的并行度。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{dataset} \PYG{k}{as} \PYG{n+nn}{ds}
\PYG{k+kn}{import} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{dataset}\PYG{n+nn}{.}\PYG{n+nn}{transforms}\PYG{n+nn}{.}\PYG{n+nn}{c\PYGZus{}transforms} \PYG{k}{as} \PYG{n+nn}{c\PYGZus{}transforms}
\PYG{k+kn}{import} \PYG{n+nn}{mindspore}\PYG{n+nn}{.}\PYG{n+nn}{dataset}\PYG{n+nn}{.}\PYG{n+nn}{transforms}\PYG{n+nn}{.}\PYG{n+nn}{vision}\PYG{n+nn}{.}\PYG{n+nn}{c\PYGZus{}transforms} \PYG{k}{as} \PYG{n+nn}{vision}

\PYG{c+c1}{\PYGZsh{} 读取数据}
\PYG{n}{dataset\PYGZus{}dir} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{path/to/imagefolder\PYGZus{}directory}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{ds}\PYG{o}{.}\PYG{n}{ImageFolderDatasetV2}\PYG{p}{(}\PYG{n}{dataset\PYGZus{}dir}\PYG{p}{,} \PYG{n}{num\PYGZus{}parallel\PYGZus{}workers}\PYG{o}{=}\PYG{l+m+mi}{8}\PYG{p}{)}
\PYG{n}{transforms\PYGZus{}list} \PYG{o}{=} \PYG{p}{[}\PYG{n}{vision}\PYG{o}{.}\PYG{n}{Decode}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
                   \PYG{n}{vision}\PYG{o}{.}\PYG{n}{Resize}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{l+m+mi}{256}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
                   \PYG{n}{vision}\PYG{o}{.}\PYG{n}{RandomRotation}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
                   \PYG{n}{vision}\PYG{o}{.}\PYG{n}{Normalize}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,}  \PYG{l+m+mf}{115.0}\PYG{p}{,} \PYG{l+m+mf}{121.0}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mf}{71.0}\PYG{p}{,} \PYG{l+m+mf}{68.0}\PYG{p}{,} \PYG{l+m+mf}{70.0}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
                   \PYG{n}{vision}\PYG{o}{.}\PYG{n}{HWC2CHW}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{onehot\PYGZus{}op} \PYG{o}{=} \PYG{n}{c\PYGZus{}transforms}\PYG{o}{.}\PYG{n}{OneHot}\PYG{p}{(}\PYG{n}{num\PYGZus{}classes}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} 解码算子并行度为3}
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{input\PYGZus{}columns}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{image}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{operations}\PYG{o}{=}\PYG{n}{vision}\PYG{o}{.}\PYG{n}{Decode}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{num\PYGZus{}parallel\PYGZus{}workers}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} 缩放算子并行度为2}
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{input\PYGZus{}columns}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{image}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{operations}\PYG{o}{=}\PYG{n}{vision}\PYG{o}{.}\PYG{n}{Resize}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{l+m+mi}{256}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{num\PYGZus{}parallel\PYGZus{}workers}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} 随机旋转算子并行度为4}
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{input\PYGZus{}columns}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{image}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{operations}\PYG{o}{=}\PYG{n}{vision}\PYG{o}{.}\PYG{n}{RandomRotation}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{num\PYGZus{}parallel\PYGZus{}workers}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} 正规化算子并行度为3}
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{input\PYGZus{}columns}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{image}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{operations}\PYG{o}{=}\PYG{n}{vision}\PYG{o}{.}\PYG{n}{Normalize}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,}  \PYG{l+m+mf}{115.0}\PYG{p}{,} \PYG{l+m+mf}{121.0}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mf}{71.0}\PYG{p}{,} \PYG{l+m+mf}{68.0}\PYG{p}{,} \PYG{l+m+mf}{70.0}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{num\PYGZus{}parallel\PYGZus{}workers}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} 通道转置算子并行度为1}
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{input\PYGZus{}columns}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{image}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{operations}\PYG{o}{=}\PYG{n}{vision}\PYG{o}{.}\PYG{n}{HWC2CHW}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{num\PYGZus{}parallel\PYGZus{}workers}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{input\PYGZus{}columns}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{label}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{operations}\PYG{o}{=}\PYG{n}{onehot\PYGZus{}op}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
我们发现，虽然算子并行具备更高的性能潜力，但却需要我们对每一个算子设置合理并行参数。这不仅对用户提出了较高的要求，同时也增加了由于不合理的并行参数设置导致性能反而下降的风险。为了让用户更加轻松的使用算子并行，tf.data和MindData都增加了流水线关键参数动态调优功能，基于对流水线执行时的性能监控计算得到合理的参数以尽可能达到最优的数据预处理吞吐率
\DUrole{bibtex}{{[}murray2021tf{]}}。


\subsubsection{数据处理计算图优化}
\label{\detokenize{chapter_data_processing/performance:id11}}
\sphinxAtStartPar
在前文中，我们专注于通过并行架构来高效执行用户构建的数据预处理计算图。但我们可以思考如下问题：用户给定的计算图是否是一个高效的计算图?
如果不高效，我们是否能够在保证等价变换的前提下将用户的数据计算图进行优化重写得到执行性能预期更好的计算图？没错，这和我们在前几章中学习的模型计算图编译优化有着相同的思想，即通过分析变换计算图IR得到更优的IR表示来达到更好的执行性能。常用的数据图优化策略有算子融合以及map操作向量化两种。算子融合将map+map、map+batch、map+filter、filter+filter等算子组合融合成一个等价复合算子，将原先需要在两个线程组中执行的计算融合为在一个线程组中执行的复合计算，减少线程间的同步通信开销，达到了更优的性能。而map操作向量化则将常见的dataset.map(f).batch(b)操作组合变换调整为dataset.batch(b).map(parallel\_for(f))，借助现代CPU的对并行操作更友好的SIMD指令集来加速数据预处理。


\section{保序性设计}
\label{\detokenize{chapter_data_processing/data_order:id1}}\label{\detokenize{chapter_data_processing/data_order::doc}}
\sphinxAtStartPar
和常规数据并行计算任务不同的是，机器学习场景下的数据并行处理为了确保实验的可复现性需要维护保序的性质。在具体实现中，我们需要保证并行数据预处理后的数据输出顺序与输入顺序保持相同(即下图中的SeqB和SeqA相同)。这确保了每一次的数据模块的结果输出顺序由数据混洗模块输出顺序唯一确定，有助于用户在不同的实验之间进行比较和调试。不同的机器学习系统采用了不同的方案来确保保序性，我们以MindSpore的实现为例子进行介绍以加深读者对这部分内容的理解。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{data_ordering}.png}
\caption{数据的保序性——确保SeqB与SeqA相同}\label{\detokenize{chapter_data_processing/data_order:id2}}\label{\detokenize{chapter_data_processing/data_order:data-order-definition}}\end{figure}

\sphinxAtStartPar
MindSpore通过约束算子线程组间的通信行为来确保对当前算子的下游算子的输入顺序与自己的输入顺序相同，基于这种递归的约束，确保了整个并行数据处理最后一个算子的输出顺序与第一个算子的输入顺序相同。具体实现中，MindSpore以Connector为算子线程组间的通信组件，对Connector的核心操作为上游算子的Push操作以及下游算子的Pop操作，我们重点关注MindSpore对这两个行为的约束。

\sphinxAtStartPar
Connector的使用有如下两个要求：
\begin{itemize}
\item {} 
\sphinxAtStartPar
Connector两端的数据生产线程组和数据消费线程组中的线程分别从0开始编号。

\item {} 
\sphinxAtStartPar
确保数据生产者的输入数据顺序是在各个生产者线程间为按顺序轮询分布(Round\sphinxhyphen{}Robin
distribution), 即当生产者线程组大小为M时，生产者线程0拥有第(0 + M *
k)个数据，生产者线程1拥有第(1 + M * k)，生产者线程2拥有第(2 + M *
k)个数据等(其中k=0，1，2，3…)。

\end{itemize}

\sphinxAtStartPar
Connector中维护与生产者线程数目相同的队列并确保向Connector中放入数据时，每个生产者线程生产的数据只放到对应编号的队列中，这样可以确保Connector中的数据在不同的队列间的分布与在不同生产者线程组之间的分布相同(代码片段中的Push函数)。接着当Connector的消费者线程组从Connector中获取数据时，我们需要确保最终数据在不同的消费者线程间依然为按顺序轮询分布，即当消费者线程组大小为N时，消费者线程0拥有第(0
+ N * k)个数据，消费者线程1拥有第(1 + N *
k)个数据，消费者线程2拥有第(2 + N *
k)个数据等(其中k=0，1，2，3…)。为此当有消费者线程从Connector中请求数据时，Connector在确保当前请求消费者线程编号i与待消费数据标号j符合\(i=j\%N\)的关系下(其中N为消费者线程数目)按照轮循的方式从各个队列中获取数据，如果二者标号不符合上述关系，则该请求阻塞等待。通过这种通信的约束方式，MindSpore实现了保序功能。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{mindspore_data_order}.jpeg}
\caption{MindSpore保序性实现}\label{\detokenize{chapter_data_processing/data_order:id3}}\label{\detokenize{chapter_data_processing/data_order:mindspore-data-order-implementation}}\end{figure}


\section{单机数据处理性能的扩展}
\label{\detokenize{chapter_data_processing/extension:id1}}\label{\detokenize{chapter_data_processing/extension::doc}}
\sphinxAtStartPar
上文我们介绍了通过并行架构发挥多核CPU算力来加速数据预处理，以满足芯片上模型计算对于数据消费的吞吐率需求，这在大部分情况下都能解决用户的问题。然而数据消费性能随着AI芯片的发展在逐年快速增长（即模型计算速率在变快），而主要借助CPU算力的数据模块却由于摩尔定律的逐渐终结无法享受到芯片性能提升带来的硬件红利，使得数据生产的性能很难像模型计算性能一样逐年突破。不仅如此，近几年AI服务器上AI芯片数量的增长速度远超CPU数量的增长速度，进一步加剧了芯片的数据消费需求与数据模块的数据生产性能之间的矛盾。我们以英伟达(NVIDIA)公司生产的NVIDIA
DGX系列服务器为例子，DGX\sphinxhyphen{}1服务器中配置有40个CPU核和8个GPU芯片，而到了下一代的NVIDIA
DGX\sphinxhyphen{}2服务器时，GPU芯片的数目增长了到了16个，而CPU核的数目仅从40个增加到了48个。由于所有的GPU芯片在训练时共享CPU的算力，故平均而言每个GPU芯片(数据消费者)能够使用的算力从NVIDIA
DGX\sphinxhyphen{}1时的5CPU核/GPU下降到了 NVIDIA
DGX\sphinxhyphen{}2的3CPU核/GPU，CPU的算力瓶颈会导致用户使用多卡训练时无法达到预期的扩展性能。针对单机上的CPU算力不足的问题，我们给出两种目前常见的两种解决方案，即基于CPU+AI芯片的异构数据处理的加速方案和基于分布式数据预处理的扩展方案。


\subsection{基于异构计算的数据预处理}
\label{\detokenize{chapter_data_processing/extension:id2}}
\sphinxAtStartPar
由于AI芯片相比于CPU拥有更丰富的算力资源，故在CPU算力成为数据预处理瓶颈时通过借助AI加速芯片来做数据预处理是一个行之有效的方案。虽然AI芯片不具备通用的数据预处理能力，但是由于大部分高耗时的数据预处理都是Tensor相关的计算，如语音中的快速傅立叶变换(Fast
Fourier Transform,
FFT)，图像中的去噪等，使得部分操作可以被卸载到AI芯片上来加速。如华为昇腾Ascend310芯片上的Dvpp模块为芯片内置的硬件解码器，相较于CPU拥有对图形处理更强劲的性能，Dvpp支持JPEG图片的解码缩放等图像处理基础操作，用户实际数据预处理中可以指定部分图像处理在昇腾Ascend310芯片上完成以提升数据模块性能。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{namespace} \PYG{n}{ms} \PYG{o}{=} \PYG{n}{mindspore}\PYG{p}{;}
\PYG{n}{namespace} \PYG{n}{ds} \PYG{o}{=} \PYG{n}{mindspore}\PYG{p}{:}\PYG{p}{:}\PYG{n}{dataset}\PYG{p}{;}

\PYG{o}{/}\PYG{o}{/} \PYG{n}{初始化操作}
\PYG{o}{/}\PYG{o}{/}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}

\PYG{o}{/}\PYG{o}{/} \PYG{n}{构建数据处理算子}

\PYG{o}{/}\PYG{o}{/} \PYG{l+m+mf}{1.} \PYG{n}{解码}
\PYG{n}{std}\PYG{p}{:}\PYG{p}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{ds}\PYG{p}{:}\PYG{p}{:}\PYG{n}{TensorTransform}\PYG{o}{\PYGZgt{}} \PYG{n}{decode}\PYG{p}{(}\PYG{n}{new} \PYG{n}{ds}\PYG{p}{:}\PYG{p}{:}\PYG{n}{vision}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Decode}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
\PYG{o}{/}\PYG{o}{/} \PYG{l+m+mf}{2.} \PYG{n}{缩放}
\PYG{n}{std}\PYG{p}{:}\PYG{p}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{ds}\PYG{p}{:}\PYG{p}{:}\PYG{n}{TensorTransform}\PYG{o}{\PYGZgt{}} \PYG{n}{resize}\PYG{p}{(}\PYG{n}{new} \PYG{n}{ds}\PYG{p}{:}\PYG{p}{:}\PYG{n}{vision}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Resize}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{256}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
\PYG{o}{/}\PYG{o}{/} \PYG{l+m+mf}{3.} \PYG{n}{归一化}
\PYG{n}{std}\PYG{p}{:}\PYG{p}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{ds}\PYG{p}{:}\PYG{p}{:}\PYG{n}{TensorTransform}\PYG{o}{\PYGZgt{}} \PYG{n}{normalize}\PYG{p}{(}\PYG{n}{new} \PYG{n}{ds}\PYG{p}{:}\PYG{p}{:}\PYG{n}{vision}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Normalize}\PYG{p}{(}
    \PYG{p}{\PYGZob{}}\PYG{l+m+mf}{0.485} \PYG{o}{*} \PYG{l+m+mi}{255}\PYG{p}{,} \PYG{l+m+mf}{0.456} \PYG{o}{*} \PYG{l+m+mi}{255}\PYG{p}{,} \PYG{l+m+mf}{0.406} \PYG{o}{*} \PYG{l+m+mi}{255}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{p}{\PYGZob{}}\PYG{l+m+mf}{0.229} \PYG{o}{*} \PYG{l+m+mi}{255}\PYG{p}{,} \PYG{l+m+mf}{0.224} \PYG{o}{*} \PYG{l+m+mi}{255}\PYG{p}{,} \PYG{l+m+mf}{0.225} \PYG{o}{*} \PYG{l+m+mi}{255}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
\PYG{o}{/}\PYG{o}{/} \PYG{l+m+mf}{4.} \PYG{n}{剪裁}
\PYG{n}{std}\PYG{p}{:}\PYG{p}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{ds}\PYG{p}{:}\PYG{p}{:}\PYG{n}{TensorTransform}\PYG{o}{\PYGZgt{}} \PYG{n}{center\PYGZus{}crop}\PYG{p}{(}\PYG{n}{new} \PYG{n}{ds}\PYG{p}{:}\PYG{p}{:}\PYG{n}{vision}\PYG{p}{:}\PYG{p}{:}\PYG{n}{CenterCrop}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{224}\PYG{p}{,} \PYG{l+m+mi}{224}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}

\PYG{o}{/}\PYG{o}{/} \PYG{n}{构建流水并指定使用昇腾Ascend进行计算}
\PYG{n}{ds}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Execute} \PYG{n}{preprocessor}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{decode}\PYG{p}{,} \PYG{n}{resize}\PYG{p}{,} \PYG{n}{center\PYGZus{}crop}\PYG{p}{,} \PYG{n}{normalize}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{MapTargetDevice}\PYG{p}{:}\PYG{p}{:}\PYG{n}{kAscend310}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{;}

\PYG{o}{/}\PYG{o}{/} \PYG{n}{执行数据处理流水}
\PYG{n}{ret} \PYG{o}{=} \PYG{n}{preprocessor}\PYG{p}{(}\PYG{n}{image}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{image}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
相比较Dvpp只支持图像的部分预处理操作，英伟达公司研发的DALI
\DUrole{bibtex}{{[}nvidia\_dali{]}}是一个更加通用的基于GPU的数据预处理加速框架。DALI中包含如下三个核心概念：
\begin{itemize}
\item {} 
\sphinxAtStartPar
DataNode:表示一组Tensor的集合

\item {} 
\sphinxAtStartPar
Operator:对DataNode进行变换处理的算子，一个Operator的输入和输出均为DataNode。比较特殊的是，DALI中的算子可以被设置为包括cpu，gpu，mixed三种不同执行模式，其中cpu模式下算子的输入输出均为cpu上的DataNode，gpu模式下算子的输入输出均为gpu上的DataNode，而mixed模式下的算子的输入为cpu的DataNode而输出为gpu的DataNode。

\item {} 
\sphinxAtStartPar
Pipeline:用户通过Operator描述DataNode的处理变换过程而构建的数据处理流水

\end{itemize}

\sphinxAtStartPar
实际使用中用户通过设置算子的运行模式(mode)来配置算子的计算是用CPU还是GPU完成计算，同时DALI中有如下限制：当一个算子为mixed模式或者gpu模式时，其所有的下游算子强制要求必须为gpu模式执行。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{dali_overview}.png}
\caption{NVIDIA DALI概览}\label{\detokenize{chapter_data_processing/extension:id6}}\label{\detokenize{chapter_data_processing/extension:dali-overview}}\end{figure}

\sphinxAtStartPar
下面展示一段使用DALI构建数据处理流水线的示例代码，我们从文件中读取图片数据经过混合模式的解码再经过运算在GPU上的旋转和缩放算子处理后返回给用户处理
结果。由于其展示出的优异性能，
DALI被广泛的用于高性能推理服务和多卡训练性能的优化上。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{nvidia}\PYG{n+nn}{.}\PYG{n+nn}{dali} \PYG{k}{as} \PYG{n+nn}{dali}

\PYG{n}{pipe} \PYG{o}{=} \PYG{n}{dali}\PYG{o}{.}\PYG{n}{pipeline}\PYG{o}{.}\PYG{n}{Pipeline}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{num\PYGZus{}threads} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{device\PYGZus{}id} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{k}{with} \PYG{n}{pipe}\PYG{p}{:}
    \PYG{n}{files}\PYG{p}{,} \PYG{n}{labels} \PYG{o}{=} \PYG{n}{dali}\PYG{o}{.}\PYG{n}{fn}\PYG{o}{.}\PYG{n}{readers}\PYG{o}{.}\PYG{n}{file}\PYG{p}{(}\PYG{n}{file\PYGZus{}root} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{./my\PYGZus{}file\PYGZus{}root}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{images} \PYG{o}{=} \PYG{n}{dali}\PYG{o}{.}\PYG{n}{fn}\PYG{o}{.}\PYG{n}{decoders}\PYG{o}{.}\PYG{n}{image}\PYG{p}{(}\PYG{n}{files}\PYG{p}{,} \PYG{n}{device} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mixed}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{images} \PYG{o}{=} \PYG{n}{dali}\PYG{o}{.}\PYG{n}{fn}\PYG{o}{.}\PYG{n}{rotate}\PYG{p}{(}\PYG{n}{images}\PYG{p}{,} \PYG{n}{angle} \PYG{o}{=} \PYG{n}{dali}\PYG{o}{.}\PYG{n}{fn}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{n+nb}{range}\PYG{o}{=}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{45}\PYG{p}{,}\PYG{l+m+mi}{45}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{images} \PYG{o}{=} \PYG{n}{dali}\PYG{o}{.}\PYG{n}{fn}\PYG{o}{.}\PYG{n}{resize}\PYG{p}{(}\PYG{n}{images}\PYG{p}{,} \PYG{n}{resize\PYGZus{}x} \PYG{o}{=} \PYG{l+m+mi}{300}\PYG{p}{,} \PYG{n}{resize\PYGZus{}y} \PYG{o}{=} \PYG{l+m+mi}{300}\PYG{p}{)}
    \PYG{n}{pipe}\PYG{o}{.}\PYG{n}{set\PYGZus{}outputs}\PYG{p}{(}\PYG{n}{images}\PYG{p}{,} \PYG{n}{labels}\PYG{p}{)}

\PYG{n}{pipe}\PYG{o}{.}\PYG{n}{build}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{outputs} \PYG{o}{=} \PYG{n}{pipe}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{基于分布式的数据预处理}
\label{\detokenize{chapter_data_processing/extension:id4}}
\sphinxAtStartPar
分布式数据预处理是另一种解决CPU算力性能不足的可选方案。一种常见的做法是借助Spark、Dask等现有大数据计算框架进行数据预处理并将结果写入分布式文件系统，而训练的机器只需要读取预处理的结果数据并进行训练即可。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{distribute}.png}
\caption{基于第三方分布式计算框架的分布式数据预处理}\label{\detokenize{chapter_data_processing/extension:id7}}\label{\detokenize{chapter_data_processing/extension:distributed-data-preprocess-based-on-3rd-party-software}}\end{figure}

\sphinxAtStartPar
该方案虽然在业内被广泛使用，却面临着三个问题：
\begin{itemize}
\item {} 
\sphinxAtStartPar
由于数据处理和数据训练采用不同的框架，使得用户为此常常需要在两个不同的框架中编写不同语言的程序，增加了用户的使用负担。

\item {} 
\sphinxAtStartPar
由于数据处理系统和机器学习两个系统间无法做零拷贝的数据共享，使得数据的序列化和反序列化常常成为不可忽视的额外开销。

\item {} 
\sphinxAtStartPar
由于大数据计算框架并不是完全针对机器学习场景，使得某些分布式预处理操作如全局的数据混洗无法被高效的实现。

\end{itemize}

\sphinxAtStartPar
为了更适配机器学习场景的数据预处理，分布式机器学习框架Ray借助其自身的任务调度能力实现了简单的分布式的数据预处理——
Ray Dataset
\sphinxcite{chapter_reinforcement_learning/summary:moritz2018ray}，由于数据预处理和训练处在同一个框架内，在降低了用户的编程负担的同时也通过数据的零拷贝共享消除了序列化/反序列化带来的额外开销。Ray
Dataset支持如map、batch、map、filter等简单并行数据集变换算子、以及如mean等一些基础的聚合操作算子。同时Ray
Dataset也支持排序、随机打乱、GroupBy等全局混洗操作，该方案目前处在研究开发中，还未被广泛的采用，感兴趣的读者可以翻阅相关资料进一步的了解。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ray}\PYG{o}{.}\PYG{n}{data}\PYG{o}{.}\PYG{n}{read\PYGZus{}parquet}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{foo.parquet}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYGZbs{}
    \PYG{o}{.}\PYG{n}{filter}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{0}\PYG{p}{)} \PYGZbs{}
    \PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)} \PYGZbs{}
    \PYG{o}{.}\PYG{n}{random\PYGZus{}shuffle}\PYG{p}{(}\PYG{p}{)} \PYGZbs{}
    \PYG{o}{.}\PYG{n}{write\PYGZus{}parquet}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bar.parquet}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\section{总结}
\label{\detokenize{chapter_data_processing/summary:id1}}\label{\detokenize{chapter_data_processing/summary::doc}}
\sphinxAtStartPar
本章我们围绕着易用性、高效性和保序性三个维度展开研究如何设计实现机器学习系统中的数据预处理模块。在易用性维度我们重点探讨了数据模块的编程模型，通过借鉴历史上优秀的并行数据处理系统的设计经验，我们认为基于描述数据集变换的编程抽象较为适合作为数据模块的编程模型，在具体的系统实现中，我们不仅要在上述的编程模型的基础上提供足够多内置算子方便用户的数据预处理编程，同时还要考虑如何支持用户方便的使用自定义算子。在高效性方面，我们从数据读取和计算两个方面分别介绍了特殊文件格式设计和计算并行架构设计。我们也使用我们在前几章中学习到的模型计算图编译优化技术来优化用户的数据预处理计算图，以进一步的达到更高的数据处理吞吐率。机器学习场景中模型对数据输入顺序敏感，于是衍生出来保序性这一特殊性质，我们在本章中对此进行了分析并通过MindSpore中的Connector的特殊约束实现来展示真实系统实现中如何确保保序性。最后，我们也针对部分情况下单机CPU数据预处理性能的问题，介绍了当前基于异构处理加速的纵向扩展方案，和基于分布式数据预处理的横向扩展方案，我们相信读者学习了本章后能够对机器学习系统中的数据模块有深刻的认知，也对数据模块未来面临的挑战有所了解。


\section{扩展阅读}
\label{\detokenize{chapter_data_processing/summary:id2}}\begin{itemize}
\item {} 
\sphinxAtStartPar
流水线粒度并行实现示例建议阅读 \sphinxhref{https://github.com/pytorch/pytorch/tree/master/torch/utils/data}{Pytorch
DataLoader}%
\begin{footnote}[39]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/pytorch/pytorch/tree/master/torch/utils/data}
%
\end{footnote}。

\item {} 
\sphinxAtStartPar
算子粒度并行实现示例建议阅读
\sphinxhref{https://gitee.com/mindspore/mindspore/tree/master/mindspore/ccsrc/minddata}{MindData}%
\begin{footnote}[40]\sphinxAtStartFootnote
\sphinxnolinkurl{https://gitee.com/mindspore/mindspore/tree/master/mindspore/ccsrc/minddata}
%
\end{footnote}。

\end{itemize}


\chapter{模型部署}
\label{\detokenize{chapter_model_deployment/index:id1}}\label{\detokenize{chapter_model_deployment/index::doc}}
\sphinxAtStartPar
在前面的章节中，我们讲述了机器学习模型训练系统的基本组成,这一章节我们将讲述模型部署的相关知识。模型部署是将训练好的模型部署到运行环境中进行推理的过程，模型部署的过程中需要解决训练模型到推理模型的转换，硬件资源对模型的限制，模型推理的时延、功耗、内存占用等指标对整个系统的影响以及模型的安全等一系列的问题。

\sphinxAtStartPar
本章将主要介绍机器学习模型部署的主要流程，包括训练模型到推理模型的转换、适应硬件限制的模型压缩技术、模型推理及性能优化以及模型的安全保护，最后我们会给出一个模型部署端到端的实践用例。

\sphinxAtStartPar
本章的学习目标包括：
\begin{itemize}
\item {} 
\sphinxAtStartPar
了解训练模型到推理模型转换及优化

\item {} 
\sphinxAtStartPar
掌握模型压缩的常用方法：量化、稀疏和知识蒸馏

\item {} 
\sphinxAtStartPar
掌握模型推理的流程及常用的性能优化的技术

\item {} 
\sphinxAtStartPar
了解模型安全保护的常用方法

\end{itemize}


\section{概述}
\label{\detokenize{chapter_model_deployment/model_deployment_introduction:id1}}\label{\detokenize{chapter_model_deployment/model_deployment_introduction::doc}}
\sphinxAtStartPar
模型完成训练后，需要将模型及参数持久化成文件，不同的训练框架导出的模型文件中存储的数据结构不同，这给模型的推理系统带来了不便。推理系统为了支持不同的训练框架的模型，需要将模型文件中的数据转换成统一的数据结构。此外，在训练模型转换成推理模型的过程中，需要进行一些如算子融合、常量折叠等模型的优化以提升推理的性能。

\sphinxAtStartPar
推理模型部署到不同的场景，需要满足不同的硬件设备的限制，例如，在具有强大算力的计算中心或数据中心的服务器上可以部署大规模的模型，而在边缘侧服务器、个人电脑以及智能手机上算力和内存则相对有限，部署的模型的规模就相应地要降低。在超低功耗的微控制器上，则只能部署非常简单的机器学习模型。此外，不同硬件对于不同数据类型（如FP32、FP16、BF16、INT8等）的支持程度也不相同。为了满足这些硬件的限制，在有些场景下需要对训练好的模型进行压缩，降低模型的复杂度或者数据的精度，减少模型的参数，以适应硬件的限制。

\sphinxAtStartPar
模型部署到运行环境中执行推理，推理的时延、内存占用、功耗等是影响用户使用的关键因素，优化模型推理的方式有两种，一是设计专有的机器学习的芯片，相对于通用的计算芯片，这些专有芯片一般在能效比上具有很大的优势。二是通过软硬协同最大程度地发挥硬件的能力。对于第二种方式，以CPU为例，如何切分数据块以满足cache大小，如何对数据进行重排以便计算时可以连续访问，如何减少计算时的数据依赖以提升硬件流水线的并行，如何使用扩展指令集以提升计算性能，这些都需要针对不同的CPU架构进行设计和优化。

\sphinxAtStartPar
对于一个企业来讲，模型是属于重要的资产，因此，在模型部署到运行环境以后，保护模型的安全至关重要。本章节会介绍如模型混淆等一些常见的机器学习模型的安全保护手段。
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{模型压缩}

\end{itemize}

\sphinxAtStartPar
通过量化、剪枝等手段减小模型体积以及计算复杂度的技术，可以分为需要重训的压缩技术和不需要重训的压缩技术两类。
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{算子融合}

\end{itemize}

\sphinxAtStartPar
通过表达式简化、属性融合等方式将多个算子合并为一个算子的技术，融合可以降低模型的计算复杂度及模型的体积。
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{常量折叠}

\end{itemize}

\sphinxAtStartPar
将符合条件的算子在离线阶段提前完成前向计算，从而降低模型的计算复杂度和模型的体积。常量折叠的条件是算子的所有输入在离线阶段均为常量。
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{数据排布}

\end{itemize}

\sphinxAtStartPar
根据后端算子库支持程度和硬件限制，搜索网络中每层的最优数据排布格式，并进行数据重排或者插入数据重排算子，从而降低部署时的推理时延。
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{模型混淆}

\end{itemize}

\sphinxAtStartPar
对训练好的模型进行混淆操作，主要包括新增网络节点和分支、替换算子名的操作，攻击者即使窃取到混淆后的模型也不能理解原模型的结构。此外，混淆后的模型可以直接在部署环境中以混淆态执行，保证了模型在运行过程中的安全性。


\section{训练模型到推理模型的转换及优化}
\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:id1}}\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer::doc}}

\subsection{模型转换}
\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:id2}}
\sphinxAtStartPar
前面我们提到过，不同的训练框架（Tensorflow、PyTorch、MindSpore、MXNet、CNTK等）都定义了自己的模型的数据结构，推理系统需要将它们转换到统一的一种数据结构上。Open
Neural Network
Exchange(ONNX）正是为此目的而设计的。ONNX支持广泛的机器学习运算符集合，并提供了不同训练框架的转换器，例如TensorFlow模型到ONNX模型的转换器、PyTorch模型到ONNX模型的转换器等。
模型转换本质上是将模型这种结构化的数据，从一种数据结构转换为另一种数据结构的过程。进行模型转换首先要分析两种数据结构的异同点，然后针对结构相同的数据做搬运；对于结构相似的数据做一一映射；对于结构差异较大的数据则需要根据其语义做合理的数据转换；更进一步如果两种数据结构上存在不兼容，则模型转换无法进行。ONNX的一个优势就在于其强大的表达能力，从而大多数业界框架的模型都能够转换到ONNX的模型上来而不存在不兼容的情况.
模型可以抽象为一种图，从而模型的数据结构可以解构为以下两个要点：
\begin{itemize}
\item {} 
\sphinxAtStartPar
模型拓扑表达：从图的角度来说，就是图的边；从模型的角度来说，就是模型中的数据流和控制流等，模型数据流和控制流的定义又可以引申出子图的表达形式、模型输入输出的表达形式、控制流结构的表达形式等。比如Tensorflow1.x中的控制流表达为一种有环图，通过Enter、Exit、Switch、LoopCond、NextIteration等算子来解决成环，而ONNX通过Loop，If等算子来表达控制流，从而避免引入了有环，所以在将Tensorflow1.x的控制流模型转化为ONNX模型时，需要将Tensorflow模型中的控制流图结构融合成ONNX的While或者If算子。

\item {} 
\sphinxAtStartPar
算子原型定义：从图的角度来说，就是图的顶点；从模型角度来说，就是模型中的数据处理节点或者控制流节点。算子原型包括但不限于算子类型、算子输入输出的定义、算子属性的定义等。比如Caffe的slice算子和ONNX的slice算子的语义其实是不一致的，Caffe的slice算子应该映射到ONNX的Split算子，所以在将Caffe模型转换成ONNX模型时，需要将Caffe的Slice算子映射到ONNX的Split算子。比如Tensorflow中的中的FusedBatchNorm算子在Caffe中找不到相同语义的算子，需要将Caffe的BatchNorm算子和Scale算子组合起来才能表达相同的语义。
通常模型转换的过程也就是转换模型中的拓扑关系和映射模型中的算子原型。

\end{itemize}

\sphinxAtStartPar
在完成模型转换之后，通常地，我们会将一些不依赖于输入的工作提前去完成。这些工作包括了如常量折叠、算子融合、算子替换、算子重排等一些优化手段。这些优化手段的概念在前面的章节其实已经提及到，比如在编译器前端阶段，通常也会做常量折叠；在编译器后端阶段，通常会根据后端的硬件支持程度，对算子进行融合和拆分。但是有些优化工作只有在部署阶段才能进行或者彻底进行。


\subsection{算子融合}
\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-sec-fusion}}\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:id3}}
\sphinxAtStartPar
算子融合，就是将深度神经网络模型中的多个算子，按照一定的规则，合并成一个新的算子。通过算子融合，可以减少模型在线推理时的计算量、访存开销，从而降低推理时的时延和功耗。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=150\sphinxpxdimen]{{storage}.png}
\caption{计算机分层存储架构}\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:id6}}\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-storage}}\end{figure}

\sphinxAtStartPar
算子融合带来的性能上的收益主要来自两个方面，一是通过融合，充分利用寄存器和缓存，避免多个算子运算时，数据在CPU和内存之间的存储和读取的耗时。如
\hyperref[\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-storage}]{图\ref{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-storage}}}，可以看到计算机的储存系统，从最靠近cpu的寄存器L1、L2等多级缓存，到内存、硬盘，其存储的容量越来越大，但读取数据的耗时也越来越大。融合后，前一次计算的结果可以先暂存在CPU的寄存器(Register)或者缓存（Cache）中，下一次计算直接从寄存器或者缓存中读取，减少了内存读写的IO次数。二是通过融合，可以将一些计算量提前完成，避免了前向推理时的冗余计算或者循环冗余计算。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{conv-bn-fusion}.png}
\caption{Convolution + Batchnorm算子融合}\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:id7}}\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-tab-conv-bn-fusion}}\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-conv-bn-fusion}}\end{figure}

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-conv-bn-fusion}]{图\ref{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-conv-bn-fusion}}}，我们以Convolution算子和Batchnorm算子的融合为例，阐述算子融合的基本原理，图中蓝色框表示算子，黄色框表示融合后新增或者改变的算子，白色框表示算子中的权重或者常数张量。其融合的过程是一个计算表达式简化的过程，Convolution算子的计算过程可以等效为一个矩阵乘，其公式可以表达为
\eqref{equation:chapter_model_deployment/model_converter_and_optimizer:ch08-equ-conv_equation}。
\begin{equation}\label{equation:chapter_model_deployment/model_converter_and_optimizer:ch08-equ-conv_equation}
\begin{split}\pmb{Y_{conv}}=\pmb{W_{conv}}*\pmb{X_{conv}}+\pmb{B_{conv}}\end{split}
\end{equation}
\sphinxAtStartPar
这里我们不需要理解公式
\eqref{equation:chapter_model_deployment/model_converter_and_optimizer:ch08-equ-conv_equation}中每个变量的含义，只需要注意到一点，该公式是\(\pmb{Y_{conv}}\)关于\(\pmb{X_{conv}}\)的，其他符号均表示常量。

\sphinxAtStartPar
Batchnorm算子的计算过程如公式 \eqref{equation:chapter_model_deployment/model_converter_and_optimizer:ch08-equ-bn_equation}所示。
\begin{equation}\label{equation:chapter_model_deployment/model_converter_and_optimizer:ch08-equ-bn_equation}
\begin{split}\pmb{Y_{bn}}=\gamma\frac{\pmb{X_{bn}}-\mu_{\mathcal{B}}}{\sqrt{{\sigma_{\mathcal{B}}}^{2}+\epsilon}}+\beta\end{split}
\end{equation}
\sphinxAtStartPar
同样，这里我们不需要理解batchnorm中的所有参数的含义，只需要了解公式
\eqref{equation:chapter_model_deployment/model_converter_and_optimizer:ch08-equ-bn_equation}是\(\pmb{Y_{bn}}\)关于\(\pmb{X_{bn}}\)的，其他符号均表示常量。

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-conv-bn-fusion}]{图\ref{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-conv-bn-fusion}}}，当Convlution算子的输出作为Batchnorm输入时，最终Batchnorm算子的计算公式也就是要求\(\pmb{Y_{bn}}\)关于\(\pmb{X_{conv}}\)的计算公式，我们将\(\pmb{Y_{conv}}\)代入到\(\pmb{X_{bn}}\)，然后将常数项合并提取后，可以得到公式
:eqref:\sphinxcode{\sphinxupquote{equ:conv\sphinxhyphen{}bn\sphinxhyphen{}equation\sphinxhyphen{}3}}。
\begin{equation}\label{equation:chapter_model_deployment/model_converter_and_optimizer:ch08-equ-conv_bn_equation_3}
\begin{split}\pmb{Y_{bn}}=\pmb{A}*\pmb{X_{conv}}+\pmb{B}\end{split}
\end{equation}
\sphinxAtStartPar
其中\(\pmb{A}\)和\(\pmb{B}\)为两个矩阵。可以看到,公式
\eqref{equation:chapter_model_deployment/model_converter_and_optimizer:ch08-equ-conv_bn_equation_3}其实就是一个Convolution的计算公式。这个结果表明，在模型部署时，我们可以将Convolution和Batchnorm两个算子的计算等价为一个Convolution算子。我们将上述以计算公式的合并和简化为基础的算子融合称为计算公式融合。

\sphinxAtStartPar
在Convolution算子和Batchnorm算子融合的前后，网络结构相当于减少了一个Batchnorm算子，相应的网络中的参数量和网络所需的计算量都减少了；同时由于算子数量的减少，访存次数也相应地减少了。综合来看，该融合Pattern优化了模型部署时的功耗、性能，同时对于模型的体积大小也有少许收益。

\sphinxAtStartPar
在融合过程中，Convolution计算公式和Batchnorm计算公式中被认为是常量的符号在训练时均为参数，并不是常量。训练阶段如果进行该融合会导致模型参数的缺失。从该融合Pattern的结果来看，融合后网络中减少了一个Batchnorm算子，减少了一个Batchnorm算子的参数量，其实就是改变了深度神经网络的算法，会影响到网络的准确率，这是不可接受的。所以Convolution算子与Batchnorm算子的融合一般是在部署阶段特有的一种优化手段，其优化效果我们以MinsSpore
Lite为例，构造了包含一个Convolution和一个Batchnorm的sample网络，分别以样例网络和mobilenet\sphinxhyphen{}v2网络为例，在华为Mate30手机上，以两线程运行模型推理，取3000轮推理的平均时耗作为模型推理性能的指标，对比融合前后该指标的变化。从表
\hyperref[\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-tab-conv-bn-fusion}]{图\ref{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-tab-conv-bn-fusion}}}可以看到，对于sample网络和mobilenet\sphinxhyphen{}v2网络，融合后分别获得了8.5\%和11.7\%的推理性能提升，这个性能提升非常可观。并且这个性能提升没有带来任何的副作用，也没有对于硬件或算子库的提出额外要求。


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Convolution + Batchnorm融合前后推理性能（单位：ms）}\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:id8}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
网络
&\sphinxstyletheadfamily 
\sphinxAtStartPar
sample
&\sphinxstyletheadfamily 
\sphinxAtStartPar
mobilenet\sphinxhyphen{}v2
\\
\hline
\sphinxAtStartPar
融合前
&
\sphinxAtStartPar
0.035
&
\sphinxAtStartPar
15.415
\\
\hline
\sphinxAtStartPar
融合后
&
\sphinxAtStartPar
0.031
&
\sphinxAtStartPar
13.606
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{算子替换}
\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:id4}}
\sphinxAtStartPar
算子替换，即将模型中某些算子替换计算逻辑一致但对于在线部署更友好的算子。算子替换的原理是通过合并同类项、提取公因式等数学方法，将算子的计算公式加以简化，并将简化后的计算公式映射到某类算子上。算子替换可以达到降低计算量、降低模型大小的效果。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{bn-replace}.png}
\caption{Batchnorm算子替换}\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:id9}}\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-bn-replace}}\end{figure}

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-bn-replace}]{图\ref{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-bn-replace}}}，我们以Batchnorm算子替换成Scale算子为例，阐述算子替换的原理。我们直接将Batchnorm的计算公式
\eqref{equation:chapter_model_deployment/model_converter_and_optimizer:ch08-equ-bn_equation}进行分解，并将常量合并简化，Batchnorm的计算公式可以写成：
\begin{equation}\label{equation:chapter_model_deployment/model_converter_and_optimizer:ch08-equ-replace_scale}
\begin{split}\pmb{Y_{bn}}=scale*\pmb{X_{bn}}+offset\end{split}
\end{equation}
\sphinxAtStartPar
其中scale和offset为两个标量。可以看到，计算公式简化后，我们可以将其映射到一个Scale算子。

\sphinxAtStartPar
在Batchnorm算子被替换为Scale算子的前后，网络中的参数量、计算量都减少了，该算子替换策略可以优化模型部署时的功耗和性能。同理，该算子替换优化策略只能在部署阶段才能进行，因为一方面在部署阶段Batchnorm计算公式中被认为是常量的符号，在训练时是参数并非常量。另一方面该优化策略会降低模型的参数量，改变模型的结构，降低模型的表达能力，影响训练收敛时模型的准确率。


\subsection{算子重排}
\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:id5}}
\sphinxAtStartPar
算子重排是指将模型中算子的拓扑序按照某些规则进行重新排布，在不降低模型的推理精度的前提下，降低模型推理的计算量。常用的算子重排技术有针对于Slice算子、StrideSlice算子、Crop算子等裁切类算子的前移、Reshape算子和Transpose算子的重排、BinaryOp算子的重排等。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{crop-reorder}.png}
\caption{Crop算子重排}\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:id10}}\label{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-crop-reorder}}\end{figure}

\sphinxAtStartPar
如 \hyperref[\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-crop-reorder}]{图\ref{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-crop-reorder}}}，Crop算子是从输入的feature
map中裁取一部分作为输出，经过Crop算子后，feature
map的size就降低了。如果我们将这个裁切的过程前移，提前对feature
map进行裁切，那么后续算子的计算量也会相应地减少，从而提高模型部署时的推理性能。Crop算子前移带来的性能提升跟Crop算子的参数有关。但是Crop算子一般只能沿着element
wise类算子前移。

\sphinxAtStartPar
通过前面的实验数据我们可以看到，通过推理前的模型优化，可以为推理的时延、功耗、内存占用带来极大的收益。


\section{模型压缩}
\label{\detokenize{chapter_model_deployment/model_compression:ch08-sec-model-compression}}\label{\detokenize{chapter_model_deployment/model_compression:id1}}\label{\detokenize{chapter_model_deployment/model_compression::doc}}
\sphinxAtStartPar
在上一小节中，我们简要介绍了模型转换的目的，并重点讲述了模型部署时的一些常用的模型优化手段。考虑到不同场景的硬件对模型的要求不同，比如部署在手机上，对于模型的大小比较敏感，一般在兆级别。因此，对于一些较大的模型，我们往往需要通过一些模型压缩的技术，使其能满足不同计算硬件的要求。


\subsection{量化}
\label{\detokenize{chapter_model_deployment/model_compression:id2}}
\sphinxAtStartPar
模型量化是指以较低的推理精度损失将连续取值（通常为FP32或者大量可能的离散值）的浮点型权重或者通过各个算子的数据定点近似（通常为INT8）为有限多个离散值的过程，如
\hyperref[\detokenize{chapter_model_deployment/model_compression:ch08-fig-quant-minmax}]{图\ref{\detokenize{chapter_model_deployment/model_compression:ch08-fig-quant-minmax}}}，T是量化前的数据范围。通过以更少的位数表示浮点数据，模型量化可以减少模型尺寸，进而减少在推理时的内存消耗，并且在一些低精度运算较快的处理器上可以增加推理速度。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{quant-minmax}.png}
\caption{量化原理}\label{\detokenize{chapter_model_deployment/model_compression:id10}}\label{\detokenize{chapter_model_deployment/model_compression:ch08-fig-quant-minmax}}\end{figure}

\sphinxAtStartPar
计算机中不同数据类型的占用比特数及其表示的数据范围各不相同。可以根据实际业务需求将原模型量化成不同比特数的模型，一般深度神经网络的模型用单精度浮点数表示，如果能用有符号整数来近似原模型的参数，那么被量化的权重参数存储大小就可以降到原先的四分之一，用来量化的比特数越少，量化后的模型压缩率越高。工业界目前最常用的量化位数是8比特，低于8比特的量化被称为低比特量化。1比特是模型压缩的极限，可以将模型压缩为1/32，在推理时也可以使用高效的XNOR和BitCount位运算来提升推理速度。

\sphinxAtStartPar
另外，根据量化数据表示的原始数据范围是否均匀，还可以将量化方法分为线性量化和非线性量化。实际的深度神经网络的权重和激活值通常是不均匀的，因此理论上使用非线性量化能够达到更高的精度，但在实际推理中非线性量化的计算复杂度较高，通常使用线性量化。下面着重介绍线性量化的原理。

\sphinxAtStartPar
假设r表示量化前的浮点数，量化后的整数q可以表示为：
\begin{equation}\label{equation:chapter_model_deployment/model_compression:chapter_model_deployment/model_compression:0}
\begin{split}q=clip(round(\frac{r}{s}+z),q_{min},q_{max})\end{split}
\end{equation}
\sphinxAtStartPar
\(round(\cdot)\)和\(clip(\cdot)\)分别表示取整和截断操作，\(q_{min}\)和\(q_{max}\)是量化后的最小值和最大值。\(s\)是数据量化的间隔，\(z\)是表示数据偏移的偏置，\(z\)为0的量化被称为对称（Symmetric）量化，不为0的量化称为非对称（Asymmetric）量化。对称量化可以避免量化算子在推理中计算z相关的部分，降低推理时的计算复杂度；非对称量化可以根据实际数据的分布确定最小值和最小值，可以更加充分的利用量化数据信息，使得计算精度更高。

\sphinxAtStartPar
根据量化参数\(s\)和\(z\)的共享范围，量化方法可以分为逐层量化和逐通道量化。逐层量化以一层网络为量化单位，每层网络的一组量化参数；逐通道量化以一层网络的每个量化通道为单位，每个通道单独使用一组量化参数。逐通道量化由于量化粒度更细，能获得更高的量化精度，但计算也更复杂。

\sphinxAtStartPar
根据量化过程中是否需要训练，可以将模型量化分为量化感知训练（Quantization
Aware Training, QAT）和训练后量化（Post Training Quantization,
PTQ）两种,其中感知量化训练是指在模型训练过程中加入伪量化算子，通过训练时统计输入输出的数据范围可以提升量化后模型的精度，适用于对模型精度要求较高的场景；训练后量化指对训练后的模型直接量化，只需要少量校准数据，适用于追求高易用性和缺乏训练资源的场景。


\subsubsection{量化感知训练}
\label{\detokenize{chapter_model_deployment/model_compression:id3}}
\sphinxAtStartPar
量化感知训练是在训练过程中模拟量化，利用伪量化节点将量化带来的精度变化计入训练误差，使得优化器能在训练过程中尽量减少量化误差，得到更高的模型精度。量化感知训练的具体流程如下：
\begin{itemize}
\item {} 
\sphinxAtStartPar
初始化：设置权重和激活值的范围\(q_{min}\)和\(q_{max}\)的初始值；

\item {} 
\sphinxAtStartPar
构建模拟量化网络：在需要量化的权重和激活值后插入伪量化节点；

\item {} 
\sphinxAtStartPar
量化训练：重复执行以下步骤直到网络收敛，计算量化网络层的权重和激活值的范围\(q_{min}\)和\(q_{max}\)，前向计算反向传播更新网络权重参数；

\item {} 
\sphinxAtStartPar
导出量化网络：获取\(q_{min}\)和\(q_{max}\)，并计算量化参数\(s\)和\(z\)；
根据公式计算权重的量化整数值，并替换对应网络层的参数和数据类型；
删除伪量化节点，在量化网络层前后分别插入量化和反量化算子。

\end{itemize}


\subsubsection{训练后量化}
\label{\detokenize{chapter_model_deployment/model_compression:id4}}
\sphinxAtStartPar
训练后量化也可以分成两种，权重量化和全量化。权重量化仅量化模型的权重以压缩模型的大小，在推理时将权重反量化为原始的FP32数据，后续推理流程与普通的FP32模型一致。权重量化的好处是不需要校准数据集，不需要实现量化算子，且模型的精度误差较小，由于实际推理使用的仍然是FP32算子，所以推理性能不会提高。全量化不仅会量化模型的权重，还会量化模型的激活值，在模型推理时执行量化算子来加快模型的推理速度。为了量化激活值，需要用户提供一定数量的校准数据集用于统计每一层激活值的分布，并对量化后的算子做校准。校准数据集可以来自训练数据集或者真实场景的输入数据，需要数量通常非常小。在做训练后量化时会以校准数据集为输入，执行推理流程然后统计每层激活值的数据分布并得到相应的量化参数，具体的操作流程如下：
\begin{itemize}
\item {} 
\sphinxAtStartPar
使用直方图统计的方式得到原始FP32数据的统计分布\(P_f\)；

\item {} 
\sphinxAtStartPar
在给定的搜索空间中选取若干个\(q_{min}\)和\(q_{max}\)分别对激活值量化，得到量化后的数据\(Q_q\)；

\item {} 
\sphinxAtStartPar
使用直方图统计得到\(Q_q\)的统计分布;

\item {} 
\sphinxAtStartPar
计算每个\(Q_q\)与\(P_f\)的统计分布差异，并找到差异性最低的一个对应的\(q_{min}\)和\(q_{max}\)来计算相应的量化参数，常见的用于度量分布差异的指标包括KL散度(Kullback\sphinxhyphen{}Leibler
Divergence)、对称KL散度(Symmetric Kullback\sphinxhyphen{}Leibler
Divergence)和JS散度(Jenson\sphinxhyphen{}Shannon Divergence)。

\end{itemize}

\sphinxAtStartPar
除此之外，由于量化存在固有误差，还需要校正量化误差。以矩阵乘为例，\(a=\sum_{i=1}^Nw_ix_i+b\)，w表示权重，x表示激活值，b表示偏置。首先需要对量化的均值做校正，对fp32算子和量化算子输出的每个通道求平均，假设某个通道i的fp32算子输出均值为\(a_i\)，量化算子反量化输出均值为\(a_{qi}\)，将这个通道两个均值的差\(a_i-a_q\)加到对应的通道上即可使得最终的输出均值和fp32一致。另外还需要保证量化后的分布和量化前是一致的，设某个通道权重数据的均值、方差为\(E(w_c)\)、\(||w_c-E(w_c)||\)，量化后的均值和方差为\(E(\hat{w_c})\)、\(||\hat{w_c}-E(\hat{w_c})||\)，对权重做如下校正：
\begin{equation}\label{equation:chapter_model_deployment/model_compression:chapter_model_deployment/model_compression:1}
\begin{split}\hat{w_c}\leftarrow\zeta_c(\hat{w_c}+u_c)\end{split}
\end{equation}\begin{equation}\label{equation:chapter_model_deployment/model_compression:chapter_model_deployment/model_compression:2}
\begin{split}u_c=E(w_c)-E(\hat{w_c})\end{split}
\end{equation}\begin{equation}\label{equation:chapter_model_deployment/model_compression:chapter_model_deployment/model_compression:3}
\begin{split}\zeta_c=\frac{||w_c-E(w_c)||}{||\hat{w_c}-E(\hat{w_c})||}\end{split}
\end{equation}
\sphinxAtStartPar
量化方法作为一种通用的模型压缩方法，可以大幅提升神经网络存储和压缩的效率，已经取得了广泛的应用。


\subsection{模型稀疏}
\label{\detokenize{chapter_model_deployment/model_compression:id5}}
\sphinxAtStartPar
模型稀疏是通过去除神经网络中部分组件（如权重、特征图、卷积核）降低网络的存储和计算代价，它和模型权重量化、权重共享、池化等方法一样，属于一种为达到降低模型计算复杂度的目标而引入的一种强归纳偏置。


\subsubsection{模型稀疏的动机}
\label{\detokenize{chapter_model_deployment/model_compression:id6}}
\sphinxAtStartPar
因为卷积神经网络中的卷积计算可以被看作输入数据和卷积核中权重的加权线性组合，所以细小的权重对输出数据就具有相对较小的影响。对模型进行稀疏操作的合理性主要来源于两方面的假设：
\begin{itemize}
\item {} 
\sphinxAtStartPar
其一，针对权重参数来说，当前许多神经网络模型存在过参数化（Over\sphinxhyphen{}parameterized）的现象，动辄具有几千万甚至数亿规模的参数量。

\item {} 
\sphinxAtStartPar
其二，针对模型推理过程中生成的激活值特征图，对于许多检测、分类、分割等视觉任务来说激活值特征图中能利用的有效信息相对于整张图仅占较小的比例。

\end{itemize}

\sphinxAtStartPar
根据以上描述按照模型稀疏性来源的不同，主要分为权重稀疏和激活值稀疏，它们的目的都是为了减少模型当中的冗余成分来达到降低计算量和模型存储的需求。具体来说，对模型进行稀疏就是根据模型的连接强弱程度（一般根据权重或激活的绝对值大小），对一些强度较弱的连接进行剪枝（将权重参数或激活值置为0）来达到模型稀疏并提高模型推理性能的目的。特别地，我们将模型权重或激活值张量中0值所占的比例称为模型稀疏度。一般而言，模型稀疏度越高带来的模型准确率下降越大，因此我们的目标是尽可能在提高模型稀疏度的同时保证模型准确率下降较小。

\sphinxAtStartPar
实际上，如同神经网络本身的发明受到了神经生物学启发一样，神经网络模型稀疏方法同样受到了神经生物学的启发。在一些神经生物学的发现中，人类以及大多数哺乳动物的大脑都会出现一种叫做突触修剪的活动。突触修剪即神经元的轴突和树突发生衰退和完全死亡，这一活动发生在哺乳动物的婴幼儿时期，然后一直持续到成年以后。这种突触修剪机制不断简化和重构哺乳动物大脑的神经元连接，使得哺乳动物的大脑能以更低的能量获得更高效的工作方式。


\subsubsection{结构与非结构化稀疏}
\label{\detokenize{chapter_model_deployment/model_compression:id7}}
\sphinxAtStartPar
首先我们考虑权重稀疏，对于权重稀疏来说，按照稀疏模式的不同，主要分为结构化和非结构化稀疏。简单来讲，结构化稀疏就是在通道或者卷积核层面对模型进行剪枝。这种稀疏方式能够得到规则且规模更小的权重矩阵，因此比较适合CPU和GPU进行加速计算。但与此同时，结构化稀疏是一种粗粒度的稀疏方式，将会对模型的推理准确率造成较大的下降。

\sphinxAtStartPar
而非结构化稀疏，可以对权重张量中任意位置的权重进行裁剪，因此这种稀疏方式属于细粒度的稀疏。这种稀疏方式相对于结构化稀疏，造成的模型准确率下降较小。但是也正是因为这种不规则的稀疏方式，导致稀疏后的模型难以利用硬件获得较高的加速比。其背后原因主要有以下几点：
\begin{itemize}
\item {} 
\sphinxAtStartPar
不规则排布的模型权重矩阵会带来大量的控制流指令，比如由于大量0值的存在，我们会不可避免地引入大量if\sphinxhyphen{}else分支判断指令，因此会降低指令层面的并行度。

\item {} 
\sphinxAtStartPar
权重矩阵的不规则内存排布会造成线程发散和负载不均衡，而不同卷积核往往是利用多线程进行计算的，因此这也影响了线程层面的并行度。

\item {} 
\sphinxAtStartPar
权重矩阵的不规则内存排布造成了较低的访存效率，因为它降低了数据的局部性以及缓存命中率。

\end{itemize}

\sphinxAtStartPar
为了解决以上非结构化稀疏带来的种种问题，近期出现的研究当中通过引入特定稀疏模式将结构化稀疏和非结构化稀疏结合了起来，从而一定程度上兼具结构化和非结构化稀疏的优点并克服了两者的缺点。


\subsubsection{稀疏策略}
\label{\detokenize{chapter_model_deployment/model_compression:id8}}
\sphinxAtStartPar
明确了模型稀疏的对象之后，我们需要确定模型稀疏的具体策略，具体来说我们需要决定何时对模型进行稀疏以及如何对模型进行稀疏。目前最常见模型稀疏的一般流程为：预训练、剪枝、微调。具体而言，我们首先需要训练得到一个收敛的稠密模型，然后在此基础上进行稀疏和微调。选择在预训练之后进行稀疏动作的原因基于这样一个共识，即预训练模型的参数蕴含了学习到的知识，继承这些知识然后进行稀疏得到的模型效果要比从头开始训练好。除了基于预训练模型进行进一步修剪之外，训练和剪枝交替进行也是一种常用的策略。相比于一步修剪的方法，这种逐步的修剪方式，使得训练和剪枝紧密结合，可以更有效地发现冗余的卷积核，被广泛采用于现代神经网络剪枝方法中。

\sphinxAtStartPar
以下通过一个具体实例(Deep Compression({[}@han2015deep{]})）
来说明如何进行网络修剪：如
\hyperref[\detokenize{chapter_model_deployment/model_compression:ch08-fig-deepcomp}]{图\ref{\detokenize{chapter_model_deployment/model_compression:ch08-fig-deepcomp}}}所示，在去掉大部分的权值之后，深度卷积神经网络的精度将会低于其原始的精度。对剪枝后稀疏的神经网络进行微调，可以进一步提升压缩后网络的精度。剪枝后的模型可以进一步进行量化，使用更低比特的数据来表示权值；此外，结合霍夫曼（Huffman）编码可以进一步地降低深度神经网络的存储。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{deepcomp}.png}
\caption{Deep Compression({[}@han2015deep{]})}\label{\detokenize{chapter_model_deployment/model_compression:id11}}\label{\detokenize{chapter_model_deployment/model_compression:ch08-fig-deepcomp}}\end{figure}

\sphinxAtStartPar
除了直接去除冗余的神经元之外，基于字典学习的方法也可以用来去掉深度卷积神经网络中无用的权值({[}@bagherinezhad2017lcnn{]})。通过学习一系列卷积核的基，可以把原始卷积核变换到系数域上并且它们稀疏。比如，Bagherinezhad等人({[}@bagherinezhad2017lcnn{]})将原始卷积核分解成卷积核的基和稀疏系数的加权线性组合。


\subsection{知识蒸馏}
\label{\detokenize{chapter_model_deployment/model_compression:id9}}
\sphinxAtStartPar
知识蒸馏，也被称为教师\sphinxhyphen{}学生神经网络学习算法，已经受到业界越来越多的关注。大型深度模型在实践中往往会获得良好的性能，因为当考虑新数据时，过度参数化会提高泛化性能。在知识蒸馏中，小模型（学生模型）通常是由一个大模型（教师模型）监督，算法的关键问题是如何从老师模型转换的知识传授给学生模型。通过把一个全新的更深的更窄结构的深度神经网络当作学生神经网络，然后把一个预先训练好的神经网络模型当作教师神经网络。利用这个教师神经网络模型来帮助学生神经网络模型的算法是当下的一个研究热点。

\sphinxAtStartPar
Hinton等人({[}@Distill{]})首先提出了教师神经网络\sphinxhyphen{}学生神经网络学习框架，通过最小化两个神经网络之间的差异来学习一个更窄更深的神经网络。记教师神经网络为\(\mathcal{N}_{T}\)，它的参数为\(\theta_T\)，同时记学生神经网络为\(\mathcal{N}_{S}\)，相应的参数为\(\theta_S\)。一般而言，学生神经网络相较于教师神经网络具有更少的参数。

\sphinxAtStartPar
文献({[}@Distill{]})提出的知识蒸馏（knowledge
distillation，KD）方法，同时令学生神经网络的分类结果接近真实标签并且令学生神经网络的分类结果接近于教师神经网络的分类结果，即，
\begin{equation}\label{equation:chapter_model_deployment/model_compression:ch08-equ-c2Fcn_distill}
\begin{split}\mathcal{L}_{KD}(\theta_S) = \mathcal{H}(o_S,\mathbf{y}) +\lambda\mathcal{H}(\tau(o_S),\tau(o_T))\end{split}
\end{equation}
\sphinxAtStartPar
其中，\(\mathcal{H}(\cdot,\cdot)\)是交叉熵函数，\(o_S\)和\(o_T\)分别是学生网络和教师网络的输出，\(\mathbf{y}\)是标签。公式
\eqref{equation:chapter_model_deployment/model_compression:ch08-equ-c2Fcn_distill}中的第一项使得学生神经网络的分类结果接近预期的真实标签，而第二项的目的是提取教师神经网络中的有用信息并传递给学生神经网络，\(\lambda\)是一个权值参数用来平衡两个目标函数。\(\tau(\cdot)\)是一个软化（soften）函数，将网络输出变得更加平滑。

\sphinxAtStartPar
公式
\eqref{equation:chapter_model_deployment/model_compression:ch08-equ-c2Fcn_distill}仅仅从教师神经网络分类器输出的数据中提取有价值的信息，并没有从其它中间层去将教师神经网络的信息进行挖掘。因此，Romero等人{[}@FitNet{]}）进一步地开发了一种学习轻型学生神经网络的方法，该算法可以从教师神经网络中任意的一层来传递有用的信息给学生神经网络。此外，事实上，并不是所有的输入数据对卷积神经网络的计算和完成后续的任务都是有用的。例如，在一张包含一个动物的图像中，对分类和识别结果比较重要的是动物所在的区域，而不是那些无用的背景信息。所以，有选择性地从教师神经网络的特征图中提取信息是一个更高效的方式。于是，Zagoruyko和Komodakis（{[}@attentionTS{]}）提出了一种基于感知（attention）损失函数的学习方法来提升学生神经网络的性能，公式
\eqref{equation:chapter_model_deployment/model_compression:ch08-equ-c2Fcn_distill}仅仅从教师神经网络分类器输出的数据中提取有价值的信息，并没有从其它中间层去将教师神经网络的信息进行挖掘。因此，Romero等人{[}@FitNet{]}）进一步地开发了一种学习轻型学生神经网络的方法，该算法可以从教师神经网络中任意的一层来传递有用的信息给学生神经网络。此外，事实上，并不是所有的输入数据对卷积神经网络的计算和完成后续的任务都是有用的。例如，在一张包含一个动物的图像中，对分类和识别结果比较重要的是动物所在的区域，而不是那些无用的背景信息。所以，有选择性地从教师神经网络的特征图中提取信息是一个更高效的方式。于是，Zagoruyko和Komodakis（{[}@attentionTS{]}）提出了一种基于感知（attention）损失函数的学习方法来提升学生神经网络的性能，如
\hyperref[\detokenize{chapter_model_deployment/model_compression:ch08-fig-distillation}]{图\ref{\detokenize{chapter_model_deployment/model_compression:ch08-fig-distillation}}}所示。该算法在学习学生神经网络的过程中，引入了感知模块Attention，选择性地将教师神经网络中的信息传递给学生神经网络，并帮助其进行训练。感知图可以识别输入图像不同位置对最终分类结果的重要性，并从教师网络传递到学生网络。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{distillation}.png}
\caption{一种基于感知（attention）的教师神经网络\sphinxhyphen{}学生神经网络学习算法}\label{\detokenize{chapter_model_deployment/model_compression:id12}}\label{\detokenize{chapter_model_deployment/model_compression:ch08-fig-distillation}}\end{figure}

\sphinxAtStartPar
知识蒸馏是一种有效的帮助小网络优化的方法，能够进一步和剪枝、量化等其他压缩方法结合，训练得到精度高、计算量小的高效模型。


\section{模型推理}
\label{\detokenize{chapter_model_deployment/model_inference:id1}}\label{\detokenize{chapter_model_deployment/model_inference::doc}}
\sphinxAtStartPar
训练模型经过前面的转换、压缩等流程后，需要部署在计算硬件上进行推理。执行推理主要包含以下步骤：
\begin{itemize}
\item {} 
\sphinxAtStartPar
前处理：将原始数据处理成适合网络输入的数据。

\item {} 
\sphinxAtStartPar
执行推理：将离线转换得到的模型部署到设备上执行推理流程，根据输入数据计算得到输出数据。

\item {} 
\sphinxAtStartPar
后处理：模型的输出结果做进一步的加工处理，如筛选阈值。

\end{itemize}


\subsection{前处理与后处理}
\label{\detokenize{chapter_model_deployment/model_inference:id2}}

\subsubsection{前处理}
\label{\detokenize{chapter_model_deployment/model_inference:id3}}
\sphinxAtStartPar
前处理主要完成数据预处理，在现实问题中，我们得到的原始数据往往非常混乱，机器学习模型无法识别并从中提取信息。数据预处理的目的是将原始数据例如图片、语音、文本等，处理成适合网络输入的tensor数据，并消除其中无关的信息，恢复有用的真实信息，增强有关信息的可检测性，最大限度地简化数据，从而改进模型的特征抽取、图像分割、匹配和识别等可靠性。

\sphinxAtStartPar
常见的数据预处理手段有：
\begin{itemize}
\item {} 
\sphinxAtStartPar
特征编码：将描述特征的原始数据编码成数字，输入给机器学习模型，因为它们只能处理数字数据。常见的编码方法有：离散化、序号编码、One\sphinxhyphen{}hot编码，二进制编码等；

\item {} 
\sphinxAtStartPar
数据归一化：修改数据的值使其达到共同的标度但不改变它们之间的相关性，消除数据指标之间的量纲影响。常用的技术有：Min\sphinxhyphen{}Max归一化将数据缩放到给定范围，Z\sphinxhyphen{}score归一化使数据符合正态分布；

\item {} 
\sphinxAtStartPar
处理离群值:
离群值是与数据中的其他值保持一定距离的数据点，适当地排除离群值可以提升模型的准确性。

\end{itemize}


\subsubsection{后处理}
\label{\detokenize{chapter_model_deployment/model_inference:id4}}
\sphinxAtStartPar
通常，模型推理结束后，需要把推理的输出数据传递给用户完成后处理，常见的数据后处理手段有：
\begin{itemize}
\item {} 
\sphinxAtStartPar
连续数据离散化：模型实际用于预测离散数据，例如商品数量时，用回归模型预测得到的是连续值，需要四取五入、取上下限阈值等得到实际结果；

\item {} 
\sphinxAtStartPar
数据可视化：将数据图形化、表格化，便于找到数据之间的关系，来决定下一步的分析策略；

\item {} 
\sphinxAtStartPar
手动拉宽预测范围：回归模型往往预测不出很大或很小的值，结果都集中在中部区域。例如医院的化验数据，通常是要根据异常值诊断疾病。手动拉宽预测范围，将偏离正常范围的值乘一个系数，可以放大两侧的数据，得到更准确的预测结果。

\end{itemize}


\subsection{并行计算}
\label{\detokenize{chapter_model_deployment/model_inference:ch08-sec-parallel-inference}}\label{\detokenize{chapter_model_deployment/model_inference:id5}}
\sphinxAtStartPar
为提升推理的性能，需要重复利用多核的能力，所以一般推理框架会引入多线程机制。主要的思路是将算子的输入数据进行切分，通过多线程去执行不同数据切片，实现算子并行计算，从而成倍提升算子计算性能。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{parallel}.png}
\caption{矩阵乘数据切分}\label{\detokenize{chapter_model_deployment/model_inference:id9}}\label{\detokenize{chapter_model_deployment/model_inference:ch08-fig-parallel}}\end{figure}

\sphinxAtStartPar
如图所示，对于矩阵乘可以按左矩阵的行进行切分，可以利用三个线程分别计算A1*B，A2*B，A3*B，实现矩阵乘多线程并行计算。

\sphinxAtStartPar
为方便算子并行计算，同时避免频繁创建销毁线程的开销，推理框架一般会使用线程池机制。业界有两种较为通用的做法：
\begin{itemize}
\item {} 
\sphinxAtStartPar
使用OpenMp编程接口：OpenMP（Open
Multi\sphinxhyphen{}Processing）是一套支持跨平台共享内存方式的多线程并发的编程API，如算子并行最常用的接口”parallel
for”，实现for循环体的代码被多线程并行执行。

\item {} 
\sphinxAtStartPar
推理框架实现针对算子并行计算的线程池，相对OpenMp提供的接口会更有针对性，性能会更高，且更轻量。

\end{itemize}


\subsection{算子优化}
\label{\detokenize{chapter_model_deployment/model_inference:ch08-sec-kernel-optimization}}\label{\detokenize{chapter_model_deployment/model_inference:id6}}
\sphinxAtStartPar
在部署AI模型时，我们期望模型执行训练或推理的时间尽可能地短，以获得更优越的性能。对于一个固定的深度学习网络，框架调度的时间占比往往很小，性能的瓶颈就在算子的执行。下面从硬件指令和算法角度介绍一些算子优化的方法。


\subsubsection{硬件指令优化}
\label{\detokenize{chapter_model_deployment/model_inference:id7}}
\sphinxAtStartPar
绝大多数的设备上都有CPU，因此算子在CPU上的时间尤为重要，下面介绍一下在ARM
CPU硬件指令优化的方法。
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
汇编语言

\end{enumerate}

\sphinxAtStartPar
开发者使用的C++、Java等高级编程语言会通过编译器输出为机器指令码序列，而高级编程语言能做的事通常受编译器所限，汇编语言是靠近机器的语言，可以一对一实现任何指令码序列，编写的程序存储空间占用少、执行速度快、效率优于高级编程语言。

\sphinxAtStartPar
在实际应用中，最好是程序的大部分用高级语言编写，运行性能要求很高的部分用汇编语言来编写，通过混合编程实现优势互补。深度学习的卷积、矩阵乘等算子涉及大量的计算，使用汇编语言能够给模型训练和推理性能带来数十到数百倍量级的提升。

\sphinxAtStartPar
下面以ARMv8系列处理器为例，介绍和硬件指令相关的优化。
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
寄存器与NEON指令

\end{enumerate}

\sphinxAtStartPar
ARMv8系列的CPU上有32个NEON寄存器v0\sphinxhyphen{}v31，如
\hyperref[\detokenize{chapter_model_deployment/model_inference:ch08-fig-register}]{图\ref{\detokenize{chapter_model_deployment/model_inference:ch08-fig-register}}}所示，NEON寄存器v0可存放128bit的数据，即4个float32，8个float16，16个int8等。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{register}.png}
\caption{ARMv8处理器NEON寄存器v0的结构}\label{\detokenize{chapter_model_deployment/model_inference:id10}}\label{\detokenize{chapter_model_deployment/model_inference:ch08-fig-register}}\end{figure}

\sphinxAtStartPar
针对该处理器，可以采用SIMD(Single Instruction，Multiple
Data，单指令、多数据)提升数据存取计算的速度。相比于单数据操作指令，NEON指令可以一次性操作NEON寄存器的多个数据。例如：对于浮点数的fmla指令，用法为fmla
v0.4s, v1.4s, v2.4s，如
\hyperref[\detokenize{chapter_model_deployment/model_inference:ch08-fig-fmla}]{图\ref{\detokenize{chapter_model_deployment/model_inference:ch08-fig-fmla}}}所示，用于将v1和v2两个寄存器中相对应的float值相乘累加到v0的值上。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{fmla}.png}
\caption{fmla指令计算功能}\label{\detokenize{chapter_model_deployment/model_inference:id11}}\label{\detokenize{chapter_model_deployment/model_inference:ch08-fig-fmla}}\end{figure}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{2}
\item {} 
\sphinxAtStartPar
汇编语言优化

\end{enumerate}

\sphinxAtStartPar
对于已知功能的汇编语言程序来说，计算类指令通常是固定的，性能的瓶颈就在非计算指令上。如
\hyperref[\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-storage}]{图\ref{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-fig-storage}}}所示，计算机各存储设备类似于一个金字塔结构，最顶层空间最小，但是速度最快，最底层速度最慢，但是空间最大。L1\sphinxhyphen{}L3统称为cache(高速缓冲存储器)，CPU访问数据时，会首先访问位于CPU内部的cache，没找到再访问CPU之外的主存，此时引入了缓存命中率的概念来描述在cache中完成数据存取的占比。要想提升程序的性能，缓存命中率要尽可能的高。

\sphinxAtStartPar
下面简单列举一些提升缓存命中率、优化汇编性能的手段：

\sphinxAtStartPar
（1）循环展开：尽可能使用更多的寄存器，以代码体积换性能；

\sphinxAtStartPar
（2）指令重排：打乱不同执行单元的指令以提高流水线的利用率，提前有延迟的指令以减轻延迟，减少指令前后的数据依赖等；

\sphinxAtStartPar
（3）寄存器分块：合理分块NEON寄存器，减少寄存器空闲，增加寄存器复用；

\sphinxAtStartPar
（4）计算数据重排：尽量保证读写指令内存连续，提高缓存命中率；

\sphinxAtStartPar
（5）使用预取指令：将要使用到的数据从主存提前载入缓存，减少访问延迟。


\subsubsection{算法优化}
\label{\detokenize{chapter_model_deployment/model_inference:id8}}
\sphinxAtStartPar
多数AI模型的推理时间主要耗费在卷积、矩阵乘算子的计算上，占到了整网百分之九十甚至更多的时间。本小节主要介绍卷积算子算法方面的优化手段，可以应用到各种硬件设备上。
卷积的计算可以转换为两个矩阵相乘，在前述5.3.3小节中，已经详细介绍了矩阵乘GEMM运算的优化。对于不同的硬件，确定合适的矩阵分块，优化数据访存与指令并行，可以最大限度的发挥硬件的算力，提升推理性能。

\sphinxAtStartPar
1.Img2col

\sphinxAtStartPar
将卷积的计算转换为矩阵乘，一般采用Img2col的方法实现。在常见的神经网络中，卷积的输入通常都是4维的，默认采用的数据排布方式为NHWC，如
\hyperref[\detokenize{chapter_model_deployment/model_inference:ch08-fig-conv-nhwc}]{图\ref{\detokenize{chapter_model_deployment/model_inference:ch08-fig-conv-nhwc}}}所示，是一个卷积示意图。输入维度为（1,IH,IW,IC），卷积核维度为（OC,KH,KW,IC），输出维度为（1,OH,OW,OC）。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{conv_nhwc}.png}
\caption{通用卷积示意图}\label{\detokenize{chapter_model_deployment/model_inference:id12}}\label{\detokenize{chapter_model_deployment/model_inference:ch08-fig-conv-nhwc}}\end{figure}

\sphinxAtStartPar
对卷积的Img2col规则如下。如
\hyperref[\detokenize{chapter_model_deployment/model_inference:ch08-fig-img2col-input}]{图\ref{\detokenize{chapter_model_deployment/model_inference:ch08-fig-img2col-input}}}所示，对该输入做重排，得到的矩阵见右侧，行数对应输出的OH*OW的个数；每个行向量里，先排列计算一个输出点所需要输入上第一个通道的KH*KW个数据，再按次序排列之后的通道，直到通道IC。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{img2col_input}.png}
\caption{输入Img2col的矩阵}\label{\detokenize{chapter_model_deployment/model_inference:id13}}\label{\detokenize{chapter_model_deployment/model_inference:ch08-fig-img2col-input}}\end{figure}

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_model_deployment/model_inference:ch08-fig-img2col-weight}]{图\ref{\detokenize{chapter_model_deployment/model_inference:ch08-fig-img2col-weight}}}所示，对权重数据做重排。将1个卷积核展开为权重矩阵的一列，因此共有OC列，每个列向量上先排列第一个输入通道上KH*KW的数据，再依次排列后面的通道直到IC。通过重排，卷积的计算就可以转换为两个矩阵相乘的求解。在实际实现时，Img2col和GEMM的数据重排会同时进行，以节省运行时间。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{img2col_weight}.png}
\caption{卷积核Img2col的矩阵}\label{\detokenize{chapter_model_deployment/model_inference:id14}}\label{\detokenize{chapter_model_deployment/model_inference:ch08-fig-img2col-weight}}\end{figure}

\sphinxAtStartPar
2.Winograd算法

\sphinxAtStartPar
卷积计算归根到底是矩阵乘法，两个二维矩阵相乘的时间复杂度是\(O(n^3)\)。我们可以使用Winograd来降低矩阵乘法的复杂度。

\sphinxAtStartPar
以一维卷积运算为例，记为F(m，r)，其中，m代表输出的个数，r为卷积核的个数。输入为\(d=[d_0 \ d_1 \ d_2 \ d_3]\)，卷积核为\(g=[g_0 \ g_1 \ g_2]^T\)，该卷积计算可以写成矩阵形式如公式
\eqref{equation:chapter_model_deployment/model_inference:ch08-equ-conv_matmul_one_dimension}所示，需要6次乘法和4次加法。
\begin{equation}\label{equation:chapter_model_deployment/model_inference:ch08-equ-conv_matmul_one_dimension}
\begin{split}F(2, 3)=\left[ \begin{matrix} d_0 & d_1 & d_2 \\ d_1 & d_2 & d_3 \end{matrix} \right] \left[ \begin{matrix} g_0 \\ g_1 \\ g_2 \end{matrix} \right]=\left[ \begin{matrix} y_0 \\ y_1 \end{matrix} \right]\end{split}
\end{equation}
\sphinxAtStartPar
可以观察到，卷积运算转换为矩阵乘法时输入矩阵中存在着重复元素\(d_1\)和\(d_2\)，因此，卷积转换的矩阵乘法相对一般的矩阵乘有了优化空间。可以通过计算中间变量\(m_0-m_3\)得到矩阵乘的结果，见公式
\eqref{equation:chapter_model_deployment/model_inference:ch08-equ-conv-2-winograd}：
\begin{equation}\label{equation:chapter_model_deployment/model_inference:ch08-equ-conv-2-winograd}
\begin{split}F(2, 3)=\left[ \begin{matrix} d_0 & d_1 & d_2 \\ d_1 & d_2 & d_3 \end{matrix} \right] \left[ \begin{matrix} g_0 \\ g_1 \\ g_2 \end{matrix} \right]=\left[ \begin{matrix} m_0+m_1+m_2 \\ m_1-m_2+m_3 \end{matrix} \right]\end{split}
\end{equation}
\sphinxAtStartPar
其中，\(m_0-m_3\)的分别见公式
\eqref{equation:chapter_model_deployment/model_inference:ch08-equ-winograd-param}：
\begin{equation}\label{equation:chapter_model_deployment/model_inference:ch08-equ-winograd-param}
\begin{split}\begin{aligned}m_0=(d_0-d_2)*g_0 \\m_1=(d_1+d_2)*(\frac{g_0+g_1+g_2}{2}) \\m_2=(d_0-d_2)*(\frac{g_0-g_1+g_2}{2}) \\m_2=(d_1-d_3)*g_2\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
通过\(m_0-m_3\)间接计算r1，r2，需要的运算次数包括：输入d的4次加法；输出m的4次乘法和4次加法。在推理阶段，权重的数值是常量，因此卷积核上的运算可以在图编译阶段计算，不计入在线的run时间。所以总的运算次数为4次乘法和8次加法，与直接运算的6次乘法和4次加法相比，乘法次数减少，加法次数增加。在计算机中，乘法一般比加法慢，通过减少乘法次数，增加少量加法，可以实现加速。

\sphinxAtStartPar
计算过程写成矩阵形式如公式
\eqref{equation:chapter_model_deployment/model_inference:ch08-equ-winograd-matrix}所示，其中，⊙为对应位置相乘，A、B、G都是常量矩阵。这里写成矩阵计算是为了表达清晰，实际使用时，按照公式
\eqref{equation:chapter_model_deployment/model_inference:ch08-equ-winograd-param}手写展开的计算速度更快。
\begin{equation}\label{equation:chapter_model_deployment/model_inference:ch08-equ-winograd-matrix}
\begin{split}\mathbf{Y}=\mathbf{A^T}(\mathbf{G}g)*(\mathbf{B^T}d)\end{split}
\end{equation}\begin{equation}\label{equation:chapter_model_deployment/model_inference:ch08-equ-winograd-matrix-bt}
\begin{split}\mathbf{B^T}=\left[ \begin{matrix} 1 & 0 & -1 & 0 \\ 0 & 1 & 1 & 0 \\ 0 & -1 & 1 & 0 \\ 0 & 1 & 0 & -1 \end{matrix} \right]\end{split}
\end{equation}\begin{equation}\label{equation:chapter_model_deployment/model_inference:ch08-equ-winograd-matrix-g}
\begin{split}\mathbf{G}=\left[ \begin{matrix} 1 & 0 & 0 \\ 0.5 & 0.5 & 0.5 \\ 0.5 & -0.5 & 0.5 \\ 0 & 0 & 1 \end{matrix} \right]\end{split}
\end{equation}\begin{equation}\label{equation:chapter_model_deployment/model_inference:ch08-equ-winograd-matrix-at}
\begin{split}\mathbf{A^T}=\left[ \begin{matrix} 1 & 1 & -1 & 0 \\ 0 & 1 & -1 & -1  \end{matrix} \right] \\\end{split}
\end{equation}
\sphinxAtStartPar
通常深度学习领域通常使用的都是2D卷积，将F(2，3)扩展到F(2x2，3x3)，可以写成矩阵形式，如公式
\eqref{equation:chapter_model_deployment/model_inference:ch08-equ-winograd-two-dimension-matrix}所示。此时，Winograd算法的乘法次数为16，而直接卷积的乘法次数为36，降低了2.25倍的乘法计算复杂度。
\begin{equation}\label{equation:chapter_model_deployment/model_inference:ch08-equ-winograd-two-dimension-matrix}
\begin{split}\mathbf{Y}=\mathbf{A^T}(\mathbf{G}g\mathbf{G^T})*(\mathbf{B^T}d\mathbf{B})\mathbf{A}\end{split}
\end{equation}
\sphinxAtStartPar
Winograd算法的整个计算过程在逻辑上可以分为4步，如
\hyperref[\detokenize{chapter_model_deployment/model_inference:ch08-fig-winograd}]{图\ref{\detokenize{chapter_model_deployment/model_inference:ch08-fig-winograd}}}所示：

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{winograd}.png}
\caption{winograd步骤示意图}\label{\detokenize{chapter_model_deployment/model_inference:id15}}\label{\detokenize{chapter_model_deployment/model_inference:ch08-fig-winograd}}\end{figure}

\sphinxAtStartPar
针对任意的输出大小，要使用F(2x2，3x3)的Winograd算法，需要将输出切分成2x2的块，找到对应的输入，按照上述的四个步骤，就可以求出对应的输出值。当然，Winograd算法并不局限于求解F(2x2，3x3)，针对任意的F(m*m，r*r)，都可以找到适当的常量矩阵A、B、G，通过间接计算的方式减少乘法次数。但是随着m、r的增大，输入、输出涉及的加法以及常量权重的乘法次数都在增加，那么乘法次数带来的计算量下降会被加法和常量乘法所抵消。因此，在实际使用场景中，还需要根据Winograd的实际收益来选择。

\sphinxAtStartPar
本小节主要介绍了模型推理时的数据处理和性能优化手段。选择合适的数据处理方法，可以更好地提取输入特征，处理输出结果。并行计算以及算子级别的硬件指令与算法优化可以最大限度的发挥硬件的算力。除此之后，内存的占用及访问速率也是影响推理性能的重要因素，因此推理时需要设计合理的内存复用策略，关于内存复用的策略我们在编译器后端章节已经做了阐述。


\section{模型的安全保护}
\label{\detokenize{chapter_model_deployment/model_security:id1}}\label{\detokenize{chapter_model_deployment/model_security::doc}}
\sphinxAtStartPar
AI服务提供商在本地完成模型训练和调优后，将模型部署到第三方外包平台上（如终端设备、边缘设备和云服务器）来提供推理服务。由于AI模型的设计和训练需要投入大量时间、数据和算力，如何保护模型的知识产权（包括模型结构和参数等信息），防止模型在部署过程中的传输、存储以及运行环节被窃取，已经成为服务/模型提供商最为关心的问题之一。


\subsection{概述}
\label{\detokenize{chapter_model_deployment/model_security:id2}}
\sphinxAtStartPar
模型的安全保护可以分为静态保护和动态保护两个方面。静态保护指的是模型在传输和存储时的保护，目前业界普遍采用的是基于文件加密的模型保护方案，AI模型文件以密文形态传输和存储，执行推理前在内存中解密。在整个推理过程中，模型在内存中始终是明文的，存在被敌手从内存中转储的风险。动态保护指的是模型在运行时的保护，目前业界已有的模型运行时保护方案主要有以下三个技术路线：一是基于TEE（Trusted
execution
environment）的模型保护方案，TEE通常指的是通过可信硬件隔离出来的一个“安全区”，AI模型文件在非安全区加密存储和传输，在安全区中解密运行。该方案在CPU上的推理时延较小，但依赖特定可信硬件，有一定的部署难度。此外，受硬件资源约束，难以保护大规模深度模型，且目前仍无法有效支持异构硬件加速。二是基于密态计算的保护方案，该方案基于密码学方法（如同态加密、多方安全计算等），保证模型在传输、存储和运行过程中始终保持密文状态。该方案不依赖特定硬件，但面临非常大的计算或通信开销问题，且无法保护模型结构信息。三是基于混淆的模型保护方案，该方案主要通过对模型的计算逻辑进行加扰，使得敌手即使能获取到模型也无法理解。与前两种技术路线相比，该方案仅带来较小的性能开销，且精度损失很低，同时，不依赖特定硬件，可支持大模型的保护。下面将重点介绍基于混淆的模型保护技术。


\subsection{模型混淆}
\label{\detokenize{chapter_model_deployment/model_security:id3}}
\sphinxAtStartPar
模型混淆技术可以自动混淆明文AI模型的计算逻辑，使得攻击者即使在传输和存储时获取到模型也无法理解；且支持模型混淆态执行，保证模型运行时的机密性。同时不影响模型原本的推理结果、仅带来较小的推理性能开销。模型混淆技术主要包含以下几个步骤：

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=400\sphinxpxdimen]{{model_obfuscate}.png}
\caption{模型混淆实现步骤图}\label{\detokenize{chapter_model_deployment/model_security:id4}}\label{\detokenize{chapter_model_deployment/model_security:ch08-fig-model-obfuscate}}\end{figure}

\sphinxAtStartPar
结合
\hyperref[\detokenize{chapter_model_deployment/model_security:ch08-fig-model-obfuscate}]{图\ref{\detokenize{chapter_model_deployment/model_security:ch08-fig-model-obfuscate}}}，详细阐述模型混淆的执行步骤：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{(}{)}%
\item {} 
\sphinxAtStartPar
解析模型并获取计算图

\end{enumerate}

\sphinxAtStartPar
对于一个训练好的模型，首先根据模型结构解析模型文件并获取模型计算逻辑的图表达（计算图）用于后续操作。获取的计算图包括节点标识、节点算子类型、节点参数权重以及网络结构等信息。
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{(}{)}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
对计算图的网络结构加扰

\end{enumerate}

\sphinxAtStartPar
通过图压缩和图增广等技术，对计算图中节点与节点之间的依赖关系进行加扰，达到隐藏模型真实计算逻辑的效果。其中，图压缩通过整图检查来匹配原网络中的关键子图结构，这些子图会压缩并替换为单个新的计算节点。对于压缩后的计算图，图增广通过在网络结构中加入新的输入/输出边，进一步隐藏节点间的真实依赖关系。新增的输入/输出边可以来自/指向图中现已有的节点，也可以来自/指向本步骤新增的混淆节点。
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{(}{)}%
\setcounter{enumi}{2}
\item {} 
\sphinxAtStartPar
对计算图的节点匿名化

\end{enumerate}

\sphinxAtStartPar
遍历步骤(2)处理后的计算图，筛选出需要保护的节点。对于图中的每个需要保护的节点，将节点标识、节点算子类型以及其它能够描述节点计算逻辑的属性替换为无语义信息的符号。对于节点标识匿名化，本步骤保证匿名化后的节点标识仍然是唯一的，以区分不同的节点。对于算子类型匿名化，为了避免大规模计算图匿名化导致的算子类型爆炸问题，可以将计算图中算子类型相同的节点划分为若干不相交的集合，同一个集合中节点的算子类型替换为相同的匿名符号。步骤(5)将保证节点匿名化后，模型仍然是可被识别和执行的。
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{(}{)}%
\setcounter{enumi}{3}
\item {} 
\sphinxAtStartPar
对计算图的参数权重加扰

\end{enumerate}

\sphinxAtStartPar
对于每个需要保护的权重，通过一个随机噪声和映射函数对权重进行加扰。每个权重加扰时可以使用不同的随机噪声和映射函数，步骤(6)将保证权重加扰不会影响模型执行结果的正确性。将经过步骤(2)(3)(4)处理后的计算图保存为模型文件供后续使用。
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{(}{)}%
\setcounter{enumi}{4}
\item {} 
\sphinxAtStartPar
算子接口变换

\end{enumerate}

\sphinxAtStartPar
步骤(5)(6)将对每个需要保护的算子类型进行算子形态变换，生成若干候选混淆算子。原算子与混淆算子之间是一对多的对应关系，候选混淆算子的数量等于步骤(3)划分的节点集合的数量。
本步骤根据步骤(2)(3)(4)的得到的匿名化算子类型、算子输入/输出关系等信息，对相应算子的接口进行变换。算子接口的变换方式包括但不局限于输入输出变换、接口名称变换。其中，输入输出变换通过修改原算子的输入输出数据，使得生成的混淆算子与原算子的接口形态不同。新增的输入输出数据包括步骤(2)图增广新增的节点间数据依赖和步骤(4)权重混淆引入的随机噪声。接口名称变换将原算子名称替换为步骤(3)生成的匿名化算子名称，保证节点匿名化后的模型仍然是可被识别和执行的，且算子的名称不会泄露其计算逻辑。
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{(}{)}%
\setcounter{enumi}{5}
\item {} 
\sphinxAtStartPar
算子实现变换

\end{enumerate}

\sphinxAtStartPar
对算子的代码实现进行变换。代码实现的变换方式包括但不局限于字符串加密、冗余代码等软件代码混淆技术，保证混淆算子与原算子实现语义相同的计算逻辑，但是难以阅读和理解。不同的算子可以采用不同代码混淆技术的组合进行代码变换。除代码等价变形之外，混淆算子还实现了一些额外的计算逻辑，如对于步骤(4)中参数被加扰的算子，混淆算子也实现了权重加扰的逆映射函数，用于在算子执行过程中动态消除噪声扰动，保证混淆后模型的计算结果与原模型一致。将生成的混淆算子保存为库文件供后续使用。
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{(}{)}%
\setcounter{enumi}{6}
\item {} 
\sphinxAtStartPar
部署模型和算子库

\end{enumerate}

\sphinxAtStartPar
将混淆态模型文件以及相应的混淆算子库文件部署到目标设备上。
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{(}{)}%
\setcounter{enumi}{7}
\item {} 
\sphinxAtStartPar
混淆模型加载

\end{enumerate}

\sphinxAtStartPar
根据模型结构解析混淆态模型文件并获取模型计算逻辑的图表达，即经过步骤(2)(3)(4)处理后得到的混淆计算图。
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{(}{)}%
\setcounter{enumi}{8}
\item {} 
\sphinxAtStartPar
计算图初始化

\end{enumerate}

\sphinxAtStartPar
对计算图进行初始化，生成执行任务序列。根据安全配置选项，若需要保护模型运行时安全，则直接对混淆计算图进行初始化，生成执行任务序列，序列中的每个计算单元对应一个混淆算子或原算子的执行。若仅需保护模型传输和存储时安全，则可先将内存中的混淆计算图恢复为原计算图，然后对原计算图进行初始化，生成执行任务序列，序列中的每个单元对应一个原算子的执行，这样可以进一步降低推理时的性能开销。
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{(}{)}%
\setcounter{enumi}{9}
\item {} 
\sphinxAtStartPar
推理任务执行

\end{enumerate}

\sphinxAtStartPar
根据AI应用程序输入的推理数据，遍历执行任务序列中的每个计算单元，得到推理结果。若当前计算单元对应的算子是混淆算子时，调用混淆算子库；否则，调用原算子库。


\section{总结}
\label{\detokenize{chapter_model_deployment/summary:id1}}\label{\detokenize{chapter_model_deployment/summary::doc}}\begin{itemize}
\item {} 
\sphinxAtStartPar
不同的模型部署场景下，通常对于模型大小、运行时内存占用、推理时延和推理功耗等指标有限制。

\item {} 
\sphinxAtStartPar
针对模型大小指标，通常在离线阶段通过模型压缩技术来优化，比如量化技术、剪枝技术、知识蒸馏技术等，除此之外，一部分模型优化技术，比如融合技术
\hyperref[\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-sec-fusion}]{\ref{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-sec-fusion}}节}等，也有助于模型轻量化，不过其效果比较微弱。

\item {} 
\sphinxAtStartPar
针对运行时内存指标，主要有三方面的优化：优化模型大小、优化部署框架包大小以及优化运行时临时内存。模型大小的优化手段在上一点中已经说明；部署框架包大小主要通过精简框架代码、框架代码模块化等方式来优化。运行时临时内存主要通过内存池实现内存之间的复用来优化，这部分可以参见
\hyperref[\detokenize{chapter_backend_and_runtime/memory_allocator:ch05-sec-memory-pool}]{\ref{\detokenize{chapter_backend_and_runtime/memory_allocator:ch05-sec-memory-pool}}节}。

\item {} 
\sphinxAtStartPar
针对模型的推理时延指标，主要有两方面的优化，一方面是离线时通过模型优化技术
\hyperref[\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-sec-fusion}]{\ref{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-sec-fusion}}节}和模型压缩技术
\hyperref[\detokenize{chapter_model_deployment/model_compression:ch08-sec-model-compression}]{\ref{\detokenize{chapter_model_deployment/model_compression:ch08-sec-model-compression}}节}尽可能降低模型推理所需的计算量；另一方面是通过加大推理的并行力度
\hyperref[\detokenize{chapter_model_deployment/model_inference:ch08-sec-parallel-inference}]{\ref{\detokenize{chapter_model_deployment/model_inference:ch08-sec-parallel-inference}}节}和优化算子实现
\hyperref[\detokenize{chapter_model_deployment/model_inference:ch08-sec-kernel-optimization}]{\ref{\detokenize{chapter_model_deployment/model_inference:ch08-sec-kernel-optimization}}节}来充分挖掘硬件的计算潜力。值得注意的是，除了考虑计算量和算力，推理时的访存开销也是一个重要的影响因素，这一点在
\hyperref[\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-sec-fusion}]{\ref{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-sec-fusion}}节}小节和
\hyperref[\detokenize{chapter_model_deployment/model_inference:ch08-sec-kernel-optimization}]{\ref{\detokenize{chapter_model_deployment/model_inference:ch08-sec-kernel-optimization}}节}小节中进行了相关优化。

\item {} 
\sphinxAtStartPar
针对模型的推理功耗，主要的优化思路是降低模型的计算量，这与针对模型推理时延的优化手段有重合之处，可以参考离线的模型优化技术
\hyperref[\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-sec-fusion}]{\ref{\detokenize{chapter_model_deployment/model_converter_and_optimizer:ch08-sec-fusion}}节}和模型压缩技术
\hyperref[\detokenize{chapter_model_deployment/model_compression:ch08-sec-model-compression}]{\ref{\detokenize{chapter_model_deployment/model_compression:ch08-sec-model-compression}}节}。

\item {} 
\sphinxAtStartPar
本章除了介绍优化模型部署的各方面指标的优化技术以外，还介绍了安全部署相关的技术，如模型混淆、模型加密等。部署安全一方面可以保护企业的重要资产，另一方面可以防止黑客通过篡改模型从而入侵攻击部署环境。

\end{itemize}


\section{扩展阅读}
\label{\detokenize{chapter_model_deployment/summary:id2}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Google量化白皮书 \sphinxhref{https://arxiv.org/abs/1806.08342}{量化}%
\begin{footnote}[41]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/1806.08342}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
诺亚高精度剪枝算法 \sphinxhref{https://arxiv.org/abs/2010.10732}{剪枝}%
\begin{footnote}[42]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/2010.10732}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
针对多核处理器的自动图并行调度框架
\sphinxhref{https://proceedings.mlsys.org/paper/2021/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf}{性能优化}%
\begin{footnote}[43]\sphinxAtStartFootnote
\sphinxnolinkurl{https://proceedings.mlsys.org/paper/2021/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf}
%
\end{footnote}

\end{itemize}


\chapter{分布式训练}
\label{\detokenize{chapter_distributed_training/index:id1}}\label{\detokenize{chapter_distributed_training/index::doc}}
\sphinxAtStartPar
随着机器学习的进一步发展，科学家们设计出更大型，更多功能的机器学习模型（例如说，GPT\sphinxhyphen{}3）。这种模型含有大量参数，需要复杂的计算以及处理海量的数据。单个机器上有限的资源无法满足训练大型机器学习模型的需求。因此，我们需要设计分布式训练系统，从而将一个机器学习模型任务拆分成多个子任务，并将子任务分发给多个计算节点，解决资源瓶颈。

\sphinxAtStartPar
在本章节中，我们会引入分布式机器学习系统的相关概念，设计挑战，系统实现和实例研究。我们会首先讨论分布式训练系统的定义，设计动机和好处。进一步，我们会讨论常见的分布式训练方法：数据并行，模型并行和流水线并行。在实际中，这些分布式训练方法会被参数服务器（Parameter
Servers），或者是集合通信库（Collective Communication
Libraries）实现。不同的系统实现具有各自的优势和劣势。我们会用大型预训练模型和大型深度学习推荐系统作为实例来探讨不同系统实现的利与弊。

\sphinxAtStartPar
本章的学习目标包括：
\begin{itemize}
\item {} 
\sphinxAtStartPar
掌握分布式训练相关系统组件的定义，设计动机和好处

\item {} 
\sphinxAtStartPar
掌握常见的分布式训练方法：数据并行，模型并行和流水线并行

\item {} 
\sphinxAtStartPar
掌握常见的分布式训练框架实现：参数服务器和集合通信

\item {} 
\sphinxAtStartPar
理解常见分布式训练的实例，和采用不同实现方法的利弊。

\end{itemize}


\section{系统概述}
\label{\detokenize{chapter_distributed_training/overview:id1}}\label{\detokenize{chapter_distributed_training/overview::doc}}

\subsection{设计动机}
\label{\detokenize{chapter_distributed_training/overview:id2}}
\sphinxAtStartPar
接下来，我们详细讨论分布式训练系统的设计动机。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-computation-increase}.png}
\caption{对比机器学习模型参数量增长和计算硬件的算力增长}\label{\detokenize{chapter_distributed_training/overview:id7}}\label{\detokenize{chapter_distributed_training/overview:ch10-computation-increase}}\end{figure}


\subsubsection{算力不足}
\label{\detokenize{chapter_distributed_training/overview:id3}}
\sphinxAtStartPar
单处理器的算力不足是促使人们设计分布式训练系统的一个主要原因。一个处理器的算力可以用\sphinxstylestrong{每秒钟浮点数操作}（Floating
Point Operations Per Second，FLOPS）来衡量。如
\hyperref[\detokenize{chapter_distributed_training/overview:ch10-computation-increase}]{图\ref{\detokenize{chapter_distributed_training/overview:ch10-computation-increase}}}所示，根据摩尔定律（Moore’s
Law），中央处理器的算力每18个月增长2倍。虽然计算加速卡，如GPU和Tensor
Processing
Unit（TPU），针对机器学习计算（如矩阵相乘）提供了大量的算力。这些加速卡的发展最终也受限于摩尔定律，增长速度也停留在每18个月2倍。而与此同时，机器学习模型正在快速发展。短短数年，我们从仅能识别有限物体的AlexNet模型，一路发展到在复杂任务中打败人类的AlphaStar。这期间，模型对于算力需求每18个月增长了35倍。解决处理器性能和算力需求之间的鸿沟的关键就在于利用分布式计算。通过大型数据中心和云计算设施，我们可以快速获取大量的处理器。通过分布式训练系统有效管理这些处理器，我们可以实现算力的快速增长，从而持续满足模型的需求。


\subsubsection{内存不足}
\label{\detokenize{chapter_distributed_training/overview:id4}}
\sphinxAtStartPar
在训练机器学习模型的过程中，训练系统需要在内存中存储大量数据。这些数据包括：模型参数（Parameters）以及训练和更新这些参数所产生的中间数据，如特征图（Feature
Map）和梯度（Gradients）。假设一个深度神经网络模型具有10亿的参数，所有特征图共有20亿参数，每个参数都由一个32位浮点数表达，而更新这些参数至少还需要产生与特征图和参数等量的梯度。由于一个32位浮点数需要4个字节（Byte）的内存来存储，那么训练这个10亿规模的模型就需要至少24GB（\(24 \times 10^9\)
Byte）的内存。现在，随着大型预训练模型的崛起，一个深度神经网络（如GPT\sphinxhyphen{}3）会拥有超过千亿的参数。假设我们依然使用32位浮点数来存储参数，激活值和梯度，那么训练这个模型就至少需要1.2TB的内存。而如今的训练加速卡（如NVIDIA
A100）仅能提供最高80GB的内存。单卡内存空间的增长受到硬件规格，散热和成本等诸多因素，难以进一步快速增长。因此，我们需要分布式训练系统来同时使用数百个训练加速卡，从而为千亿级别的模型提供所需的TB级别的内存。


\subsection{分布式训练架构}
\label{\detokenize{chapter_distributed_training/overview:id5}}
\sphinxAtStartPar
受限于单节点的有限算力，内存和存储资源，人们把关注投向了日益成熟的云计算数据中心。一个数据中心管理着数十万个计算服务器。随着数据中心的全球部署，人们可以很方便地获得数百个服务器。这些服务器可以通过分布式训练系统来协调和管理，解决训练大型机器学习模型过程遇到的算力，内存和存储不足，从而完成训练过程的加速。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-single-vs-multi}.png}
\caption{单节点计算和多节点分布式计算}\label{\detokenize{chapter_distributed_training/overview:id8}}\label{\detokenize{chapter_distributed_training/overview:ch10-single-vs-multi}}\end{figure}

\sphinxAtStartPar
在设计分布式训练系统的过程中，我们需要找出有资源瓶颈的计算任务，根据计算任务的特点，将其拆分成多个子任务，然后将子任务分发给多个节点（可以是服务器，机器，或者是加速卡）并行完成。
\hyperref[\detokenize{chapter_distributed_training/overview:ch10-single-vs-multi}]{图\ref{\detokenize{chapter_distributed_training/overview:ch10-single-vs-multi}}}描述了如何将单节点执行转换为分布式执行的一般过程。在机器学习系统中，一个计算任务往往会有一组数据（例如训练样本）或者任务（例如算子）作为输入，利用一个计算节点（例如GPU）生成一组输出（例如梯度）。假如单节点成为瓶颈，我们可以利用分布式计算进行加速。分布式执行一般具有三个步骤：第一步，我们需要将输入进行\sphinxstylestrong{切分}。第二步，每个输入部分会分发给不同的计算节点，实现\sphinxstylestrong{并行}计算。第三步，每个计算节点的输出，进一步\sphinxstylestrong{合并}，最终得到和单节点等价的计算结果。这种切分\sphinxhyphen{}并行\sphinxhyphen{}合并的模式，本质上实现了分而治之算法（Divide\sphinxhyphen{}and\sphinxhyphen{}Conquer
Algorithm）的设计思想：由于每个计算节点只需要负责更小的子任务，因此其可以更快速的完成计算，最终形成对整个计算过程的加速。


\subsection{用户益处}
\label{\detokenize{chapter_distributed_training/overview:id6}}
\sphinxAtStartPar
通过使用分布式训练系统，我们往往可以获得以下几个关键好处：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{提升系统性能}：使用分布式训练，往往可以带来训练性能的巨大提升。一个分布式训练系统往往用以下这个指标来衡量性能：到达目标精度所需的时间（time\sphinxhyphen{}to\sphinxhyphen{}accuracy）。这个指标由两个参数决定:一个数据周期所需的完成时间，以及一个数据周期模型所提升的精度。通过持续增加并行处理节点，我们可以将数据周期的完成时间不断变短，最终显著减少到达目标精度所需的时间。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{经济性（Economy）}：使用分布式训练，我们也可以进一步减少训练及其模型所需的成本。受限于单节点散热的上限，单节点的算力越高，其所需的散热硬件成本也更高。因此，在提供同等的算力的条件下，组合多个计算节点是一个更加经济高效的方式。这促使云服务商（如亚马逊和微软等）需要更加注重给用户提供成本高效的分布式机器学习系统。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{抵御硬件故障}：分布式训练系统同时能有效提升抵御硬件故障的能力。机器学习训练集群往往由商用硬件（Commodity
Hardware）组成，这类硬件（例如说，磁盘和网卡）运行一定周期就会产生故障。而仅使用单个硬件进行训练的话，那么一个硬件的故障就会造成整个训练的任务的失败。通过将这个训练任务由多个硬件共同完成，即使一个硬件故障了，我们也可以通过将这个硬件上相应的计算子任务转移给其余硬件，继续完成训练，从而避免训练任务的失败。

\end{itemize}


\section{分布式方法}
\label{\detokenize{chapter_distributed_training/methods:id1}}\label{\detokenize{chapter_distributed_training/methods::doc}}
\sphinxAtStartPar
我们会讨论分布式训练系统实现的常用并行方法。我们首先给出并行方法的设计目标以及分类。然后，我们会详细描述各个并行方法。


\subsection{概述}
\label{\detokenize{chapter_distributed_training/methods:id2}}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-single-node}.png}
\caption{单节点训练系统}\label{\detokenize{chapter_distributed_training/methods:id6}}\label{\detokenize{chapter_distributed_training/methods:ch10-single-node}}\end{figure}

\sphinxAtStartPar
分布式训练系统的设计目标是：将单节点训练系统转化成\sphinxstylestrong{等价的}并行训练系统，从而在不影响模型精度的条件下完成训练过程的加速。一个单节点训练系统往往如
\hyperref[\detokenize{chapter_distributed_training/methods:ch10-single-node}]{图\ref{\detokenize{chapter_distributed_training/methods:ch10-single-node}}}所示。一个训练过程会由多个数据小批次（mini\sphinxhyphen{}batch）完成。在图中，一个数据小批次被标示为\sphinxstylestrong{数据}。训练系统会利用数据小批次来生成梯度，提升模型精度。这个过程由一个训练\sphinxstylestrong{程序}实现。在实际中，这个程序往往实现了一个多层神经网络的执行过程。该神经网络的执行由一个计算图（Computational
Graph）表达。这个图有多个相互连接的算子（Operator），每个算子会拥有计算参数。每个算子往往会实现一个神经网络层（Neural
Network Layer），而参数则代表了这个层在训练中所更新的权重（Weights）。

\sphinxAtStartPar
为了更新参数，计算图的执行会分为\sphinxstylestrong{前向}传播和\sphinxstylestrong{反向}传播两个阶段。前向传播的第一步会将数据读入第一个算子，该算子会根据当前的参数，计算出传播给下一个算子的数据。算子依次重复这个前向传播的过程（算子1
\sphinxhyphen{}> 算子2 \sphinxhyphen{}>
算子3），直到最后一个算子结束。最后的算子随之马上开始反向传播。反向传播中，每个算子依次计算出梯度（梯度3
\sphinxhyphen{}> 梯度2 \sphinxhyphen{}>
梯度1），并利用梯度更新本地的参数。反向传播最终在第一个算子结束。反向传播的结束也标志本次数据小批次的结束，系统随之读取下一个小批次，继续更新模型。


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{分布式训练方法分类}\label{\detokenize{chapter_distributed_training/methods:id7}}\label{\detokenize{chapter_distributed_training/methods:ch10-parallel-methods}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline

\sphinxAtStartPar

&\sphinxstyletheadfamily 
\sphinxAtStartPar
单数据
&\sphinxstyletheadfamily 
\sphinxAtStartPar
多数据
\\
\hline
\sphinxAtStartPar
单程序
&
\sphinxAtStartPar
单程序单数据：单点执行
&
\sphinxAtStartPar
单程序多数据：数据并行
\\
\hline
\sphinxAtStartPar
多程序
&
\sphinxAtStartPar
多程序单数据：模型并行
&
\sphinxAtStartPar
多程序多数据：混合并行
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
给定一个单节点训练系统，人们会对\sphinxstylestrong{数据}和\sphinxstylestrong{程序}分区（Partition），从而完成并行加速。
\hyperref[\detokenize{chapter_distributed_training/methods:ch10-parallel-methods}]{表\ref{\detokenize{chapter_distributed_training/methods:ch10-parallel-methods}}}总结了不同的切分方法。单节点训练系统可以被归类于单程序单数据模式。而假如用户希望使用更多的设备来实现并行计算，他们首先可以选择对数据进行分区，并将同一个程序复制到多个设备上并行执行。这种方式是单程序多数据模式，常被称为\sphinxstylestrong{数据并行}（Data
Parallelism）。另一种并行方式是对程序进行分区：程序的算子会被分发给多个设备按照依次完成。这种模式是多程序单数据模式，常被称为\sphinxstylestrong{模型并行}（Model
Parallelism）。当训练超大型智能模型时，开发人们往往要同时对数据和程序进行切分，从而实现最高程度的并行。这种模式是多程序多数据模式，常被称为\sphinxstylestrong{混合并行}（Hybrid
Parallelism）。

\sphinxAtStartPar
接下来，我们详细讲解各种并行方法的执行过程。


\subsection{数据并行}
\label{\detokenize{chapter_distributed_training/methods:id3}}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-data-parallel}.png}
\caption{数据并行训练系统}\label{\detokenize{chapter_distributed_training/methods:id8}}\label{\detokenize{chapter_distributed_training/methods:ch10-data-parallel}}\end{figure}

\sphinxAtStartPar
数据并行往往可以解决单节点的算力不足。这种并行方式在人工智能框架中最为常见，具体实现包括：TensorFlow
DistributedStrategy，PyTorch Distributed，Horovod
DistributedOptimizer等。在一个数据并行系统中，假设用户给定一个训练批大小\(N\)，并且希望使用\(M\)个并行设备来加速训练。那么，该训练批大小会被分为\(M\)个分区，每个设备会分配到\(N/M\)个训练样本。这些设备共享一个训练程序的副本，在不同数据分区上独立执行，计算梯度。不同的设备（假设设备编号为\(i\)）会根据本地的训练样本估计出梯度\(G_i\)。为了确保训练程序参数的一致性，本地梯度\(G_i\)需要聚合，计算出平均梯度\((\sum_{i=1}^{N} G_i) / N\)。最终，训练程序利用平均梯度修正模型参数，完成小批量的训练。

\sphinxAtStartPar
\hyperref[\detokenize{chapter_distributed_training/methods:ch10-data-parallel}]{图\ref{\detokenize{chapter_distributed_training/methods:ch10-data-parallel}}}展示了2个设备构成的数据并行例子。假设用户给定的批大小（Batch
Size）是64，那么每个设备会分配到32个训练样本，并且具有相同的神经网络参数（程序副本）。本地的训练样本会依次通过这个程序副本中的算子，完成前向传播和反向传播。在反向传播的过程中，程序副本会生成局部梯度。不同设备上对应的局部梯度（如设备1和设备2上各自的梯度1）会进行聚合，从而计算平均梯度。这个聚合的过程往往由集合通信库（Collective
Communication）的Allreduce操作来完成。


\subsection{模型并行}
\label{\detokenize{chapter_distributed_training/methods:id4}}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-model-parallel-intra-op}.png}
\caption{模型并行系统：算子内并行}\label{\detokenize{chapter_distributed_training/methods:id9}}\label{\detokenize{chapter_distributed_training/methods:ch10-model-parallel-intra-op}}\end{figure}

\sphinxAtStartPar
模型并行往往用于解决单节点的内存不足问题。一个常见的内存不足场景是模型中含有大型算子，例如说深度神经网络中需要计算大量分类的全连接层（Fully
Connected
Layer）。完成这种大型算子计算所需的内存可能超过单设备的内存容量。那么我们需要对这个大型算子进行切分。假设这个算子具有\(P\)个参数，而我们拥有\(N\)个设备，那么我们可以将\(P\)个参数平均分配给\(N\)个设备（每个设备分配\(P/N\)个参数），从而让每个设备负责更少的计算量，能够在内存容量的限制下完成前向传播和反向传播中所需的计算。这种切分方式是模型并行的应用，被称为\sphinxstylestrong{算子内并行}（Intra\sphinxhyphen{}operator
Parallelism）。

\sphinxAtStartPar
\hyperref[\detokenize{chapter_distributed_training/methods:ch10-model-parallel-intra-op}]{图\ref{\detokenize{chapter_distributed_training/methods:ch10-model-parallel-intra-op}}}给出了一个由2个设备实现的算子内并行的例子。在这个例子中，假设一个神经网络具有2个算子，算子1的计算（包含正向和反向传播）需要预留16G的内存，算子2的计算需要预留1G的内存。而本例中的设备最多可以提供10G的内存。为了完成这个神经网络的训练，我们需要对算子1实现并行。具体做法是，将算子1的参数平均分区，设备1和设备2各负责其中部分算子1的参数。由于设备1和设备2的参数不同，因此它们各自负责程序分区1和程序分区2。在训练这个神经网络的过程中，数据（小批量）会首先传给算子1。由于算子1的参数分别由2个设备负责，因此数据会被广播给这2个设备。不同设备根据本地的参数分区完成前向计算，生成的本地计算结果需要进一步合并（Combine），发送给下游的算子2。在反向传播中，算子2的数据会被广播给设备1和设备2，这些设备根据本地的算子1分区各自完成局部的反向计算。计算结果进一步合并传播回数据，最终完成反向传播。

\sphinxAtStartPar
另一种内存不足的场景是：模型的总内存需求超过了单设备的内存容量。在这种场景下，假如我们总共有\(N\)个算子和\(M\)个设备，我们可以将算子平摊给这\(M\)个设备，让每个设备仅需负责\(N/M\)个算子的前向和反向计算，降低设备的内存开销。这种并行方式是模型并行的另一种应用，被称为\sphinxstylestrong{算子间并行}（Inter\sphinxhyphen{}operator
Parallelism）。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-model-parallel-inter-op}.png}
\caption{模型并行系统：算子间并行}\label{\detokenize{chapter_distributed_training/methods:id10}}\label{\detokenize{chapter_distributed_training/methods:ch10-model-parallel-inter-op}}\end{figure}

\sphinxAtStartPar
\hyperref[\detokenize{chapter_distributed_training/methods:ch10-model-parallel-inter-op}]{图\ref{\detokenize{chapter_distributed_training/methods:ch10-model-parallel-inter-op}}}给出了一个由2个设备实现的算子间并行的例子。在这个例子中，假设一个神经网络具有2个算子，算子1和算子2各自需要10G的内存完成计算，则模型总共需要20G的内存。而每个设备仅能提供10G内存。在这个例子中，用户可以把算子1放置在设备1上，算子2放置在设备2上。在前向传播中，算子1的输出会被发送（Send）给下游的设备2。设备2接收（Receive）来自上游的数据，完成算子2的前向计算。在反向传播中，设备2将算子2的反向计算结果发送给设备1。设备1完成算子1的反向计算，完成本次训练。


\subsection{混合并行}
\label{\detokenize{chapter_distributed_training/methods:id5}}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-hybrid-parallel}.png}
\caption{混合并行系统}\label{\detokenize{chapter_distributed_training/methods:id11}}\label{\detokenize{chapter_distributed_training/methods:ch10-hybrid-parallel}}\end{figure}

\sphinxAtStartPar
在训练大型人工智能模型中，我们往往会同时面对算力不足和内存不足。因此，我们需要混合使用数据并行和模型并行，这种方法被称为混合并行。
\hyperref[\detokenize{chapter_distributed_training/methods:ch10-hybrid-parallel}]{图\ref{\detokenize{chapter_distributed_training/methods:ch10-hybrid-parallel}}}提供了一个由4个设备实现的混合并行的例子。在这个例子中，我们首先实现算子间并行来解决训练程序内存开销过大的问题：该训练程序的算子1和算子2被分摊到了设备1和设备2上。进一步，我们通过数据并行来添加3和设备4，提升系统算力。为了达到这一点，我们对训练数据进行分区（数据分区1和数据分区2），并将模型（算子1和算子2）分配复制到设备3和设备4上生成可以并行执行的程序副本。在前向计算的过程中，设备1和设备3上的算子1副本同时开始，计算结果分别发送（Send）给设备2和设备4完成算子2副本的计算。在反向计算中，设备2和设备4同时开始计算梯度，本地梯度通过Allreduce进行平均。反向计算传递到设备1和设备3上的算子1副本结束。


\section{流水线并行}
\label{\detokenize{chapter_distributed_training/pipeline:id1}}\label{\detokenize{chapter_distributed_training/pipeline::doc}}
\sphinxAtStartPar
在数据并行和模型并行以外，流水线并行是另一种常用的并行加速方法。流水线并行往往被应用在大型模型并行系统中。这种系统通过算子内并行和算子间并行解决单设备内存不足的问题。然而，当这类系统的运行中，计算图中的下游设备需要长期持续处于空闲状态，等待上游设备的计算完成，才可以开始计算，这极大降低了设备的平均使用率。这种现象被称为模型并行空洞（Model
Parallelism Bubble）。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-pipeline-parallel}.png}
\caption{流水线并行系统。\(F_{i,j}\)表示第\(j\)个微批量的第\(i\)个前向stage，\(B_{i,j}\)表示第\(j\)个微批量的第\(i\)个反向stage。}\label{\detokenize{chapter_distributed_training/pipeline:id2}}\label{\detokenize{chapter_distributed_training/pipeline:ch10-pipeline-parallel}}\end{figure}

\sphinxAtStartPar
为了减少空洞，提升设备使用率，我们可以在模型并行系统中构建流水线。这种做法的核心想法是将一个数据小批量（Data
Mini\sphinxhyphen{}batch）划分为多个微批量（Micro\sphinxhyphen{}batch）。假设一个数据小批量有\(D\)个训练数据，这个小批量可以被划分为\(M\)个微批量，那么微批量的大小就是\(D/M\)。每个微批量相应进入训练系统，完成前向传播（Forwards
propagation）和反向传播（Backwards
propagation），计算出梯度。每个微批量对应的梯度将会缓存，等到全部微批量完成，缓存的梯度会被加和，算出平均梯度，更新模型参数。

\sphinxAtStartPar
\hyperref[\detokenize{chapter_distributed_training/pipeline:ch10-pipeline-parallel}]{图\ref{\detokenize{chapter_distributed_training/pipeline:ch10-pipeline-parallel}}}
进一步给出了一个流水线并行的执行例子。在本例中，模型参数需要切分给4个设备存储。为了充分利用起来这4个设备，我们将小批量切分为2个微批量。当设备1完成第一个微批量的前向传播后（表示为\(F_{0,0}\)）后，他会将中间结果发送给设备2，触发响应的前向传播任务（表示为\(F_{1,0}\)）。与此同时，设备1也可以开始第二个微批量的前向传播任务（表示为\(F_{0,1}\)）。前向传播会在流水线的最后一个设备–设备3–完成。系统于是开始反向传播。设备4开始第1个微批量的反向传播任务（表示为\(B_{3,0}\)）。该任务完成后的中间结果会被发送给设备3，触发响应的反向传播任务（表示为\(B_{2,0}\)）。与此同时，设备4会缓存好对应第1个微批量的梯度，接下来开始第2个微批量计算（表示为\(B_{3,1}\)）。当设备4完成了全部的反向传播计算后，他会将本地缓存的梯度进行相加，并且除以微批量数量，计算出平均梯度，该梯度用于更新模型参数。

\sphinxAtStartPar
流水线并行的关键因素是流水线泡沫（Bubble）。当设备完成前向传播后，必须等到全部反向传播开始，在此期间设备会处于空闲状态。在
\hyperref[\detokenize{chapter_distributed_training/pipeline:ch10-pipeline-parallel}]{图\ref{\detokenize{chapter_distributed_training/pipeline:ch10-pipeline-parallel}}}
中，我们可以看到设备1在完成2个前向传播任务后，要等很多时间才能开始2个反向传播任务。这其中的等待时间即被称为泡沫。为了减少设备的等待时间，一种常见的做法是尽可能的增加微批量的数量，从而让反向传播尽可能早的开始。然而，使用非常小的微批量大小，可能会造成加速器无法被充分利用。因此最优的微批量大小是多种因素的折中。其中最核心的因素是流水线泡沫的大小和加速器的计算能力。


\section{集合通信}
\label{\detokenize{chapter_distributed_training/collective:id1}}\label{\detokenize{chapter_distributed_training/collective::doc}}
\sphinxAtStartPar
作为并行计算中的一个重要概念，集合通信算子经常会被用来构建单程序流/多数据流编程环境（single
program\sphinxhyphen{}multiple data,
SPMD）中的许多交互模式。近年来，该领域无论是在对不同硬件架构的支持还是算法性能的发展上都成果颇丰，而因SPMD在大型深度学习系统中与数据并行的深厚联系，这些框架也在其中受益匪浅。因此，相比点对点
(Point\sphinxhyphen{}to\sphinxhyphen{}Point, p2p)
通信，我们有更大的兴趣去探讨如何高效地在数据中心（Data
Centers）中实现这些集合通信范式。首先，我们会介绍一些集合通信中常见的算子，一个经典的利用All算法解决分布式训练系统中网络瓶颈的示例，探讨该算法在不同网络拓扑结构下的差异性以及一些重要指标（算法带宽，总线带宽）的计算方法，最后简略介绍现有机器学习系统对不同集合通信算法的支持。


\subsection{常见算子}
\label{\detokenize{chapter_distributed_training/collective:id2}}
\sphinxAtStartPar
在分布式内存模型（Distributed Memory
Model）中，一些常见的进程间数据交互模式由硬件支持和并行算法的内在性质而涌现。因此，主流的并行计算架构标准（例如MPI）和机器学习系统的底层集合通信库（例如gloo，NCCL）通常会支持数个经典的算子并针对其做优化，一般包括Broadcast，Reduce，AllGather，ReduceScatter
和 AllReduce。在一个基于 \DUrole{bibtex}{{[}Sanders2019\sphinxhyphen{}cq{]}}
的简化理论模型下，可以对这些算子的特性进行简单的介绍并探讨具体的实现方法和计算开销。


\subsubsection{基本定义}
\label{\detokenize{chapter_distributed_training/collective:id4}}
\sphinxAtStartPar
首先，假定一个简化后的分布式内存模型：存在p个随机存取存储器（Random
Access Machines, RAM）作为基础的处理单元（Processing Element,
PE)，并由一个网络来连接所有的机器。每个处理单元有自己的独立内存，并且所有的处理单元间的通信都通过网络传输。同时，每个处理单元都知道自己的编号\(i\)，通常在\(1\)到\(p\)之间。
网络之间的通信在最底层的情况下均为点对点的全双工通信（full\sphinxhyphen{}duplex
point\sphinxhyphen{}to\sphinxhyphen{}point communication)：
\begin{itemize}
\item {} 
\sphinxAtStartPar
每次通信有且仅有一个发送者（sender）和一个接收者（receiver）。

\item {} 
\sphinxAtStartPar
在某个特定时刻，每个处理单元仅能至多发送或接收一个信息。但是，在网络中可以同时传输多个信息。每个处理单元也可以在发送一个信息的同时接收一个信息。

\item {} 
\sphinxAtStartPar
传输一个长度为l的信息会花费\(a+bl\)的时间，其中\(a\)代表延迟（latency），即单位信息通过网络从一个处理单元出发到达另一个处理单元所需的时间；\(b\)代表传输延迟（transmission
delay），即把单位信息从处理单元中放到网络通信单元所需的时间。前者的大小一般取决于两个处理单元间的物理距离（同一个机架，同一个数据中心，横跨全球等），而后者的大小一般取决于通信网络的带宽。在这个模型下，假定所有处理单元之间的a和b均为恒定值。

\item {} 
\sphinxAtStartPar
通信可以指定一个发送者或者一个接收者：由于每个存储单元都有相对应的编号，我们可以定义两个函数send(i,l)
和receive(i,l)。其中send函数会把信息l从当前的处理单元发送至编号为i的处理单元，而receive函数会从编号为i的处理单元接收信息l。在调用send函数时，处理单元必须同时调用receive来保证编号为i的处理单元收到了该信息。因此，也可以说send和receive
同步（synchronize）了发送者和接收者。

\item {} 
\sphinxAtStartPar
作为拓展，我们也可以定义上述函数的一个变种：i = send(m) 和 i =
receive(m)，即在传输信息时不规定发送者或接收者。这种情况下，网络中的任意一个处理单元都可以发送或接收该信息，而最终完成传输的处理单元的编号会作为函数的返回值。

\item {} 
\sphinxAtStartPar
虽然在现实生活中错误（fault）时常发生，但是在这个模型里，暂不考虑通信丢失（dropped
message）和通信毁坏（corrupted message）的情况。

\end{itemize}

\sphinxAtStartPar
分布式内存模型中对于通信同步和传输的结合使得在这个理论模型下开发的代码更好维护。额外的，由于这个框架下提出的算法往往会产生一些很有规律的，包含了网络中所有处理单元的交互模式，通常会在最基础的点对点通信上维护一个算子库，用来归纳总结这些高效且更易于理解的算法，我们将其称为集合通信算子。


\subsubsection{Broadcast}
\label{\detokenize{chapter_distributed_training/collective:broadcast}}
\sphinxAtStartPar
在SPMD中，最常见的一个交互模式经常是把一个位于处理单元i的信息发送到全部其他的节点，用于同步某种全局的变量或者参数。为此Broadcast算子可以定义为从编号为\(i\)的处理单元发送长度为\(l\)的信息给全部剩余的\(p-1\)个处理单元。在这里，一种简单的方法是在一个循环中使用\(p-1\)次send/receive来实现Broadcast，但这并不能很好地利用通信可并行化的特质（该算法只有\((a+bl)(p-1)\)的线性时间复杂度）。为此，我们可以利用分治思想（divide\sphinxhyphen{}and\sphinxhyphen{}conquer）来对上述算法进行优化。假设所有的处理单元可以重新对编号进行排列，使得Broadcast的发送者为编号为\(1\)的处理单元。同时，为了简化计算过程，假设对于某个自然数\(n\)，\(p = 2^n\)。
现在，我们可以通过从1 向 \(p/2\)
发送一次信息来把问题转化为两个大小为\(p/2\)的子问题：编号为1的处理单元对1到\(p/2-1\)
的Broadcast，以及编号为\(p/2\)的处理单元对\(p/2\)到\(p\)的Broadcast。我们便可以通过在这两个子问题上进行递归来完成这个算法，并把临界条件定义为编号为i的处理单元在\([i,i]\)这个区间里的Broadcast。此时，由于i本身已经拥有该信息，我们不需要做任何操作便可直接完成Broadcast。这个优化后的算法有\((a+bl)\log p\)
时间复杂度，因为在算法的每一阶段\(t\)，我们有\(2^t\)个计算单元在并行运行Broadcast算子。同时，算法一定会在\(\log p\)
步之内结束。


\subsubsection{Reduce}
\label{\detokenize{chapter_distributed_training/collective:reduce}}
\sphinxAtStartPar
除了Broadcast，另一个常见的交互模式为程序试图概述在部分处理单元上得到的中间值。这时候，对于一个符合结合律（associative
property）的算子\(f\)，我们可以定义Reduce算子，即将所有处理单元上的某个值两两配对重复应用该算子，并把最终结果储存在编号为\(i\)的计算单元上。常见的应用于Reduce中的算子有加和，乘积，最大值，最小值和平均值等。一个简易的Reduce的优化实现同样可以用分治思想来实现，即把\(1\)到\(p/2-1\)的Reduce结果存到编号为\(1\)的处理单元中，然后把\(p/2\)到\(p\)的Reduce结果存到\(p/2\)上。最后，我们可以把\(p/2\)的结果发送至\(1\)，执行\(f\)，并把最后的结果存至\(i\)。假设\(f\)的运行时间复杂度为常数并不改变其输出信息的长度\(l\)，Reduce的时间复杂度仍然为\((a+bl)\log p\)。


\subsubsection{AllReduce}
\label{\detokenize{chapter_distributed_training/collective:allreduce}}
\sphinxAtStartPar
AllReduce算子为Reduce的一个变种，即将f的结果存至所有处理单元上。在这里，我们给出一个简化版的AllReduce
实现方式，即首先把最终值通过Reduce存到编号为\(1\)的处理单元，再将该值通过Broadcast广播到所有的处理单元上。在两个子算子都使用上述的算法情况下，AllReduce的时间复杂度仍为\((a+bl)\log p。\)


\subsubsection{Gather}
\label{\detokenize{chapter_distributed_training/collective:gather}}
\sphinxAtStartPar
Gather算子尝试将每个处理单元上的信息全部聚合到编号为\(i\)的处理单元上，通常用于组装散落在每个处理单元上的独立信息。在聚合函数符合结合律的情况下，可以通过将其设为Reduce算子中的\(f\)来实现Gather算子。但是，在这种情况下，无论是基于链表还是数组的实现，在每一步的Reduce子问题中\(f\)的时间复杂度或输出长度\(l\)都发生了改变。因此，Gather并不具有先前Reduce或者Broadcast的时间复杂度，而是\(a \log p + (p-1) bl\)。这是因为在算法的每一阶段t，我们传输的信息长度为\(l 2^t\)。


\subsubsection{AllGather}
\label{\detokenize{chapter_distributed_training/collective:allgather}}
\sphinxAtStartPar
相比起Gather，AllGather
算子会把聚合的结果存到所有的处理单元上。在这里，一个简单的做法是使用Gather和Broadcast把聚合结果先存到编号为1的处理单元中，再将其广播到剩余的处理单元上。这会产生一个\(a \log p + (p-1) bl + (a+plb) \log p\)的时间复杂度，因为在Broadcast时如果忽略链表/数组实现所带来的额外空间开销，每次通信的长度为\(pl\)而不是\(l\)。简化后，我们得到了一个\(a \log p + plb \log p\)
的时间复杂度。在一个基于超立方体的算法下，我们可以将其进一步优化到和Gather一样的\(a \log p + (p-1) bl\)
（\DUrole{bibtex}{{[}Sanders2019\sphinxhyphen{}cq{]}}），然而由于篇幅问题便不再赘述。


\subsubsection{Scatter}
\label{\detokenize{chapter_distributed_training/collective:scatter}}
\sphinxAtStartPar
Scatter算子可以被视作Gather的逆运算：把一个存在于编号为\(i\)的处理单元上，长度为\(p\)（信息长度为\(pl\)）的链式数据结构L中的值分散到每个处理单元上，使得编号为i的处理单元会得到\(L[i]\)。我们可以通过模仿Gather算法来设计一个简易的Scatter实现：每一步的运算中，与其是聚集一半处理单元的结果，我们把现在的子链继续对半切分，并把前半段和后半段作为子问题进行递归。这时候，在算法的每一阶段\(t\)，我们传输的信息长度为\(l 2^(m-t)\)，其中m是算法总共运行的步骤，不会超过\(\log p\)
（见Broadcast）。最终，Scatter算子的检疫实现和Gather一样都有\(a \log p + (p-1) bl\)
时间复杂度。在机器学习系统中，相比于链式数据结构，Scatter经常同时被用于可切分的数据结构，例如张量（tensor）在一个维度上的p等分等。


\subsubsection{ReduceScatter}
\label{\detokenize{chapter_distributed_training/collective:reducescatter}}
\sphinxAtStartPar
ReduceScatter算子可以视为Reduce 和
Scatter算子的组合体，即对于每个处理单元上分别拥有的一个链式/可切分数据结构，在通过f
概述后再重新分散到各个单元中。虽然我们已经知道了Reduce 和Scatter
各自的时间复杂度，但是在对ReduceScatter做时间复杂度分析时需要注意两部之间信息长度的变化：假设每个处理单元上的数据结构所需通信长度为\(pl\)，第一阶段的Reduce算法需要\((a+plb)\log p\)
时间复杂度。参照Scatter的分析，第二阶段的算子则需要
\(a \log p + (p-1) bl\) 时间复杂度。综合下来，ReduceScatter 需要
\(a \log p + plb \log p\)
的时间复杂度，和AllGather相同。同时，运行ReduceScatter 和
AllGather的效果等同于运行一次AllReduce。

\sphinxAtStartPar
在SPMD中，通常还有一些额外的集合通信算子，如Prefix
Sum，Barrier，All\sphinxhyphen{}to\sphinxhyphen{}All等，但由于篇幅限制以及与机器学习系统的有限联系，便不再赘述。最后，由于该模型下通信网络的拓扑结构较为简单，上文中呈现二叉树形的递归树也可以达到很好的实际运行速度。所有关于时间复杂度的分析也是基于这些相对简化的假设情况。后文中，我们将会用AllReduce举例介绍如何在更复杂的拓扑结构下设计不同的集合通信算子变种，并在时间复杂度之外去关注实际的通信量和运算时间。


\subsection{在数据中心的梯度计算}
\label{\detokenize{chapter_distributed_training/collective:id6}}
\sphinxAtStartPar
接下来，我们将用一个示例来阐释集合通信在机器学习系统中发挥的重要作用。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-datacentre}.png}
\caption{数据中心}\label{\detokenize{chapter_distributed_training/collective:id19}}\label{\detokenize{chapter_distributed_training/collective:ch10-datacentre}}\end{figure}

\sphinxAtStartPar
\hyperref[\detokenize{chapter_distributed_training/collective:ch10-datacentre}]{图\ref{\detokenize{chapter_distributed_training/collective:ch10-datacentre}}}
描述了一个典型的用于深度学习模型训练的数据中心。数据中心中的训练服务器一般会有多个设备。如需增加服务器，我们会将多个训练服务器放置在一个机柜（Rack）上，同时接入一个架顶交换机（Top
of Rack
Switch）将其连接。在现有机柜满载的情况下，可以通过在架顶交换机间增加骨干交换机（Spine
Switch）来接入新的机柜。通过这种方式，可以在数据中心内不断增加服务器，从而为神经网络的训练提供海量的算力和内存。目前的商用数据中心可拥有近百万台服务器。

\sphinxAtStartPar
在数据中心中训练大型神经网络的首要挑战是如何高效计算大量的平均梯度。假设给定一个千亿级别参数的神经网络（比如OpenAI
发布的大型语言模型GPT\sphinxhyphen{}3 \DUrole{bibtex}{{[}gpt\sphinxhyphen{}3{]}}
有将近1750亿参数），如果用32位浮点数来表达每一个参数，那么每一步训练中，一个数据并行模式下的模型副本（Model
Replica）则需要生成700GB的本地梯度数据（即 175G \(\times\) 4 bytes =
700GB）。假如有3个模型副本，那么至少需要传输1.4TB（即，700GB
\(\times\)
\((3-1)\)）的本地梯度数据（因为对于\(N\)个副本，只需传送其中的\(N-1\)个副本来完成计算）。当平均梯度计算完成后，需要进一步将其广播（Broadcast）到全部的模型副本（即1.4TB的数据）并更新其中的本地参数，从而确保模型副本不会偏离（Diverge）主模型中的参数。

\sphinxAtStartPar
当前的数据中心一般使用以太网（Ethernet）构建不同机柜之间的网络。主流的商用以太网链路带宽一般在10Gbps到25Gbps之间。利用以太网传输海量梯度会产生严重的传输延迟，从而降低模型训练的速度。新型深度学习训练集群（如英伟达的DGX系列机器）往往配置有更快的Inifiband。单个InfiniBand链路可以提供100Gbps或200Gbps的带宽。即使拥有这种高速网络，传输TB级别的本地梯度依然需要大量延迟（即使忽略网络延迟，1TB的数据在200Gbps的链路上传输也需要至少40秒）。

\sphinxAtStartPar
为了避免通过机间网络传输数据，现代深度学习服务器一般都会配备多个加速器（例如说，英伟达的DGX\sphinxhyphen{}3服务器会配备8个A100
GPU），而在一个服务器内的多个设备可以通过高速机内网络互联（如NVLink）。这种高速机内网络可以提供高达400GBps的带宽，从而让传输TB级别的数据成为可能。然而，受限于单个服务器的散热，成本和硬件等限制，通常无法在一个服务器内无限制的持续增加设备。因此，大型深度学习模型的训练仍需要多个服务器共同完成。在计算平均梯度时，服务器需要同时借助机间网络通信接口（以太网或InfiniBand）和机内通信接口（NVLink）。


\subsection{基于AllReduce的梯度平均算法}
\label{\detokenize{chapter_distributed_training/collective:id8}}
\sphinxAtStartPar
我们将讨论如何利用AllReduce算子来实现数据中心中的高效梯度平均。首先，参照前文的分析，可以考虑一种简单的计算平均梯度的方法：在集群中分配一个设备来收集本地梯度，并在计算平均梯度后再将其广播到全部的设备。这种做法易于实现，但是引入了两个问题。首先，多台设备同时给该聚合设备发送数据时，聚合设备会因严重的带宽不足产生网络拥塞。其次，单台设备需要负担大量的梯度平均计算，而受限于单台设备上的有限算力，这种计算往往会受限于算力瓶颈。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{img/ch09/ch10-AllReduce-state}.png}
\caption{AllReduce初始状态和终止状态}\label{\detokenize{chapter_distributed_training/collective:id20}}\label{\detokenize{chapter_distributed_training/collective:ch10-allreduce-state}}\end{figure}

\sphinxAtStartPar
为了解决上述问题，可以引入AllReduce算子的Reduce\sphinxhyphen{}Broadcast实现来优化算法，其设计思路是：通过让全部的节点参与到梯度的网络通信和平均计算中，将巨大的网络和算力开销均摊给全部节点。这种做法可以解决先前单个梯度聚合节点的问题。假设有\(M\)个设备，每个设备存有一个模型副本，该模型由\(N\)个参数/梯度构成。那么按照AllReduce算子的要求，需要先将全部的参数按照设备数量切分成\(M\)个分区（Partition），使得每个分区具有\(N/M\)个参数。我们首先给出这个算法的初始和终止状态。如
\hyperref[\detokenize{chapter_distributed_training/collective:ch10-allreduce-state}]{图\ref{\detokenize{chapter_distributed_training/collective:ch10-allreduce-state}}}
所示，该例子含有3个设备。在每个设备有一个模型副本的情况下，这个副本有3个参数。那么按照AllReduce的分区方法，参数会被划分成3个分区（3个设备），而每一个分区则有1个参数（\(N/M\)，N代表3个参数，M代表3个设备）。在这个例子中，假定设备1拥有参数2,4,6，设备2拥有参数1,2,3，设备3拥有参数4,8,12，那么在使用AllReduce算子进行计算过后，全部的设备都将拥有梯度相加后的结果7,14,21，其中分区1的结果7是由3个设备中分区1的初始结果相加而成（7
= 1 + 2 +
4）。为了计算平均梯度，每个设备只需要在最后将梯度之和除以设备数量即可（分区1的最终结果为7除以3）。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{img/ch09/ch10-AllReduce-process}.png}
\caption{AllReduce算法的过程}\label{\detokenize{chapter_distributed_training/collective:id21}}\label{\detokenize{chapter_distributed_training/collective:ch10-allreduce-process}}\end{figure}

\sphinxAtStartPar
AllReduce算子会把梯度的计算拆分成\(M-1\)个Reduce算子和\(M-1\)个Broadcast算子（其中\(M\)是节点的数量）。其中，Reduce算子用于计算出梯度的和（Summation），Broadcast算子用于把梯度之和广播给全部的节点。为了说明这些算子的执行过程，可以参照
\hyperref[\detokenize{chapter_distributed_training/collective:ch10-allreduce-process}]{图\ref{\detokenize{chapter_distributed_training/collective:ch10-allreduce-process}}}
。AllReduce算子由Reduce算子开始，在第一个Reduce算子中，AllReduce算子会对全部节点进行配对（Pairing），让他们共同完成梯度相加的操作。在
\hyperref[\detokenize{chapter_distributed_training/collective:ch10-allreduce-process}]{图\ref{\detokenize{chapter_distributed_training/collective:ch10-allreduce-process}}}
的第一个Reduce算子中，设备1和设备2进行了配对共同对分区1的数据相加。其中，设备2把本地的梯度数据1发送给设备1，设备将接收到1和本地的分区1内的梯度数据：2进行相加，计算出中间（intermediate）梯度相加的结果：3。与此同时，设备1和设备3进行配对，共同完成对分区3的数据相加。而设备3和设备2进行配对，共同完成对于分区2的数据相加。

\sphinxAtStartPar
在上述Reduce的算子中，梯度的计算实现了以下几个特性:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{网络优化：}
全部设备都同时在接收和发送数据，利用起了每个设备的入口（Ingress）和出口（Egress）带宽。因此AllReduce过程中可利用的带宽是\(M \times B\)，其中\(M\)是节点数量，\(B\)是节点带宽，从而让系统实现网络带宽上的可扩展性。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{算力优化：}
全部设备的处理器都参与了梯度相加的计算。因此AllReduce过程中可利用的处理器是\(M \times P\)，其中\(M\)是节点数量，\(P\)是处理器数量，从而让系统实现计算上的可扩展性。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{负载均衡：}
由于数据分区是平均划分的，因此每次设备分摊到的通讯和计算开销是相等的。

\end{itemize}

\sphinxAtStartPar
在接下来的Reduce算子中，AllReduce算法会对不同数据分区选择另外的配对方法。例如说，在
\hyperref[\detokenize{chapter_distributed_training/collective:ch10-allreduce-process}]{图\ref{\detokenize{chapter_distributed_training/collective:ch10-allreduce-process}}}
的第二个Reduce算子中，AllReduce算法会将：设备1和设备3进行配对，负责分区1的数据相加。将设备1和设备2进行配对，负责分区2。将设备2和设备3进行配对，负责分区3。在一个3个节点的AllReduce集群里，在2个Reduce算子完成后，我们就计算出了每个分区的数据相加结果（分区1的结果7此时在设备3上，分区2的结果14此时在设备1上，分区3的结果21此时在设备2上）。

\sphinxAtStartPar
接下来，AllReduce算法将进入Broadcast阶段。这一阶段的过程和Reduce算子类似，核心区别是节点进行配对后，他们不再进行数据相加，而是将Reduce的计算结果进行广播。在
\hyperref[\detokenize{chapter_distributed_training/collective:ch10-allreduce-process}]{图\ref{\detokenize{chapter_distributed_training/collective:ch10-allreduce-process}}}
中的第一个Broadcast算子中，设备1会将分区2的结果14直接写入设备3的分区2中。设备2会讲分区3的结果21直接写入设备1中。设备3会将分区1的结果直接写入设备2中。在一个3个节点的AllReduce集群中，我们会重复2次Broadcast算子来将每个分区的Reduce结果告知全部的节点。


\subsection{带宽计算}
\label{\detokenize{chapter_distributed_training/collective:id9}}
\sphinxAtStartPar
在讨论集合通信算子的性能时，人们经常会使用一些数值化指标去量化不同的算法实现，其中一个重要概念为带宽（Bandwidth）。在文献（\DUrole{bibtex}{{[}nvidia\sphinxhyphen{}nccl{]}}）中，通常有两种主流的对带宽的计算方法，分别为算法带宽（Algorithm
Bandwidth）与总线带宽（Bus Bandwidth）。


\subsubsection{算法带宽}
\label{\detokenize{chapter_distributed_training/collective:id11}}
\sphinxAtStartPar
前文提到，在计算点对点通信所需的时间是，会在信息长度之上乘以一个系数b。这个系数就是算法带宽，泛指单位时间内执行操作（通信，计算等）的数量。一般计算公式为\(b = s/t\)，其中\(s\)代指操作的大小，\(t\)指操作指定的两个端点之间所经过的时间。以点到点通信举例，我们可以通过衡量一个大小已知的信息\(m\)在执行send函数时所花的时间来确定两个处理单元之间网络的带宽。


\subsubsection{总线带宽}
\label{\detokenize{chapter_distributed_training/collective:id12}}
\sphinxAtStartPar
虽然算法带宽的计算方法既简单又高效，但很难将其拓展至对于集合通信算子的带宽计算。这是因为，取决于具体算子和算法实现的不同，一个集合通信算子在执行过程中测得的算法带宽往往会远小于硬件本身的最高带宽。在实际运行相应的测试中，经常能观测到随着处理单元增加，算法带宽呈下降趋势。为了解决这一问题，NCCL提出了总线带宽这一概念，通过对于每个集合通信算子的分析来对测得的算法带宽乘以一个校正系数（correction
factor），来减轻处理单元数量对于测量带宽的影响并给出一个更贴近实际硬件表现的带宽值。下面列出了一些常见算子的校正系数，以及背后的简略推导。
\begin{itemize}
\item {} 
\sphinxAtStartPar
AllReduce：\(2(p-1)/p\)
对于在处理单元\(n_1, n_2 \cdots n_p\) 上的值
\(v_1, v_2 \cdots v_p\) 计算
\(v_1 (op) v_2 \cdots (op) v_p\)（其中\(op\)为符合结合律的算子），再存回每个处理单元中。在不考虑实际实现算法和网络拓扑的情况下，这个操作理论上只需要
\(2(p-1)\) 次数据传输，其中包含在每个处理单元上分开进行的
\(n-1\) 次 op的运算，以及最后 \(n\)
次最终数据值的广播，再减去第一个处理单元的运算和最后一个处理单元的广播的影响。假设每个处理单元对于外界所有信息处理的带宽为\(B\)，我们可以得出对于S个在不同处理单元上的数据运行AllReduce是能得到的最优情况下的运行时间：\(t = (2S(p-1)) / (pB)\)，进行简化后可得
\(B = (S/t)(2(p-1)/p) = b (2(p-1)/p)\)。这里的
\(2(p-1)/p\)便是我们的校正系数。

\item {} 
\sphinxAtStartPar
ReduceScatter：\((p-1)/p\)
对于每个处理单元来说，可以把ReduceScatter理解为只执行AllReduce中的聚合部分。对此，我们只需要考虑上文分析中的\(n-1\)次\(op\)的运算，整理后可得\(B = (S/t)((p-1)/p) = b ((p-1)/p)\)。

\item {} 
\sphinxAtStartPar
AllGather：\((p-1)/p\)
同理，对于每个处理单元来说，可以把AllGather理解为只执行AllReduce中的广播部分。我们同理可得\(B = (S/t)((p-1)/p) = b ((p-1)/p)\)。

\item {} 
\sphinxAtStartPar
Broadcast：\(1\)
与AllReduce不同的是，Broadcast中所有数据需要从算子本身的发送者发出。即使在上文的分治情况下，我们也需要等待所有子问题运行结束才能确保Broadcast算子本身的正确性。因此，在计算带宽时瓶颈仍为发送者对于外界所有信息处理的带宽，所以
\(B = S/t\)，即校正系数为\(1\)。

\item {} 
\sphinxAtStartPar
Reduce：\(1\)
同Broadcast，Reduce需要将所有数据送往算子的接收者，因此校正系数同样为\(1\)。

\end{itemize}

\sphinxAtStartPar
由于Gather和Scatter的带宽计算与实际聚合/分散时的数据结构相关性更高，故不给出特定的校正系数。


\subsection{样例分析}
\label{\detokenize{chapter_distributed_training/collective:id13}}
\sphinxAtStartPar
针对不同的集群性质，现代机器学习系统往往会灵活应用不同集合通信算子的组合来最大化通信效率。这里，我们提供了两个具体的案例分析，分别为微软的ZeRO
以及 OpenAI 的 DALL—E。


\subsubsection{ZeRO}
\label{\detokenize{chapter_distributed_training/collective:zero}}
\sphinxAtStartPar
ZeRO
（\DUrole{bibtex}{{[}rajbhandari2020zero{]}}）是微软提出的神经网络优化器，可用于训练千亿级参数的神经网络，也在实践中成功训练了当时世界上最大的语言模型（为高达170亿参数的transformer）。在训练这个级别的神经网络时主要遇到的问题是巨量参数对于加速器内存的占用，其中包括优化器本身的参数，反向传播时的梯度，以及模型参数本身。通过简易的计算不难得出，170亿参数的模型在32位浮点表示情况下会占用至少680GB的内存，远超于现在内存最高的深度学习加速器A100
（最高内存80GB）。于是，我们需要考虑如何高效的把模型切成数份存储在不同的加速器上，以及如何高效的通过使用集合通信算子来进行模型训练和推理。ZeRO对此提出了多个优化方法，这里例举了三个典型的例子：
1.
首先，可以发现在现代集群中，节点内部加速器的带宽往往比节点之间的带宽要大很多。这在某种程度上偏离了上文中的理论框架。为此，我们需要尽量减少节点间的通信，尽量保证大部分通信仅存在于节点内部的加速器之间。在观察模型切分时，不难看出模型本身前馈和反向传播时需要大量的在不同切片之间通信，相比下来不同模型拷贝之间的梯度聚合反而具有相对较少的通信量。针对这一特性，ZeRO选择了将单一模型的全部切片存储到同一节点内部，从而大大提高了训练效率。
2.
进一步地，假设模型中的参数在层的细粒度上呈线性，便可将其从前到后分别存储到不同加速其中。在前馈时，可以注意到某一层的计算仅依赖于其相邻层的参数。对此，与其是手动设计点到点通信，我们可以对所有包含模型参数的加速器进行一次AllGather计算，用来提取每一层之后一层的参数，以及计算该层本身的激活值。为了节约内存，我们在AllGather结束后立即丢弃除了该层以外其他层的参数。
3.
同理，在反向传播时我们只需要前一层的参数来计算本层的激活值和梯度，因此我们只需要再次使用AllGather来完成每个加速器上的梯度计算。同时，我们注意到在聚集梯度后，对于每个加速器我们仅需要在内存中的层数的梯度。对此，我们可以使用ReduceScatter算子来在平均后直接把相应的梯度存到编号为i的加速器上，而不是通常情况下的AllReduce。


\subsubsection{DALL\sphinxhyphen{}E}
\label{\detokenize{chapter_distributed_training/collective:dall-e}}
\sphinxAtStartPar
DALL\sphinxhyphen{}E
（\DUrole{bibtex}{{[}ramesh2021zero{]}}）是OpenAI提出的一个基于文字的图片生成模型，模型同样拥有高达120亿参数。在训练时，除了运用到ZeRO所使用的AllGather
+ ReduceScatter
技巧，OpenAI团队在细节上做了进一步的优化，以达到更快的训练速度。这里，我们简略介绍以下和集合通信相关的两点：
1.
我们注意到，集合通信算子的运行速度和通信本身的长度正相关。在模型训练中，这代表了模型参数本身的大小。对此，DALL\sphinxhyphen{}E
选择用矩阵分解（matrix
factorization）的方法先把高维张量调整为一个二维矩阵，通过分解后分开用集合通信算子进行传输，从而大大减少了通信量。
2.
另一个减少通信量的方法在于数据类型本身。一个显然的做法是使用16位的半精度浮点数，相比正常的32位参数表示可以节省近一倍的通信量。但是，在实践中发现低精度的数据类型会使得模型收敛不稳定，往往导致最终训练效果大打折扣。为此，OpenAI分析了DALL—E
的模型结构，并把其中的参数根据对数据类型精度的敏感性分为了多个类。其中对精度最敏感的一类照常使用32位浮点表示并只通过AllReduce来同步，而最不敏感的参数则照常通过矩阵分解进行压缩和传输。对于比较敏感的一类，例如Adam
优化其中的动能（moments）和方差（variance）参数，OpenAI 基于 IEEE 754
标准实现了两个全新的数据类型：1\sphinxhyphen{}6\sphinxhyphen{}9和0\sphinxhyphen{}6\sphinxhyphen{}10（其中第一表示正负所需的位数，第二表示指数所需的位数，第三表示有效数字所需的位数），在节省空间和保持收敛性能之间找到了一个平衡。


\subsection{集合通信与机器学习系统}
\label{\detokenize{chapter_distributed_training/collective:id16}}
\sphinxAtStartPar
最后，集合通信已经被深度集成到了整个机器学习系统之中，以至于一些在库级别以上的开发者很难意识到系统在训练和推理时的一些步骤是由底层逻辑实现的。
一般来说，不同的机器学习系统对于集合通信一般提供了两个级别的抽象，分别是更与硬件耦合的，可以直接调用集合通信算子的库，和更偏向神经网络实现的，通过内部调用集合通信算子来实现分布式训练和推理的深度学习框架。作为算法工程师，通常会接触到后者的抽象（包括Horovod,
KungFu, TensorFlow
distributed等），而作为集群的维护者，往往需要深入了解前者的运行原理和具体的调试方法。以深度学习框架
PyTorch 举例，在torch.distributed
命名空间（namespace）下实现了一系列方便开发者使用的分布式模型训练和推理函数。在其内部，会根据实际运行的集群调用更底层的集合通信算子库，例如MPI，NCCL（前文中已有介绍，适用于GPU分布式训练），gloo（适用于CPU分布式训练）等。我们来具体对比PyTorch
distributed 中对于AllReduce 的应用和 NCCL
的差异性：下面两段代码中，前者（\DUrole{bibtex}{{[}li2022ddp{]}}）通过PyTorch自带的分布式数据并行（Distributed
Data
Parallel）方法完成了一次简易的深度学习模型计算，后者则通过gloo的Python
接口pygloo和Ray（\sphinxcite{chapter_reinforcement_learning/summary:moritz2018ray}）完成了一个二维张量的AllReduce计算。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{k+kn}{import} \PYG{n+nn}{tempfile}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{distributed} \PYG{k}{as} \PYG{n+nn}{dist}
\PYG{k+kn}{import} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{nn} \PYG{k}{as} \PYG{n+nn}{nn}
\PYG{k+kn}{import} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{optim} \PYG{k}{as} \PYG{n+nn}{optim}
\PYG{k+kn}{import} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{multiprocessing} \PYG{k}{as} \PYG{n+nn}{mp}

\PYG{k+kn}{from} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{nn}\PYG{n+nn}{.}\PYG{n+nn}{parallel} \PYG{k+kn}{import} \PYG{n}{DistributedDataParallel} \PYG{k}{as} \PYG{n}{DDP}

\PYG{k}{def} \PYG{n+nf}{setup}\PYG{p}{(}\PYG{n}{rank}\PYG{p}{,} \PYG{n}{world\PYGZus{}size}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{environ}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MASTER\PYGZus{}ADDR}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{localhost}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{environ}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MASTER\PYGZus{}PORT}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{12355}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{n}{dist}\PYG{o}{.}\PYG{n}{init\PYGZus{}process\PYGZus{}group}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gloo}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{rank}\PYG{o}{=}\PYG{n}{rank}\PYG{p}{,} \PYG{n}{world\PYGZus{}size}\PYG{o}{=}\PYG{n}{world\PYGZus{}size}\PYG{p}{)}

\PYG{k}{class} \PYG{n+nc}{ToyModel}\PYG{p}{(}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{Module}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{ToyModel}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{net1} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{Linear}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{relu} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{ReLU}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{net2} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{Linear}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{net2}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{relu}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{net1}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{demo\PYGZus{}basic}\PYG{p}{(}\PYG{n}{rank}\PYG{p}{,} \PYG{n}{world\PYGZus{}size}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{setup}\PYG{p}{(}\PYG{n}{rank}\PYG{p}{,} \PYG{n}{world\PYGZus{}size}\PYG{p}{)}

    \PYG{n}{model} \PYG{o}{=} \PYG{n}{ToyModel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{rank}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} 通过调用DDP将模型在每个处理器上完成初始化}
    \PYG{n}{ddp\PYGZus{}model} \PYG{o}{=} \PYG{n}{DDP}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,} \PYG{n}{device\PYGZus{}ids}\PYG{o}{=}\PYG{p}{[}\PYG{n}{rank}\PYG{p}{]}\PYG{p}{)}

    \PYG{n}{loss\PYGZus{}fn} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{MSELoss}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{optim}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{n}{ddp\PYGZus{}model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{)}

    \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{zero\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{outputs} \PYG{o}{=} \PYG{n}{ddp\PYGZus{}model}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{labels} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{rank}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} 在反向传播时，框架内部会执行AllReduce算法}
    \PYG{n}{loss\PYGZus{}fn}\PYG{p}{(}\PYG{n}{outputs}\PYG{p}{,} \PYG{n}{labels}\PYG{p}{)}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{run\PYGZus{}demo}\PYG{p}{(}\PYG{n}{demo\PYGZus{}fn}\PYG{p}{,} \PYG{n}{world\PYGZus{}size}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{mp}\PYG{o}{.}\PYG{n}{spawn}\PYG{p}{(}\PYG{n}{demo\PYGZus{}fn}\PYG{p}{,}
             \PYG{n}{args}\PYG{o}{=}\PYG{p}{(}\PYG{n}{world\PYGZus{}size}\PYG{p}{,}\PYG{p}{)}\PYG{p}{,}
             \PYG{n}{nprocs}\PYG{o}{=}\PYG{n}{world\PYGZus{}size}\PYG{p}{,}
             \PYG{n}{join}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
    \PYG{n}{n\PYGZus{}gpus} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{cuda}\PYG{o}{.}\PYG{n}{device\PYGZus{}count}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{assert} \PYG{n}{n\PYGZus{}gpus} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Requires at least 2 GPUs to run, but got }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{n\PYGZus{}gpus}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{n}{run\PYGZus{}demo}\PYG{p}{(}\PYG{n}{demo\PYGZus{}basic}\PYG{p}{,} \PYG{n}{n\PYGZus{}gpus}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{ray}
\PYG{k+kn}{import} \PYG{n+nn}{pygloo}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{multiprocessing}

\PYG{n+nd}{@ray}\PYG{o}{.}\PYG{n}{remote}\PYG{p}{(}\PYG{n}{num\PYGZus{}cpus}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{k}{def} \PYG{n+nf}{test\PYGZus{}allreduce}\PYG{p}{(}\PYG{n}{rank}\PYG{p}{,} \PYG{n}{world\PYGZus{}size}\PYG{p}{,} \PYG{n}{fileStore\PYGZus{}path}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{context} \PYG{o}{=} \PYG{n}{pygloo}\PYG{o}{.}\PYG{n}{rendezvous}\PYG{o}{.}\PYG{n}{Context}\PYG{p}{(}\PYG{n}{rank}\PYG{p}{,} \PYG{n}{world\PYGZus{}size}\PYG{p}{)}
    \PYG{n}{attr} \PYG{o}{=} \PYG{n}{pygloo}\PYG{o}{.}\PYG{n}{transport}\PYG{o}{.}\PYG{n}{tcp}\PYG{o}{.}\PYG{n}{attr}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{localhost}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{dev} \PYG{o}{=} \PYG{n}{pygloo}\PYG{o}{.}\PYG{n}{transport}\PYG{o}{.}\PYG{n}{tcp}\PYG{o}{.}\PYG{n}{CreateDevice}\PYG{p}{(}\PYG{n}{attr}\PYG{p}{)}
    \PYG{n}{fileStore} \PYG{o}{=} \PYG{n}{pygloo}\PYG{o}{.}\PYG{n}{rendezvous}\PYG{o}{.}\PYG{n}{FileStore}\PYG{p}{(}\PYG{n}{fileStore\PYGZus{}path}\PYG{p}{)}
    \PYG{n}{store} \PYG{o}{=} \PYG{n}{pygloo}\PYG{o}{.}\PYG{n}{rendezvous}\PYG{o}{.}\PYG{n}{PrefixStore}\PYG{p}{(}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{world\PYGZus{}size}\PYG{p}{)}\PYG{p}{,} \PYG{n}{fileStore}\PYG{p}{)}

    \PYG{n}{context}\PYG{o}{.}\PYG{n}{connectFullMesh}\PYG{p}{(}\PYG{n}{store}\PYG{p}{,} \PYG{n}{dev}\PYG{p}{)}

    \PYG{n}{sendbuf} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
    \PYG{n}{recvbuf} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros\PYGZus{}like}\PYG{p}{(}\PYG{n}{sendbuf}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
    \PYG{n}{sendptr} \PYG{o}{=} \PYG{n}{sendbuf}\PYG{o}{.}\PYG{n}{ctypes}\PYG{o}{.}\PYG{n}{data}
    \PYG{n}{recvptr} \PYG{o}{=} \PYG{n}{recvbuf}\PYG{o}{.}\PYG{n}{ctypes}\PYG{o}{.}\PYG{n}{data}

    \PYG{c+c1}{\PYGZsh{} 标明发送者和者并直接调用AllReduce}
    \PYG{n}{pygloo}\PYG{o}{.}\PYG{n}{allreduce}\PYG{p}{(}\PYG{n}{context}\PYG{p}{,} \PYG{n}{sendptr}\PYG{p}{,} \PYG{n}{recvptr}\PYG{p}{,}
                    \PYG{n}{sendbuf}\PYG{o}{.}\PYG{n}{size}\PYG{p}{,} \PYG{n}{pygloo}\PYG{o}{.}\PYG{n}{glooDataType\PYGZus{}t}\PYG{o}{.}\PYG{n}{glooFloat32}\PYG{p}{,}
                    \PYG{n}{pygloo}\PYG{o}{.}\PYG{n}{ReduceOp}\PYG{o}{.}\PYG{n}{SUM}\PYG{p}{,} \PYG{n}{pygloo}\PYG{o}{.}\PYG{n}{allreduceAlgorithm}\PYG{o}{.}\PYG{n}{RING}\PYG{p}{)}

\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
    \PYG{n}{ray}\PYG{o}{.}\PYG{n}{init}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{world\PYGZus{}size} \PYG{o}{=} \PYG{n}{multiprocessing}\PYG{o}{.}\PYG{n}{cpu\PYGZus{}count}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{fileStore\PYGZus{}path} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ray}\PYG{o}{.}\PYG{n}{worker}\PYG{o}{.}\PYG{n}{\PYGZus{}global\PYGZus{}node}\PYG{o}{.}\PYG{n}{get\PYGZus{}session\PYGZus{}dir\PYGZus{}path}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/collective/gloo/rendezvous}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{makedirs}\PYG{p}{(}\PYG{n}{fileStore\PYGZus{}path}\PYG{p}{)}
    \PYG{n}{ray}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{p}{[}\PYG{n}{test\PYGZus{}allreduce}\PYG{o}{.}\PYG{n}{remote}\PYG{p}{(}\PYG{n}{rank}\PYG{p}{,} \PYG{n}{world\PYGZus{}size}\PYG{p}{,} \PYG{n}{fileStore\PYGZus{}path}\PYG{p}{)} \PYG{k}{for} \PYG{n}{rank} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{world\PYGZus{}size}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
可以注意到，前者并没有显式的调用集合通信算子，而是通过DistributedDataParallel将分布式训练和正常训练之间的不同隐藏了起来。如果我们需要在不同集群上运行这段代码，只需要在setup
函数内相对的更改PyTorch使用的底层集合通信库即可。在backward函数被调用时，才会真正的使用AllReduce算法。相比下来，如果想要直接使用gloo，不仅需要使用一步一步的创建通信所需要的数据结构，同时也很难和现有的模型训练框架无缝连接。


\section{参数服务器}
\label{\detokenize{chapter_distributed_training/parameter_servers:id1}}\label{\detokenize{chapter_distributed_training/parameter_servers::doc}}
\sphinxAtStartPar
接下来，我们介绍另一种常见的分布式训练系统实现：参数服务器。常见的深度学习框架以不同方式提供了参数服务器。TensorFlow和MindSpore原生提供了参数服务器的实现；PyTorch需要用户使用框架提供的Rpc接口自行实现；还有一些框架则需要用户使用第三方的参数服务器实现，例如PS\sphinxhyphen{}Lite。


\subsection{计算和存储分离}
\label{\detokenize{chapter_distributed_training/parameter_servers:id2}}
\sphinxAtStartPar
利用参数服务器的其中一个核心需求是实现：计算和存储的分离。在训练模型中，计算可以被理解为计算更新模型参数所需要的计算（例如说，计算本地梯度和计算平均梯度），而存储可以被理解为将模型参数存储在内存设备中（例如说，主机内存，加速卡内存和SSD设备）。传统的神经网络训练中，计算往往是核心瓶颈，因此我们只需要配置有合适数量的带有加速卡的服务器，常被称为训练服务器（Training
servers）。

\sphinxAtStartPar
随着机器学习的发展，新型的稀疏模型被开发出来。相比于传统的神经网络训练，稀疏模型的训练往往不需要大量昂贵的计算加速卡（GPU），而需要海量的内存来存储嵌入表（Embedding
table）。例如说，一个大型深度学习推荐系统中，它们往往使用小型的深度神经网络（如Multi\sphinxhyphen{}layer
Perception），训练这种神经网络只需要几个GPU即可。而另一方面，推荐系统中往往需要存储PB级别的嵌入表。嵌入表往往由推荐系统的用户特征（User
feature）和产品特征（Item
feature）构成。这些特征往往是大型向量（Vector）。现代推荐系统需要服务数亿的用户，推荐数以千万的商品。假设用户的特征是1MB，而系统需要服务10亿的用户，那么用户的嵌入表就会有1PB的大小。而这个大小远远超过了一个深度学习服务器所具有的内存。假如我们部署大量的昂贵的深度学习服务器来存储海量嵌入表，那么这些服务器上的加速卡的使用率将会极低，无法实现对于硬件的高效利用。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-parameter-servers}.png}
\caption{参数服务器}\label{\detokenize{chapter_distributed_training/parameter_servers:id5}}\label{\detokenize{chapter_distributed_training/parameter_servers:ch10-parameter-servers}}\end{figure}

\sphinxAtStartPar
为了解决上述问题，人们往往会在稀疏模型集群中混合部署：训练服务器和参数服务器，从而实现对于计算需求和内存需求分别满足。
\hyperref[\detokenize{chapter_distributed_training/parameter_servers:ch10-parameter-servers}]{图\ref{\detokenize{chapter_distributed_training/parameter_servers:ch10-parameter-servers}}}
描述了带有参数服务器的机器学习集群。这个集群中含有2个训练服务器和2个参数服务器，训练服务器一般是拥有加速卡的计算优化服务器（Compute\sphinxhyphen{}optimised
server）。而参数服务器一般是内存优化服务器（Memory\sphinxhyphen{}optimised
server），其的内存大小一般远远大于计算优化服务器。在一个稀疏模型中往往拥有神经网络参数和嵌入表参数。神经网络较小，其可以存储在训练服务器内存中。而嵌入表很大，因此需要存储在额外的参数服务器中。参数服务器一般会按照键\sphinxhyphen{}值对（Key\sphinxhyphen{}value
pairs）的方式来存储参数。常用的键包括用户名（User ID），产品名（Item
ID）或者是参数名（Parameter
Key）。常用的值是以多维度向量（Multi\sphinxhyphen{}dimensional
tensors）表达的模型参数。假如存在多个参数服务器，参数服务器会用数据分区函数（例如，哈希函数和区域划分）将健\sphinxhyphen{}值映射到不同参数服务器上。

\sphinxAtStartPar
为了完成对于模型的训练，在每一步训练中，训练服务器会根据当前的小批量训练数据，找到本批量中需要用到的参数。例如说，本小批量数据只会训练部分用户的特征，那么这些用户的特征才会需要。根据参数服务器的数据分区函数，训练服务器可以知道参数当前在哪个参数服务器上，它们因此会用参数的键（Key）向对应的参数服务器发起拉取请求（Pull
request）。参数服务器响应，并返回对应的值（Value）。训练服务器将拉取的参数（往往是嵌入表）和本地内存中的模型参数（往往是神经网络）进行合并，从而对合并的模型进行训练，计算梯度。假如训练服务器实现了数据并行，那么训练服务器计算出的本地梯度需要利用Allreduce计算出平均梯度。对于训练服务器本地内存中的参数，训练服务器可以马上利用平均梯度进行修改。对于在参数服务器中存储的参数，训练服务器发起推送请求（Push
request）将平均梯度发送到参数服务器，参数服务器更新本地存储的参数。

\sphinxAtStartPar
在以上的参数服务器架构中，机器学习集群拥有者可以灵活的根据梯度计算所需要算力配置合理数量的训练服务器。他们也可以根据参数的数量配置大部分的稀疏参数（Sparse
parameters）在参数服务器中，仅留下小部分的密集参数（Dense
parameters）在训练服务器中。密集参数和稀疏参数的核心区别是：稀疏参数在每一步训练不一定都会被用到，他们需要根据当前训练小批量来决定。而密集参数每一步训练都需要用到。因此为了避免频繁从参数服务器中拉取，密集参数往往会存储在训练服务器中。


\subsection{数据副本}
\label{\detokenize{chapter_distributed_training/parameter_servers:id3}}
\sphinxAtStartPar
在参数服务器的实际部署中，人们往往需要解决数据热点问题。互联网数据往往符合幂律概率（Power\sphinxhyphen{}law
distribution），这会导致部分稀疏参数在训练过程中被访问的次数会显著高于其他参数。例如说，热门商品的特征向量被训练服务器拉取的次数就会远远高于非热门商品。因此，存储了热门数据的参数服务器所承受的数据拉取和推送请求会远远高于其他参数服务器，因此形成数据热点，伤害了系统的可扩展性。

\sphinxAtStartPar
解决数据热点问题的关键是利用在没有副本的情况下，通用的做法是每隔一段时间将所有参数在外存中保存一份检查点（checkpoint）。当出现机器故障时，首先所有的训练必须停止，等待故障的机器恢复上线，然后从外存中重新加载检查点。这就会导致从上一次保存检查点到故障发生时的数据全部丢失。保存一次检查点的开销随模型大小而增加，训练大模型时通常每隔1\sphinxhyphen{}2小时保存一次。因此无副本的参数服务器如果发生故障，会丢失最多1\sphinxhyphen{}2小时的数据。

\sphinxAtStartPar
解决参数服务器故障和数据热点问题的常用技术是构建模型主从副本（Master\sphinxhyphen{}slave
replication）。一份参数在多个机器上拥有副本，并指定其中一个副本作为主副本。训练服务器的所有更新操作都向主副本写入并同步至从副本上。如何取得共识确定哪一个副本是主副本是分布式系统领域一个经典问题，已经有了相当多的成熟的算法，例如Paxos和Raft。此外，主副本上的更新如何复制到从副本上也同样是分布式系统领域的经典共识问题。通常系统设计者需要在可用性（Availability）和一致性（Consistency）之间做出取舍。如果参数服务器副本间采用强一致性的复制协议（例如，链式副本(Chain
replication)）则可能导致训练服务器的推送请求失败，即参数服务器不可用。反之，如果参数服务器采用弱一致性的复制协议，则可能导致副本间存储的参数不一致。


\subsection{掉队者问题}
\label{\detokenize{chapter_distributed_training/parameter_servers:id4}}
\sphinxAtStartPar
参数服务器的另一大核心作用是可以让用户方便解决掉队者问题。在之前的讨论中，在每一步训练结束后，训练服务器都需要计算平均梯度来对每一个模型副本进行更新，从而保证下一步训练开始前，全部模型副本的参数的一致性，这种对于参数一致性的确保一般被称为同步训练（Synchronous
training）。同步训练一般会有助于训练系统达到更好的模型精度，但是当系统规模变大，我们往往会在系统中引入掉队者（Straggler）。掉队者出现的原因很多。常见的原因包括：掉队者设备可能和其他设备不在同一个机柜中，因此掉队者的通讯带宽显著小于其他设备。另外，掉队者设备也可能和其他进程共享本地的服务器计算和通讯资源，形成资源竞争，从而降低了性能。

\sphinxAtStartPar
掉队者对于基于Allreduce的同步训练系统的性能有显著影响，这是因为Allreduce让全部节点参与到平均梯度的计算和通讯中，而每个节点负责等量的数据。因此任何一个掉队者的出现，都会让整个Allreduce操作延迟完成。为了解决这个问题，人们也会使用参数服务器来计算平均梯度。一种常见的设计是：训练服务器训练出本地梯度后，会把本地梯度全部推送到参数服务器。参数服务器在等到一定数据训练服务器（例如说90\%的训练服务器）的本地梯度后，就开始计算平均梯度。这样可以确保平均梯度的计算不会被落后者的出现延误。计算好的平均梯度马上推送给全部训练服务器，开始下一轮训练。

\sphinxAtStartPar
解决掉队者的另外一种常见做法是利用参数服务器实现\sphinxstylestrong{异步训练}(Asynchronous
training)。在一个异步训练系统中，每个训练服务器在训练开始时，有相同的模型参数副本。在训练中，他们计算出本地梯度后会马上将本地梯度推送到参数服务器，参数服务器将推送的梯度立刻用于更新参数，并把更新好的参数马上推送回对应的训练服务器。在这个过程中，不同的训练服务器很可能会使用不同版本的模型参数进行本地梯度的计算，这种做法有可能会伤害模型的精度，但它同时让不同训练服务器可以按照各自的运算速度来推送和拉取参数，而无需等待同伴，因此避免了掉队者对于整个集群性能的影响。


\section{总结}
\label{\detokenize{chapter_distributed_training/summary:id1}}\label{\detokenize{chapter_distributed_training/summary::doc}}\begin{itemize}
\item {} 
\sphinxAtStartPar
大型机器学习模型的出现带来了对于算力和内存需求的快速增长，催生了分布式训练系统的出现。

\item {} 
\sphinxAtStartPar
分布式训练系统的设计往往遵循“分而治之”的设计思路。

\item {} 
\sphinxAtStartPar
利用分布式训练系统，人们可以显著提升性能，经济性，并且帮助抵御硬件故障。

\item {} 
\sphinxAtStartPar
分布式训练系统可以通过数据并行增加设备来提升算力。

\item {} 
\sphinxAtStartPar
当单节点内存不足时，我们可以通过模型并行来解决单设备内存不足。模型并行有两种实现方式：算子内并行和算子间并行。

\item {} 
\sphinxAtStartPar
大型模型并行系统容易出现设备使用空洞，而这种空洞可以通过流水线并行解决。

\item {} 
\sphinxAtStartPar
分布式训练系统往往运行在商用数据中心之中，数据中心网络无法提供充足的网络带宽来传输大量训练中生成的梯度。

\item {} 
\sphinxAtStartPar
为了提供海量的带宽，机器学习集群拥有异构的网络：以太网，机内网络（NVLink）和InfiniBand。

\item {} 
\sphinxAtStartPar
为了解决单节点瓶颈，我们可以使用Allreduce来分摊梯度聚合过程中的计算和通讯开销。

\item {} 
\sphinxAtStartPar
参数服务器可以帮助机器学习集群实现计算\sphinxhyphen{}存储的分离，从而更好的支持大型稀疏模型。

\item {} 
\sphinxAtStartPar
参数服务器常用数据副本技术解决数据热点问题，同时它们也可以被用来解决同步训练系统中常见的掉队者问题。

\end{itemize}


\section{扩展阅读}
\label{\detokenize{chapter_distributed_training/summary:id2}}\begin{itemize}
\item {} 
\sphinxAtStartPar
分布式机器学习系统：\sphinxhref{https://dl.acm.org/doi/abs/10.1145/3377454}{综述}%
\begin{footnote}[44]\sphinxAtStartFootnote
\sphinxnolinkurl{https://dl.acm.org/doi/abs/10.1145/3377454}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
利用集合通信支持并行训练的实践：\sphinxhref{https://arxiv.org/abs/1802.05799}{Horovod}%
\begin{footnote}[45]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/1802.05799}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
AllReduce的工程实现细节：\sphinxhref{https://developer.nvidia.com/blog/massively-scale-deep-learning-training-nccl-2-4/}{树形结构}%
\begin{footnote}[46]\sphinxAtStartFootnote
\sphinxnolinkurl{https://developer.nvidia.com/blog/massively-scale-deep-learning-training-nccl-2-4/}
%
\end{footnote}，\sphinxhref{https://github.com/baidu-research/baidu-allreduce}{环形结构}%
\begin{footnote}[47]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/baidu-research/baidu-allreduce}
%
\end{footnote}，\sphinxhref{https://arxiv.org/abs/1811.05233}{二维环面结构}%
\begin{footnote}[48]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/1811.05233}
%
\end{footnote}，以及\sphinxhref{https://github.com/NVIDIA/nccl/issues/320}{CollNet算法}%
\begin{footnote}[49]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/NVIDIA/nccl/issues/320}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
流水线并行的实践：\sphinxhref{https://arxiv.org/abs/1811.06965}{gPipe}%
\begin{footnote}[50]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/1811.06965}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
在大规模数据并行下的实践：\sphinxhref{https://arxiv.org/abs/1706.02677}{Accurate, Large Minibatch SGD: Training
ImageNet in 1 Hour}%
\begin{footnote}[51]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/1706.02677}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
模型并行在超大模型上的实践：\sphinxhref{https://arxiv.org/abs/1910.02054}{ZeRO}%
\begin{footnote}[52]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/1910.02054}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
最后，在讨论集合通信时，经常可以看到一些关于底层通信接口的专业术语，例如以太网，Infiniband
等。这里给出一些常见术语的具体定义：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://web.archive.org/web/20181222184046/http://www.mef.net/Assets/White\_Papers/Metro-Ethernet-Services.pdf}{以太网（Ethernet)}%
\begin{footnote}[53]\sphinxAtStartFootnote
\sphinxnolinkurl{https://web.archive.org/web/20181222184046/http://www.mef.net/Assets/White\_Papers/Metro-Ethernet-Services.pdf}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://devblogs.nvidia.com/parallelforall/how-nvlink-will-enable-faster-easier-multi-gpu-computing/}{NVLink}%
\begin{footnote}[54]\sphinxAtStartFootnote
\sphinxnolinkurl{https://devblogs.nvidia.com/parallelforall/how-nvlink-will-enable-faster-easier-multi-gpu-computing/}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://aws.amazon.com/cn/hpc/efa/}{AWS Elastic Fabric Adapter
(EFA)}%
\begin{footnote}[55]\sphinxAtStartFootnote
\sphinxnolinkurl{https://aws.amazon.com/cn/hpc/efa/}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.infinibandta.org/about-infiniband/}{Infiniband}%
\begin{footnote}[56]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.infinibandta.org/about-infiniband/}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
\sphinxhref{http://reports.ias.ac.in/report/12829/understanding-the-concepts-and-mechanisms-of-rdma}{RDMA}%
\begin{footnote}[57]\sphinxAtStartFootnote
\sphinxnolinkurl{http://reports.ias.ac.in/report/12829/understanding-the-concepts-and-mechanisms-of-rdma}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.roceinitiative.org/about-overview/}{RoCE}%
\begin{footnote}[58]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.roceinitiative.org/about-overview/}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.ibm.com/docs/en/aix/7.2?topic=protocol-internet-over-infiniband-ipoib}{IPoIB}%
\begin{footnote}[59]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.ibm.com/docs/en/aix/7.2?topic=protocol-internet-over-infiniband-ipoib}
%
\end{footnote}

\end{itemize}

\end{itemize}


\section{参考文献}
\label{\detokenize{chapter_distributed_training/summary:id3}}
\sphinxAtStartPar



\chapter{第三部分：拓展篇}
\label{\detokenize{chapter_preface_extension/index:id1}}\label{\detokenize{chapter_preface_extension/index::doc}}
\sphinxAtStartPar
在本书的第三部分，我们将会介绍机器学习框架生态下的众多拓展应用。我们将会首先介绍深度学习推荐系统，这一类系统是深度学习应用最成功的领域之一，在各大互联网公司中得到了大量部署。
其后，我们将会介绍联邦学习系统，这一类希望在数据隐私保护意识快速崛起的今天具有举足轻重的作用。我们同时也会介绍可解释性AI系统，这一类系统能够对机器学习推理的过程给出解释，因此在金融，医疗等安全攸关系统（Safety\sphinxhyphen{}critical
System）中得到大量应用。
最后，我们会介绍强化学习系统，这一类系统可以帮助计算机和人类进行自动化决策，是实现通用人工智能的关键基础设施。

\sphinxAtStartPar
机器学习系统作为一个蓬勃发展的领域，我们也会在不远的将来看到更多相关系统的崛起。我们也欢迎社区贡献更多的章节进入拓展篇，如有想法可以和本书的编辑们联系。


\chapter{深度学习推荐系统}
\label{\detokenize{chapter_recommender_system/index:id1}}\label{\detokenize{chapter_recommender_system/index::doc}}
\sphinxAtStartPar
推荐系统通过对用户特征、历史行为等数据的分析，为用户推荐可能感兴趣的内容、商品或者广告。在信息爆炸的时代，高效且准确的推荐结果极大地提升了在线服务的质量。近年来，基于深度学习的推荐模型由于可以高效地利用在线服务中产生的海量数据，被谷歌、脸书、阿里巴巴等各大公司广泛应用于生产环境中。本节主要介绍深度学习推荐系统在工业界的主流系统架构、问题以及可能的解决方案。


\section{背景}
\label{\detokenize{chapter_recommender_system/overview:id1}}\label{\detokenize{chapter_recommender_system/overview::doc}}
\sphinxAtStartPar
为了方便本节的讨论，我们首先简单介绍一些推荐系统的基本概念，包括三部分：推荐模型的结构，推荐系统的架构，和评估推荐系统的关键指标。


\subsection{推荐模型}
\label{\detokenize{chapter_recommender_system/overview:id2}}
\sphinxAtStartPar
基于深度学习的推荐模型在过去几年受到了学术界和工业界的高度关注，得到了快速发展。目前主流的推荐模型
\sphinxcite{chapter_recommender_system/summary:id4,chapter_recommender_system/summary:id6,chapter_recommender_system/summary:ijcai2017-239,chapter_recommender_system/summary:naumov2019deep}的基本结构可以总结如图
\hyperref[\detokenize{chapter_recommender_system/overview:ch10-recommendation-models}]{图\ref{\detokenize{chapter_recommender_system/overview:ch10-recommendation-models}}}。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-recommendation-models}.png}
\caption{推荐模型的基本结构}\label{\detokenize{chapter_recommender_system/overview:id11}}\label{\detokenize{chapter_recommender_system/overview:ch10-recommendation-models}}\end{figure}

\sphinxAtStartPar
推荐模型以用户和内容的交互历史、用户属性、内容属性等特征作为输入，令输入特征进行充分相互作用，再将交互结果交由稠密深度神经网络来预测用户点击候选内容的可能性。为了加深读者的对推荐模型的理解，此处我们以Wide
\& Deep模型
\sphinxcite{chapter_recommender_system/summary:id4}作为例子，深入分析推荐模型的输入特征以及输入特征之间如何交互。由于推荐模型的设计不在本章的讨论范围内，所以下面的介绍中重点介绍模型的基本结构以方便理解推荐系统的设计。对Wide
\&
Deep模型的设计理念、数据生成、数据预处理等细节感兴趣的读者可以自行阅读论文以进一步了解。

\sphinxAtStartPar
Wide \&
Deep模型是一个设计简洁然而性能优异的模型，由谷歌（Google）在开发并应用于谷歌应用商店中，该模型在谷歌的实际生产环境中可以大幅提升应用的下载率。

\sphinxAtStartPar
Wide \&
Deep模型的输入数据可以分为两类：连续特征（例如：用户年龄、用户已安装的应用数量、用户参与的会话数量等）和类别特征（例如：用户ID，用户性别属性、用户设备类型、应用ID等）。连续特征的值本身是具有实际意义的数字，可以直接参与后续模型的运算。而类别特征的值并不具有实际意义，所以需要使用嵌入表（Embedding
table）将类别特征转化为数字向量，即嵌入项（Embedding item）。

\sphinxAtStartPar
Wide \&
Deep模型对输入数据进行两部分处理：Wide部分和Deep部分。Wide部分计算用户已安装应用和候选应用的外积（Cross\sphinxhyphen{}product）。Deep部分将所有连续特征和类别特征的嵌入项拼接起来输入一个多层感知机（Multilayer
perceptron）。Wide部分和Deep部分按照一定规则拼接然后得到最终的输出。


\subsection{推荐系统}
\label{\detokenize{chapter_recommender_system/overview:id8}}
\sphinxAtStartPar
在实际的生产环境中，除了推荐模型本身，推荐系统通常包括：数据收集、数据处理、数据存储、模型训练、模型存储、模型评估、推理服务等多个子系统。如图
\hyperref[\detokenize{chapter_recommender_system/overview:ch10-abstract-recommendation-systems}]{图\ref{\detokenize{chapter_recommender_system/overview:ch10-abstract-recommendation-systems}}}所示，这些子系统之间分工协作、紧密配合，构成一个从用户反馈、到模型更新、再到新推荐结果生成的闭环。下一小节中将以英伟达（NVIDIA）公司的Merlin开源框架
\sphinxcite{chapter_recommender_system/summary:merlin}为例，概括介绍推荐系统流水线上的组件，并重点介绍模型训练、推理子系统的结构。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-abstract-recommendation-systems}.png}
\caption{推荐系统的基本组件}\label{\detokenize{chapter_recommender_system/overview:id12}}\label{\detokenize{chapter_recommender_system/overview:ch10-abstract-recommendation-systems}}\end{figure}


\subsection{关键指标}
\label{\detokenize{chapter_recommender_system/overview:id10}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{准确性：}
深度学习模型给出的推荐结果的准确性是推荐系统需要关注的一个基本指标。然而不同于一般学术论文中使用的基准数据集，生产环境中推荐模型面对的是动态变化的数据分布。例如，每天的热点内容都不尽相同，而且每个用户的兴趣也会不断变化。为了保持模型的性能，推荐系统需要不断根据用户的反馈对已有模型进行更新。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{可用性：}
除了推荐准确性，对于在线服务的提供者而言，可用性是一个非常关键的指标。当用户需要一个推荐结果时，相比于给用户一个不完全准确的推荐，“无响应”的结果对于用户的体验伤害更大。因此，在某种程度上可以说系统可用性是比推荐结果的准确性更加关键的一个指标。然而这并不意味着准确性不重要，在一定的资源限制下，在线推荐系统的设计者必须谨慎地在准确性和可用性之间进行平衡。例如，使用更宽、更深、更复杂的神经网络模型可能会给出更加准确的推荐结果，但如果其推断延迟高于给定的阈值，那么这样的模型不能直接运用于生产环境中。

\end{itemize}


\section{主流系统架构}
\label{\detokenize{chapter_recommender_system/system_architecture:id1}}\label{\detokenize{chapter_recommender_system/system_architecture::doc}}
\sphinxAtStartPar
在实际的生产环境中，除了推荐模型本身，一个企业级推荐系统通常包括从用户反馈数据收集，到模型训练，再到服务用户请求的完整流水线（pipeline）。甚至从系统角度来看，推荐模型固然是系统的核心，然而其本身的代码仅占推荐系统的很小一部分，而围绕推荐模型所构建的其他基础设施占据了系统的绝大部分
\DUrole{bibtex}{{[}NIPS2015\_86df7dcf{]}}。

\sphinxAtStartPar
本节以英伟达公司的Merlin开源框架
\sphinxcite{chapter_recommender_system/summary:merlin}为例，简单介绍推荐系统的完整流水线以及各个组件在其中的作用，关于Merlin特性的完整介绍请读者参考官方文档。
Merlin是英伟达公司开发的一个开源推荐系统框架，帮助使用者构建高效的、基于GPU的推荐系统。Merlin提供三个主要开源组件：NVTabular
\DUrole{bibtex}{{[}NVTabular{]}}，HugeCTR \DUrole{bibtex}{{[}HugeCTR{]}}和Triton
\DUrole{bibtex}{{[}Triton{]}}，分别对数据处理、模型训练核推理服务进行端到端加速。下面分别展开介绍流水线中的三个主要环节：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
数据处理：推荐系统首先从用户的推荐请求中记录用户的反馈数据（例如，用户是否对推荐结果做出了正向反应）到数据存储系统中。随后数据预处理系统对原始反馈数据进行格式化、清洗、重采样等操作生成训练数据。关于数据处理组件，本书在前面章节进行了详细介绍。在Merlin中，这一步骤由NVTabular负责。同时NVTabular还为多种模型训练框架提供了数据加载器（dataloader）。

\item {} 
\sphinxAtStartPar
模型训练：推荐模型每次迭代选择一批训练数据，拉取对应的嵌入项、并送入稠密神经网络，计算损失，然后反向传播计算梯度，最终更新嵌入表和稠密神经网络。正如上文提到的，嵌入表占据了推荐模型绝大部分存储而其更新具有显著的稀疏性，因此推荐系统通常采用上一章介绍的参数服务器架构来存储模型。具体来讲，所有参数被分布存储在一组参数服务器上，而训练服务器根据训练数据从参数服务器上拉取对应的嵌入项和所有稠密神经网络参数。训练服务器本地更新之后将本地梯度或新的参数发送回参数服务器以更新全局参数。全局参数更新可以选择全同步，半同步，或异步更新。为了提升训练的吞吐，可以在训练服务器上缓存一部分参数。HugeCTR为此提供了Embedding
Training Cache和GPU embedding
cache。为了避免训练服务器和参数服务器之间的通信限制训练吞吐率，一些公司也在探索单机多GPU训练超大规模推荐系统。然而正如前文提到的，即使是单个推荐模型的参数量（\textasciitilde{}100GB）也超出了目前最新的GPU显存。有鉴于此，脸书（Facebook）公司的定制训练平台
– ZionEX
\sphinxcite{chapter_reinforcement_learning/summary:zionex}利用计算设备之间的高速链接将多台设备的存储共享起来可以单机训练TB级推荐模型。然而对于更大规模的模型或中小型企业、实验室，参数服务器架构依然是性价比最高的解决方案。

\item {} 
\sphinxAtStartPar
推理服务：类似地，推理服务器在接到一批用户的推荐请求后，从参数服务器拉去相应的嵌入项和稠密神经网络参数来响应用户的请求。推荐系统的推理服务对延迟十分敏感，例如脸书公司的DLRM
\sphinxcite{chapter_recommender_system/summary:naumov2019deep}基准在MLPerf评测中的服务器延迟限定在30ms \sphinxstepexplicit %
\begin{footnote}[1]\phantomsection\label{\thesphinxscope.1}%
\sphinxAtStartFootnote
\sphinxurl{https://mlcommons.org/en/inference-datacenter-11/}
%
\end{footnote}。因此如何在限定延迟（latency\sphinxhyphen{}bounded）的情况下尽可能提升吞吐（throughput）是推理服务面临的关键问题。在GPU推理场景下，常见的优化手段有：请求动态合批处理、核融合、低精度部署等
\DUrole{bibtex}{{[}10.1145/3437801.3441578{]}}\DUrole{bibtex}{{[}wang\sphinxhyphen{}etal\sphinxhyphen{}2021\sphinxhyphen{}lightseq{]}}.
Triton提供了请求调度的功能并且支持多种不同的机器学习框架作为后端。

\end{enumerate}

\sphinxAtStartPar
在工业界，为了提升系统在发生故障的情况下的可用性，以上介绍的各个组件在实际中部署中都应该具备基本的容灾和故障恢复能力。以推理服务为例，在线服务中的深度学习推荐模型通常都采用多副本分布式部署。同一个模型的多个副本通常会被部署在至少两个不同的地理区域内的多个数据中心中，如图
\hyperref[\detokenize{chapter_recommender_system/system_architecture:ch10-recommendation-systems}]{图\ref{\detokenize{chapter_recommender_system/system_architecture:ch10-recommendation-systems}}}所示，以应对大面积停电或者网络中断而导致整个地区的所有副本都不可用。除了容错方面的考虑，部署多个副本还有其他几点优势。首先，将模型部署在靠近用户的云服务器上可以提升响应速度。其次，部署多份副本也可以拓展模型推理服务的吞吐率。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-recommendation-systems}.png}
\caption{推荐系统的分布式架构}\label{\detokenize{chapter_recommender_system/system_architecture:id13}}\label{\detokenize{chapter_recommender_system/system_architecture:ch10-recommendation-systems}}\end{figure}


\section{现有解决方案及其存在的问题}
\label{\detokenize{chapter_recommender_system/system_problem:id1}}\label{\detokenize{chapter_recommender_system/system_problem::doc}}
\sphinxAtStartPar
在线服务系统的两个主要诉求：
\begin{itemize}
\item {} 
\sphinxAtStartPar
大模型的高效存储。
为了提升训练和推理的性能，通常推荐模型全部存储在内存中，然而纯内存存储对于内存的需求极高。推荐模型的输入中包含大量无法直接进行矩阵运算的类别数据，而由于每种类别数据包含的每种情况都需要一个单独的嵌入项来表示，而稠密深度神经网络的参数可以共享，在大规模推荐模型中，嵌入表占据了绝大部分内存
\sphinxcite{chapter_reinforcement_learning/summary:mlsys2021-979d472a,chapter_reinforcement_learning/summary:mlsys2020-f7e6c855}。举例说明，假设一个推荐模型需要处理1亿条短视频内容，而每条短视频对应的嵌入项为一个64维的32位浮点数向量，那么仅该内容嵌入表就需要占据大约24GB内存。如果考虑到用户标识符等其他嵌入表，那么单个模型可以轻易占据近100GB内存。而在工业界生产环境中，TB级的推荐模型
\sphinxcite{chapter_reinforcement_learning/summary:mlsys2020-f7e6c855}也是非常常见的。此外，在线推荐系统中需要同时运行多个模型负责不同的服务，甚至同一个服务也会上线多个模型以供算法开发人员验证不同的模型结构或者训练策略，因此系统中通常会同时存在上百个超大模型。综上所述，在线推荐系统亟需既能拓展存储容量，又不会影响训练和推理性能的存储解决方案。

\item {} 
\sphinxAtStartPar
大模型的快速更新。
在线服务系统所面对的环境是复杂多变的，因此其中的机器学习模型必须不断更新以应对新的数据分布。以一个短视频推荐系统为例，其面对的变化主要来自三点。首先，每时每刻都有大量的新视频上传，这些新视频的特征分布和模型训练时所见到的数据不同；其次，对于不断加入的新用户，模型难以直接给出最优的推荐结果；最后，全部用户和内容之间的交互在不断改变，表现为热点视频在持续变化。因此，为了应对以上变化，在线服务中不可能奢望仅仅训练一次模型就能够一劳永逸地解决问题。目前业界主流的做法是利用新产生的数据不断地增量式更新所部属的模型。在学术界和工业界大量的研究和实践
\sphinxcite{chapter_reinforcement_learning/summary:id3,chapter_reinforcement_learning/summary:id4,chapter_reinforcement_learning/summary:id6,chapter_reinforcement_learning/summary:id7}中都发现模型更新可以有效缓解概念漂移带来的危害，而且更新的频率越高，模型的性能越好。

\end{itemize}

\sphinxAtStartPar
在线推荐系统对跨地域地部署的大模型进行快速更新的需求在现有的系统中很难得到满足。一种最直观的解决方案是周期性地将训练服务器上的模型参数发给所有副本。然而这种方式面临着非常大的资源瓶颈。我们以网络开销为例进行分析。假设负责训练的参数服务器存储有100GB的参数，每10分钟将所有参数（在训练集群内部，模型更新的速度极快，10分钟足够将所有参数更新多次）发给其余2个副本。这就需要至少2.6Gbps的网络带宽。然而我们的分析只是最基本的情况，没有考虑网络传输的额外开销以及可能出现的失败重传，也没有考虑需要水平扩展至更多副本、更大模型、更高的更新频率的情况。为了缓解网络瓶颈，人们不得不选择以更慢的速度更新更大的模型，或者限制模型大小以追求更快的更新速度。简单的广播模型参数除了会有很大的资源瓶颈，还无法保证多副本之间的一致性。然而如果采用先前的数据库系统来保证一致性，只能使得资源开销更加严重，进一步限制系统的规模和效率。


\section{未来可以探索的方向}
\label{\detokenize{chapter_recommender_system/future:id1}}\label{\detokenize{chapter_recommender_system/future::doc}}
\sphinxAtStartPar
为了解决在线深度学习推荐系统的以上几点问题，研究人员也探索了几个潜在的方向。
\begin{itemize}
\item {} 
\sphinxAtStartPar
云–边–端协同推荐系统。随着边缘设备的增加以及用户端设备性能逐渐增强，服务提供者可以通过将部分计算服务从云服务器下放至边缘服务器乃至用户的设备上来提高模型的反应速度。例如，有研究
\sphinxcite{chapter_reinforcement_learning/summary:gong2020edgerec}探索了将模型的前几层下放至客户端上，并且利用用户的本地数据进行个性化训练以给出更加准确的推荐结果。当用户的兴趣发生改变时，客户端上的小模型可以实时地更新以响应用户的请求。除此之外，还可以借鉴联邦学习中的概念，例如有研究
\sphinxcite{chapter_reinforcement_learning/summary:neurips2020-a1d4c20b}探索了利用知识迁移的方法在云\sphinxhyphen{}端之间传递信息。在在线推荐系统中使用这种方法可以彻底解耦云上的大模型与客户端的小模型。

\item {} 
\sphinxAtStartPar
异构硬件多级存储。前文提到GPU显存无法装下完整的模型参数，一些现有的系统
\sphinxcite{chapter_reinforcement_learning/summary:mlsys2020-f7e6c855}为了充分利用GPU的计算优势，采用多级缓存的思想，将部分参数分级存储于显存、主存和固态硬盘上。在他们提出的这个分级系统中，主要解决了缓存策略和异构硬件的适配问题。然而在设计类似的存储系统时，还应该考虑到机器学习模型内在的一些访存特征以进一步优化。Kraken
\sphinxcite{chapter_reinforcement_learning/summary:id7}这篇工作讨论了利用机器学习模型的特征对嵌入项的哈希表的存储进行优化的方法。此外，新型硬件的发展为解决大规模推荐模型的高效存储提供了新的可能。比如非易失存储可以作为主存的扩展，进一步提升系统可以支持的模型尺寸。然而目前还没有见到专门为在线机器学习优化的非易失存储系统。另外也有工作
\sphinxcite{chapter_reinforcement_learning/summary:mlsys2021-ec895663}讨论了利用FPGA加速嵌入表的访存并且相比于CPU服务器取得了非常显著的效果。

\item {} 
\sphinxAtStartPar
内存高效的嵌入项存储与计算。除了系统上的设计，研究人员也在探索其他算法优化手段来压缩嵌入表的内存需求。直接使用低精度浮点数可以有效降低内存开销，但是还是会对模型的精度产生一定的影响。因此在在线推荐服务这种精度敏感的场景中并不适用。除此之外，
\sphinxcite{chapter_reinforcement_learning/summary:mlsys2021-979d472a}利用低秩分解可以将一个大矩阵分解为两个小矩阵（向量）。这种方法可以在保留原矩阵大量信息的前提下显著减小内存开销。除了低秩分解外，还有其他
\sphinxcite{chapter_reinforcement_learning/summary:id5}分解嵌入表的手段。还有研究
\sphinxcite{chapter_reinforcement_learning/summary:ginart2021mixed}表明，没有必要为所有的项目都使用一样长的嵌入项，可以根据嵌入项的重要性动态决定其长度以节省内存开销。作为系统设计者，如何将层出不穷的算法优化手段高效地实现是需要考虑的问题。

\end{itemize}


\section{小结}
\label{\detokenize{chapter_recommender_system/summary:id1}}\label{\detokenize{chapter_recommender_system/summary::doc}}
\sphinxAtStartPar
推荐系统作为深度学习在工业界最成功的落地成果之一，极大地提升了用户的在线服务体验，并且为各大公司创造了可观的利润，然而也带来了许多系统层面的挑战亟待解决。本节简单介绍了典型的工业界推荐系统架构及其面临的挑战，并给出了潜在的解决方案的方向。在实际生产环境中，具体的系统设计方案需要根据不同推荐场景的需求而变化，不存在一种万能的解决方案。


\section{扩展阅读}
\label{\detokenize{chapter_recommender_system/summary:id2}}\begin{itemize}
\item {} 
\sphinxAtStartPar
推荐模型：\sphinxhref{https://arxiv.org/abs/1606.07792}{Wide \& Deep}%
\begin{footnote}[60]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/1606.07792}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
开源推荐系统框架：\sphinxhref{https://irsworkshop.github.io/2020/publications/paper\_21\_Oldridge\_Merlin.pdf}{Merlin}%
\begin{footnote}[61]\sphinxAtStartFootnote
\sphinxnolinkurl{https://irsworkshop.github.io/2020/publications/paper\_21\_Oldridge\_Merlin.pdf}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
软硬件协同设计加速超大规模深度学习推荐系统训练：\sphinxhref{https://arxiv.org/abs/2104.05158v5}{ZionEX}%
\begin{footnote}[62]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/2104.05158v5}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
利用多级缓存支持超大规模深度学习推荐系统训练：\sphinxhref{https://arxiv.org/abs/2003.05622}{Distributed
Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads
Systems}%
\begin{footnote}[63]\sphinxAtStartFootnote
\sphinxnolinkurl{https://arxiv.org/abs/2003.05622}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
工业界机器学习系统的实践：\sphinxhref{https://papers.nips.cc/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html}{Hidden Technical Debt in Machine
Learning
Systems}%
\begin{footnote}[64]\sphinxAtStartFootnote
\sphinxnolinkurl{https://papers.nips.cc/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html}
%
\end{footnote}

\end{itemize}


\section{参考文献}
\label{\detokenize{chapter_recommender_system/summary:id3}}
\sphinxAtStartPar



\chapter{联邦学习系统}
\label{\detokenize{chapter_federated_learning/index:id1}}\label{\detokenize{chapter_federated_learning/index::doc}}
\sphinxAtStartPar
在本章中，我们介绍深度学习的一个重要分支——联邦学习及其在系统方面的知识。本章的学习目标包括：
\begin{itemize}
\item {} 
\sphinxAtStartPar
掌握联邦学习基本定义，并了解现有主流联邦开源框架。

\item {} 
\sphinxAtStartPar
了解横向联邦学习算法。

\item {} 
\sphinxAtStartPar
了解纵向联邦学习算法。

\item {} 
\sphinxAtStartPar
了解联邦学习加密算法。

\item {} 
\sphinxAtStartPar
了解联邦学习前沿算法和未来研究方向。

\end{itemize}


\section{概述}
\label{\detokenize{chapter_federated_learning/overview:id1}}\label{\detokenize{chapter_federated_learning/overview::doc}}
\sphinxAtStartPar
随着人工智能的飞速发展，大规模和高质量的数据越来越重要，但也制约了其进一步的发展。隐私、监管和工程等问题造成了设备与设备之间的数据不能共享，进而导致了数据孤岛问题的出现。为了解决这一难题，联邦学习（Federated
Learning，FL）应运而生。联邦学习的概念最早在2016年被提了出来，能有效帮助多个机构在满足用户隐私保护、数据安全和政府法规的要求下，进行数据使用和机器学习建模。


\subsection{定义}
\label{\detokenize{chapter_federated_learning/overview:id2}}
\sphinxAtStartPar
联邦学习的核心即为数据不动，模型动。显然，若是将数据从各方集中在一起，无法保证对用户隐私的保护，不符合相关法律法规。联邦学习让模型在各个数据方“移动”，这样就可以达到数据不出端即可建模的效果。在联邦学习中，各方数据都保留在本地，通过（在中心服务器上）交换加密的参数建立机器学习模型。


\subsection{应用场景}
\label{\detokenize{chapter_federated_learning/overview:id3}}
\sphinxAtStartPar
在实际的应用场景中，根据样本和特征的重叠情况，联邦学习可以被分为横向联邦学习（样本不同，特征重叠），纵向联邦学习（特征不同，样本重叠）和联邦迁移学习（样本和特征都不重叠）。

\sphinxAtStartPar
\sphinxstylestrong{横向联邦学习}适用于不同参与方拥有的特征相同、但参与的个体不同的场景。比如，在广告推荐场景中，算法开发人员使用不同手机用户的相同特征（点击次数、停留时间或使用频次等）的数据来建立模型；抑或者是在输入法中用来预测由于这些特征数据不能出端，因此，横向联邦学习被用来联合多用户的数据来构建模型。

\sphinxAtStartPar
\sphinxstylestrong{纵向联邦学习}适用于样本重叠多、特征重叠少的场景。比如，有两个不同机构，一家是保险公司，另一家是医院。它们的用户群体很有可能包含该地的大部分居民，用户的交集可能较大。由于保险公司记录的都是用户的收支行为与信用评级，而医院则保有用户的疾病与购药记录，因此它们的用户特征交集较小。纵向联邦学习就是将这些不同特征在加密的状态下加以聚合，以增强模型能力的联邦学习。

\sphinxAtStartPar
\sphinxstylestrong{联邦迁移学习}的核心是找到源领域和目标领域之间的相似性。比如有两个不同机构，一家是位于中国的银行，另一家是位于美国的电商。由于受到地域限制，这两家机构的用户群体交集很小。同时，由于机构类型的不同，二者的数据特征也只有小部分重合。在这种情况下，要想进行有效的联邦学习，就必须引入迁移学习，来解决单边数据规模小和标签样本少的问题，从而提升模型的效果。


\subsection{部署场景}
\label{\detokenize{chapter_federated_learning/overview:id4}}
\sphinxAtStartPar
联邦学习和参数服务器（数据中心分布式学习）架构非常相似，都是采用中心化的服务器和分散的客户端去构建同一个机器学习模型。此外，根据客户端来源和规模的不同，联邦学习还可以细分为跨组织（cross\sphinxhyphen{}silo）与跨设备（cross\sphinxhyphen{}device）联邦学习。一般而言，跨组织联邦学习的用户一般是企业、机构单位级别的，而跨设备联邦学习针对的则是便携式电子设备、移动端设备等。
\hyperref[\detokenize{chapter_federated_learning/overview:ch10-federated-learning-different-connection}]{图\ref{\detokenize{chapter_federated_learning/overview:ch10-federated-learning-different-connection}}}展示了三者的区别和联系：

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-federated-learning-different-connection}.png}
\caption{数据中心分布式训练、跨组织和跨设备联邦学习的区别和联系}\label{\detokenize{chapter_federated_learning/overview:id6}}\label{\detokenize{chapter_federated_learning/overview:ch10-federated-learning-different-connection}}\end{figure}


\subsection{常用框架}
\label{\detokenize{chapter_federated_learning/overview:id5}}
\sphinxAtStartPar
随着用户和开发人员对联邦学习技术的需求不断增长，联邦学习工具和框架的数量也越来越多。下面将介绍一些主流的联邦学习框架。

\sphinxAtStartPar
\sphinxhref{https://www.tensorflow.org/federated}{TFF}%
\begin{footnote}[70]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.tensorflow.org/federated}
%
\end{footnote} (TensorFlow
Federated)是谷歌牵头开发的联邦学习开源框架，用于在分散数据上进行机器学习和其他计算。TFF的开发是为了促进联邦学习的开放研究和实验，这是一种机器学习的方法，在许多参与的客户中训练共享的全局模型，这些客户将其训练数据保存在本地。例如，联邦学习已被用于训练移动键盘的预测模型，而无需将敏感的键入数据上载到服务器。

\sphinxAtStartPar
\sphinxhref{https://paddlefl.readthedocs.io/en/latest/index.html}{PaddleFL}%
\begin{footnote}[71]\sphinxAtStartFootnote
\sphinxnolinkurl{https://paddlefl.readthedocs.io/en/latest/index.html}
%
\end{footnote}是百度提出的一个基于PaddlePaddle的开源联邦学习框架。研究人员可以很轻松地用PaddleFL复制和比较不同的联邦学习算法，开发人员也比较容易在大规模分布式集群中部署PaddleFL联邦学习系统。PaddleFL提供很多种联邦学习策略（横向联邦学习、纵向联邦学习）及其在计算机视觉、自然语言处理、推荐算法等领域的应用。此外，PaddleFL还将提供传统机器学习训练策略的应用，例如多任务学习、联邦学习环境下的迁移学习。依靠着PaddlePaddle的大规模分布式训练和Kubernetes对训练任务的弹性调度能力，PaddleFL可以基于全栈开源软件轻松地部署。

\sphinxAtStartPar
\sphinxhref{https://fate.fedai.org}{FATE}%
\begin{footnote}[72]\sphinxAtStartFootnote
\sphinxnolinkurl{https://fate.fedai.org}
%
\end{footnote} (Federated AI Technology
Enabler)由微众银行提出，是全球首个联邦学习工业级开源框架，可以让企业和机构在保护数据安全和数据隐私的前提下进行数据协作。
FATE项目使用多方安全计算 (MPC) 以及同态加密 (HE)
技术构建底层安全计算协议，以此支持不同种类的机器学习的安全计算，包括逻辑回归、基于树的算法、深度学习和迁移学习等。
FATE于2019年2月首次对外开源，并成立 \sphinxhref{https://github.com/FederatedAI/FATE-Community/blob/master/FATE\_Project\_Technical\_Charter.pdf}{FATE
TSC}%
\begin{footnote}[73]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/FederatedAI/FATE-Community/blob/master/FATE\_Project\_Technical\_Charter.pdf}
%
\end{footnote} 对FATE社区进行开源治理，成员包含国内主要云计算和金融服务企业。

\sphinxAtStartPar
\sphinxhref{https://FedML.ai}{FedML}%
\begin{footnote}[74]\sphinxAtStartFootnote
\sphinxnolinkurl{https://FedML.ai}
%
\end{footnote}是一个USC牵头提出的联邦学习开源研究和基准库，它有助于开发新的联合学习算法和公平的性能比较。FedML支持三种计算范式(分布式训练、移动设备上训练和独立模拟)，供用户在不同的系统环境中进行实验。FedML还通过灵活和通用的API设计和参考基线实现促进多样化的算法研究。为非I.I.D设置精心策划的全面基准数据集旨在进行公平比较。

\sphinxAtStartPar
\sphinxhref{https://openmined.github.io/PySyft/index.html}{PySyft}%
\begin{footnote}[75]\sphinxAtStartFootnote
\sphinxnolinkurl{https://openmined.github.io/PySyft/index.html}
%
\end{footnote}是UCL、DeepMind和OpenMined发布的安全和隐私深度学习Python库，包括联邦学习、差分隐私和多方学习。PySyft使用差分隐私和加密计算(如多方计算(MPC)和同态加密(HE))将私有数据与模型训练解耦流。

\sphinxAtStartPar
\sphinxhref{https://github.com/bytedance/fedlearner}{Fedlearner}%
\begin{footnote}[76]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/bytedance/fedlearner}
%
\end{footnote}是字节跳动提出的协作机器学习框架，它允许对分布在机构之间的数据进行联合建模。Fedlearner附带了用于群集管理、作业管理、作业监控和网络代理的周围基础架构。Fedlearner采用云原生部署方案。数据存放在HDFS。通过Kubernetes管理和拉起任务。每个Fedlearner的训练任务需要参与双方同时拉起K8S任务，通过Master节点统一管理，Worker建实现通信。

\sphinxAtStartPar
\sphinxhref{https://openfl.readthedocs.io/en/latest/index.html}{OpenFL}%
\begin{footnote}[77]\sphinxAtStartFootnote
\sphinxnolinkurl{https://openfl.readthedocs.io/en/latest/index.html}
%
\end{footnote}是英特尔提出的用于联邦学习的Python框架。OpenFL旨在成为数据科学家的灵活、可扩展和易于学习的工具。

\sphinxAtStartPar
\sphinxhref{https://flower.dev}{Flower}%
\begin{footnote}[78]\sphinxAtStartFootnote
\sphinxnolinkurl{https://flower.dev}
%
\end{footnote}是剑桥大学、Adap等发布的联邦学习开源系统，主要针对在大规模、异质化设备上部署联邦学习算法的应用场景进行优化。

\sphinxAtStartPar
\sphinxhref{https://www.mindspore.cn/en}{MindSpore Fedrated}%
\begin{footnote}[79]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.mindspore.cn/en}
%
\end{footnote}
(昇思)是华为提出的一款开源联邦学习框架，支持千万级无状态终端设备商用化部署，在用户数据留存在本地的情况下，使能全场景智能应用。
MindSpore
Federated优先专注于大规模参与方的横向联邦的应用场景，使参与联邦学习的各用户在不共享本地数据的前提下共建AI模型。MindSpore
Fedrated主要解决隐私安全、大规模联邦聚合、易用性和跨平台部署等联邦学习在工业场景部署的难点。


\section{横向联邦学习}
\label{\detokenize{chapter_federated_learning/horizontal_fl:id1}}\label{\detokenize{chapter_federated_learning/horizontal_fl::doc}}

\subsection{云云场景中的横向联邦}
\label{\detokenize{chapter_federated_learning/horizontal_fl:id2}}
\sphinxAtStartPar
在横向联邦学习系统中，具有相同数据结构的多个参与者通过参数或云服务器协同建立机器学习模型。一个典型的假设是参与者是诚实的，而服务器是诚实但好奇的，因此不允许任何参与者向服务器泄漏信息。这种系统的训练过程通常包括以下四个步骤：

\sphinxAtStartPar
①：参与者在本地计算训练梯度，使用加密、差异隐私或秘密共享技术掩饰所选梯度，并将掩码后的结果发送到服务器；

\sphinxAtStartPar
②：服务器执行安全聚合，不了解任何参与者的信息；

\sphinxAtStartPar
③：服务器将汇总后的结果发送给参与者；

\sphinxAtStartPar
④：参与者用解密的梯度更新他们各自的模型。

\sphinxAtStartPar
和传统分布式学习相比，联邦学习存在训练结点不稳定和通信代价大的难点。这些难点导致了联邦学习无法和传统分布式学习一样：在每次单步训练之后，同步不同训练结点上的权重。为了提高计算通信比并降低频繁通信带来的高能耗，谷歌公司在2017年
\DUrole{bibtex}{{[}fedavg{]}}提出了联邦平均算法(Federated Averaging，FedAvg)。
展示了FedAvg的整体流程。在每轮联邦训练过程中，端侧进行多次单步训练。然后云侧聚合多个端侧权重，并取加权平均。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-federated-learning-fedavg}.png}
\caption{联邦平均算法}\label{\detokenize{chapter_federated_learning/horizontal_fl:id10}}\label{\detokenize{chapter_federated_learning/horizontal_fl:ch10-federated-learning-fedavg}}\end{figure}

\sphinxAtStartPar
随着研究和应用的深入，研究者们意识到了FedAvg不适用于某些场景。比如在数据异质性（端上数据不是I.I.D分布）、系统异质性（端设备时断时连）的情况下，对模型参数只进行简单的加权平均会在训练过程中引入大量偏差，从而影响模型的收敛速度、预测性能。针对此，研究人员提出了基于Momentum
\DUrole{bibtex}{{[}FedAvg\_Momentum{]}}、Variation Control
\DUrole{bibtex}{{[}scaffold{]}}、Bayesian \DUrole{bibtex}{{[}FedBE{]}}、Distillation
\DUrole{bibtex}{{[}PATE{]}}和Proximal Estimation
\DUrole{bibtex}{{[}FedProx{]}}等原理开发的算法和系统框架。


\subsection{端云场景中的横向联邦}
\label{\detokenize{chapter_federated_learning/horizontal_fl:id9}}
\sphinxAtStartPar
端云联邦的总体流程和云云联邦一样，但端云联邦学习面临的难点还包括以下三个方面：

\sphinxAtStartPar
1.高昂的通信代价。在联邦学习问题中，原始数据保存在远程客户端设备本地，必须与中央服务器不断交互才能完成全局模型的构建。通常的通信网络可能是WLAN或移动数据，网络通信速度可能比本地计算慢许多个数量级，这就造成高昂的通信代价成为了联邦学习的关键瓶颈。

\sphinxAtStartPar
2.系统异质性。由于客户端设备硬件条件（CPU、内存）、网络连接（3G、4G、5G、WIFI）和电源（电池电量）的变化，联邦学习网络中每个设备的存储、计算和通信能力都有可能不同。网络和设备本身的限制可能导致某一时间仅有一部分设备处于活动状态。此外，设备还会出现没电、网络无法接入等突发状况，导致瞬时无法连通。这种异质性的系统架构影响了联邦学习整体策略的制定。

\sphinxAtStartPar
3.隐私问题。联邦学习共享客户端设备中的模型参数更新（例如梯度信息）而不是原始数据，因此在数据隐私保护方面优于其他的分布式学习方法。然而，在训练过程中传递模型的更新信息仍然存在向第三方或中央服务器暴露敏感信息的风险。隐私保护成为联邦学习需要重点考虑的问题。

\sphinxAtStartPar
为了解决端云联邦学习带来的挑战，MindSpore Federated
Learning设计了分布式FL\sphinxhyphen{}Server架构。系统由调度器模块、服务器模块和客户端模块三个部分组成，其系统架构如
\hyperref[\detokenize{chapter_federated_learning/horizontal_fl:ch10-federated-learning-architecture}]{图\ref{\detokenize{chapter_federated_learning/horizontal_fl:ch10-federated-learning-architecture}}}所示。其中：
\begin{itemize}
\item {} 
\sphinxAtStartPar
联邦学习调度器：

\sphinxAtStartPar
联邦学习调度器（FL\sphinxhyphen{}Scheduler）协助集群组网，并负责管理面任务的下发。

\item {} 
\sphinxAtStartPar
联邦学习服务器：

\sphinxAtStartPar
联邦学习服务器（FL\sphinxhyphen{}Server）提供客户端选择、限时通信、分布式联邦聚合功能。FL\sphinxhyphen{}Server需要具备支持端云千万台设备的能力以及边缘服务器的接入和安全处理的逻辑。

\item {} 
\sphinxAtStartPar
联邦学习客户端：

\sphinxAtStartPar
联邦学习客户端（FL\sphinxhyphen{}Client）负责本地数据训练，并在和FL\sphinxhyphen{}Server进行通信时，对上传权重进行安全加密。

\end{itemize}

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{ch10-federated-learning-architecture}.svg}
\caption{联邦学习系统架构图}\label{\detokenize{chapter_federated_learning/horizontal_fl:id11}}\label{\detokenize{chapter_federated_learning/horizontal_fl:ch10-federated-learning-architecture}}\end{figure}

\sphinxAtStartPar
此外，MindSpore Federated针对端云联邦学习设计了出三大特性：

\sphinxAtStartPar
1.限时通信：在FL\sphinxhyphen{}Server和FL\sphinxhyphen{}Client建立连接后，启动全局的计时器和计数器。当预先设定的时间窗口内的FL\sphinxhyphen{}Server接收到FL\sphinxhyphen{}Client训练后的模型参数满足初始接入的所有FL\sphinxhyphen{}Client的一定比例后，就可以进行聚合。若时间窗内没有达到比例阈值，则进入下一轮迭代。保证即使有海量FL\sphinxhyphen{}Client接入的情况下，也不会由于个别FL\sphinxhyphen{}Client训练时间过长或掉线导致的整个联邦学习过程卡死。

\sphinxAtStartPar
2.松耦合组网：使用FL\sphinxhyphen{}Server集群。每个FL\sphinxhyphen{}Server接收和下发权重给部分FL\sphinxhyphen{}Client，减少单个FL\sphinxhyphen{}Server的带宽压力。此外，支持FL\sphinxhyphen{}Client以松散的方式接入。任意FL\sphinxhyphen{}Client的中途退出都不会影响全局任务，并且FL\sphinxhyphen{}Client在任意时刻访问任意FL\sphinxhyphen{}Server都能获得训练所需的全量数据。

\sphinxAtStartPar
3.加密模块：MindSpore
Federated为了防止模型梯度的泄露，部署了多种加密算法：本地差分隐私（LDP）、基于多方安全计算（MPC）的安全聚合算法和华为自研的基于符号的维度选择差分隐私算法（SignDS）。


\section{纵向联邦学习}
\label{\detokenize{chapter_federated_learning/vertical_fl:id1}}\label{\detokenize{chapter_federated_learning/vertical_fl::doc}}
\sphinxAtStartPar
现在我们介绍另一种联邦学习算法：纵向联邦学习（Vertical Federated
Learning）。纵向联邦学习的参与方拥有相同样本空间、不同特征空间的数据，通过共有样本数据进行安全联合建模，在金融、广告等领域拥有广泛的应用场景。和横向联邦学习相比，纵向联邦学习的参与方之间需要协同完成数据求交集、模型联合训练和推理，实现技术方法会相对更加复杂，并且随着参与方增多复杂度越高。

\sphinxAtStartPar
下面以企业A和企业B两方为例来介绍纵向联邦的基本架构和流程。假设企业A有特征数据\(X_a\)和标签数据\(Y\)，可以独立建模；企业B有特征数据\(X_b\)，缺乏标签数据，因此无法独立建模。由于隐私法规和行业规范等原因，两个企业之间的数据无法直接互通。企业A和企业B可采用纵向联邦学习解决方案进行合作，数据不出本地，使用双方共同样本数据\(X_a\)、\(Y\)和\(X_b\)进行联合建模和训练。最终双方都能获得一个更强大的模型。


\subsection{纵向联邦架构}
\label{\detokenize{chapter_federated_learning/vertical_fl:id2}}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-federated-learning-vfl-arch}.svg}
\caption{纵向联邦两方架构}\label{\detokenize{chapter_federated_learning/vertical_fl:id5}}\label{\detokenize{chapter_federated_learning/vertical_fl:federated-learning-vfl-arch}}\end{figure}

\sphinxAtStartPar
纵向联邦学习系统中的模型训练一般分为如下阶段： \sphinxhyphen{}
样本对齐：首先对齐企业A和企业B中具有相同ID（Identification）的样本数据。在数据对齐阶段，系统会采用加密算法对数据进行保护，确保任何一方的用户数据不会暴露。
\sphinxhyphen{}
联合训练：在确定企业A和企业B共有用户数据后，可以使用这些共有的数据来协同训练一个业务模型。模型训练过程中，模型参数信息以加密方式进行传递。已训练好的联邦学习模型可以部署在联邦学习系统的各参与方。


\subsection{样本对齐}
\label{\detokenize{chapter_federated_learning/vertical_fl:id3}}
\sphinxAtStartPar
隐私集合求交（Private Set
Intersection，PSI）技术是纵向联邦学习中数据样本对齐的常用解决方案。业界PSI实现方案有多种：基于电路、基于公钥加密、基于不经意传输协议和基于全同态加密等。不同PSI方案各有优劣势。例如，基于公钥加密方案不需要辅助服务器运行，但公钥加密的计算开销大；而基于不经意传输方案计算性能高，但通信开销较大。因此在具体应用时，要根据实际场景来选择功能、性能和安全之间的最佳平衡方案。

\sphinxAtStartPar
基于RSA盲签名是一种基于公钥加密的经典PSI方法，也是当前业界纵向联邦学习系统中广泛应用的技术之一。下面以企业A和企业B为例描述RSA盲签名算法的基本流程。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{ch10-federated-learning-vfl-data}.png}
\caption{纵向联邦样本对齐}\label{\detokenize{chapter_federated_learning/vertical_fl:id6}}\label{\detokenize{chapter_federated_learning/vertical_fl:federated-learning-vfl-data}}\end{figure}

\sphinxAtStartPar
企业A作为服务端，拥有标签数据和样本ID集合\(\{a_1, a_2, …,a_w\}\)。企业B则作为客户端，拥有样本ID集合\(\{b_1, b_2, …,b_v\}\)。
首先，在企业A的服务端利用RSA算法生成私钥和公钥。其中，私钥\((n,d)\)保留在服务端，公钥\((n,e)\)则发送给企业B的客户端。

\sphinxAtStartPar
在服务端侧生成RSA计算样本对齐ID的签名：
\begin{equation}\label{equation:chapter_federated_learning/vertical_fl:chapter_federated_learning/vertical_fl:0}
\begin{split}t_j=H^{'}(K_{a:j})\end{split}
\end{equation}
\sphinxAtStartPar
其中，
\(K_{a:j}=(H(a_j))^d \ mod \ n\)，是采用私钥\(d\)加密的对\(H(a_j)\)的RSA加密的结果。\(H()\)和\(H^{'}()\)是哈希函数。
同样，在客户端侧对样本ID进行公钥加密，并乘以一个随机数\(R_{b,i}\)用于加盲扰动：
\begin{equation}\label{equation:chapter_federated_learning/vertical_fl:chapter_federated_learning/vertical_fl:1}
\begin{split}y_i=H(b_i)\cdot(R_{b,i})^e \ mod \ n\end{split}
\end{equation}
\sphinxAtStartPar
客户端侧将上述计算出来的\(\{y_1,...,y_v\}\)值传输给服务端侧。服务端侧收到\(y_i\)值后，使用私钥\(d\)进行签名并计算：
\begin{equation}\label{equation:chapter_federated_learning/vertical_fl:chapter_federated_learning/vertical_fl:2}
\begin{split}y_i^{'}=y_i^d \ mod \ n\end{split}
\end{equation}
\sphinxAtStartPar
然后将计算出的\(\{y_1^{'},...,y_v^{'}\}\)和\(\{t_1,...,t_w\}\)发送给客户端侧。
而客户端侧收到\(y_i^{'}\)和\(t_j\)后，首先完成去盲操作：
\begin{equation}\label{equation:chapter_federated_learning/vertical_fl:chapter_federated_learning/vertical_fl:3}
\begin{split}K_{b:i}={y_i}^{'}/R_{b,i}\end{split}
\end{equation}
\sphinxAtStartPar
并将自己的ID签名与服务端发过来的ID签名进行样本对齐，得到加密和哈希组合状态下的ID交集\(I\)，
\begin{equation}\label{equation:chapter_federated_learning/vertical_fl:chapter_federated_learning/vertical_fl:4}
\begin{split}{t_i}^{'}=H^{'}(K_{b:i}) \\I=\{t_1,...,t_w\}\cap \{{t_1}^{'},...,{t_v}^{'}\}\end{split}
\end{equation}
\sphinxAtStartPar
最后，将对齐后的样本ID交集\(I\)发送给服务端，服务端利用自身的映射表单独求取明文结果。这样企业A和企业B在加密状态下完成了求取相交的用户集合，并且在整个过程中双方非重叠样本ID都不会对外暴露。


\subsection{联合训练}
\label{\detokenize{chapter_federated_learning/vertical_fl:id4}}
\sphinxAtStartPar
在确样本ID对齐后，就可以使用这些公共的数据来训练机器学习模型。目前，线性回归、决策树和神经网络等模型已经被广泛应用到纵向联邦系统中。
在纵向联邦的模型训练过程中，一般会引入第三方协作者C来实现中心服务器功能，并且假设这个第三方协作者C是可信的，不会与其他参与方合谋。中心服务器在训练过程中作为中立方，产生和分发密钥，并对加密数据进行解密和计算。但中心服务器角色是非必须的，例如在两方联邦学习的场景下，不需要第三方协作者C来协调双方的训练任务，可以由具有标签数据的企业A来充当中心服务服务器的角色。不失一般性，下面继续以包含第三方协作者C的方案来描述纵向联邦模型联合训练过程。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-federated-learning-vfl-train}.svg}
\caption{纵向联邦联合建模}\label{\detokenize{chapter_federated_learning/vertical_fl:id7}}\label{\detokenize{chapter_federated_learning/vertical_fl:federated-learning-vfl-train}}\end{figure}
\begin{itemize}
\item {} 
\sphinxAtStartPar
第一步：由第三方协作者C创建密钥对，将公钥发送给企业A和B。

\item {} 
\sphinxAtStartPar
第二步：在企业A和B侧分别计算梯度和损失计算需要的中间结果，并进行加密和交换。

\item {} 
\sphinxAtStartPar
第三步：企业A和B分别计算加密梯度和添加掩码。同时企业A还将计算加密损失值。计算完成后，企业A和B向第三方协作者C发送加密后的值。

\item {} 
\sphinxAtStartPar
第四步：第三方协作者C对梯度和损失值解密，然后将结果发送回企业A和B。

\item {} 
\sphinxAtStartPar
第五步：企业A和B将收到的值首先去除梯度上的掩码，然后更新本地模型参数。

\end{itemize}

\sphinxAtStartPar
在整个训练过程中，企业A和B之间的任何敏感数据都是经过加密算法加密之后再发出己信任域。同态加密（Homomorphic
Encryption,HE）是业界联邦学习框架常用的算法之一。同态加密是指加密过后的两份数据进行某些运算之后直接解密，可以得到真实数据经过相同运算的结果。当这种运算是加法时，就称为加法同态加密。将加密函数记为\([[\cdot]]\)，加法同态加密具有如下特征：
\begin{equation}\label{equation:chapter_federated_learning/vertical_fl:chapter_federated_learning/vertical_fl:5}
\begin{split}[[a+b]]=[[a]]+[[b]]\end{split}
\end{equation}
\sphinxAtStartPar
Paillier算法是一种满足加法的同态加密算法，已经广泛应用在第三方数据处理领域和信号处理领域。在纵向联邦学习中，通常采用Paillier加密算法对损失函数和梯度进行加密，从而实现跨机构的模型安全联合训练。

\sphinxAtStartPar
模型联合训练完成后就可以投入生产环境部署应用。由于纵向联邦中每个参与方具有部分模型结构，因此推理也需要双方协作完成计算。联合推理过程和联合训练类似，首先第三方协作者C将推理数据ID发送给企业A和B，双方在本地完成推理计算后将结果加密后传输到第三方协作者C，由C计算模型最终的联合推理结果。


\section{隐私加密算法}
\label{\detokenize{chapter_federated_learning/privacy_encryption_algorithm:id1}}\label{\detokenize{chapter_federated_learning/privacy_encryption_algorithm::doc}}
\sphinxAtStartPar
联邦学习过程中，用户数据仅用于本地设备训练，不需要上传至中央FL\sphinxhyphen{}Server。这样可以避免用户个人数据的直接泄露。然而联邦学习框架中，模型的权重以明文形式上云仍然存在间接泄露用户隐私的风险。敌手获取到用户上传的明文权重后，可以通过重构、模型逆向等攻击恢复用户的个人训练数据，导致用户隐私泄露。

\sphinxAtStartPar
MindSpore
Federated框架，提供了基于本地差分隐私（LDP）和基于多方安全计算（MPC）的安全聚合算法，在本地模型的权重上云前对其进行加噪或加扰。在保证模型可用性的前提下，解决联邦学习中的隐私泄露问题。


\subsection{基于LDP的安全聚合}
\label{\detokenize{chapter_federated_learning/privacy_encryption_algorithm:ldp}}
\sphinxAtStartPar
差分隐私（differential
privacy）是一种保护用户数据隐私的机制。差分隐私定义为：
\begin{equation}\label{equation:chapter_federated_learning/privacy_encryption_algorithm:chapter_federated_learning/privacy_encryption_algorithm:0}
\begin{split}Pr[\mathcal{K}(D)\in S] \le e^{\epsilon} Pr[\mathcal{K}(D’) \in S]+\delta\end{split}
\end{equation}
\sphinxAtStartPar
对于两个差别只有一条记录的数据集\(D, D’\)，通过随机算法\(\mathcal{K}\)，输出结果为集合\(S\)子集的概率满足上面公式。\(\epsilon\)为差分隐私预算，\(\delta\)扰动，\(\epsilon\)和\(\delta\)越小，说明\(\mathcal{K}\)在\(D\)和\(D’\)上输出的数据分布越接近。

\sphinxAtStartPar
在联邦学习中，假设FL\sphinxhyphen{}Client本地训练之后的模型权重矩阵是\(W\)，由于模型在训练过程中会“记住”训练集的特征，所以敌手可以借助\(W\)还原出用户的训练数据集。

\sphinxAtStartPar
MindSpore
Federated提供基于本地差分隐私的安全聚合算法，防止本地模型的权重上云时泄露隐私数据。

\sphinxAtStartPar
FL\sphinxhyphen{}Client会生成一个与本地模型权重\(W\)相同维度的差分噪声矩阵\(G\)，然后将二者相加，得到一个满足差分隐私定义的权重\(W_p\):
\begin{equation}\label{equation:chapter_federated_learning/privacy_encryption_algorithm:chapter_federated_learning/privacy_encryption_algorithm:1}
\begin{split}W_p=W+G\end{split}
\end{equation}
\sphinxAtStartPar
FL\sphinxhyphen{}Client将加噪后的模型权重\(W_p\)上传至云侧FL\sphinxhyphen{}Server进行联邦聚合。噪声矩阵\(G\)相当于给原模型加上了一层掩码，在降低模型泄露敏感数据风险的同时，也会影响模型训练的收敛性。如何在模型隐私性和可用性之间取得更好的平衡，仍然是一个值得研究的问题。实验表明，当参与方的数量\(n\)足够大时（一般指1000以上），大部分噪声能够相互抵消，本地差分机制对聚合模型的精度和收敛性没有明显影响。


\subsection{基于MPC的安全聚合}
\label{\detokenize{chapter_federated_learning/privacy_encryption_algorithm:mpc}}
\sphinxAtStartPar
尽管差分隐私技术可以适当保护用户数据隐私，但是当参与FL\sphinxhyphen{}Client数量比较少或者高斯噪声幅值较大时，模型精度会受较大影响。为了同时满足模型保护和模型收敛这两个要求，MindSpore
Federated提供了基于MPC的安全聚合方案。

\sphinxAtStartPar
在这种训练模式下，假设参与的FL\sphinxhyphen{}Client集合为\(U\)，对于任意FL\sphinxhyphen{}Client
\(u\)和\(v\)，
它们会两两协商出一对随机扰动\(p_{uv}\)、\(p_{vu}\)，满足
\begin{equation}\label{equation:chapter_federated_learning/privacy_encryption_algorithm:chapter_federated_learning/privacy_encryption_algorithm:2}
\begin{split}\label{puv}
    p_{uv}=
    \begin{cases}
    -p_{vu}, &u{\neq}v\\
    0, &u=v
    \end{cases}\end{split}
\end{equation}
\sphinxAtStartPar
于是每个FL\sphinxhyphen{}Client \(u\)
在上传模型权重至FL\sphinxhyphen{}Server前，会在原模型权重\(x_u\)加上它与其它用户协商的扰动：
\begin{equation}\label{equation:chapter_federated_learning/privacy_encryption_algorithm:chapter_federated_learning/privacy_encryption_algorithm:3}
\begin{split}x_{encrypt}=x_u+\sum\limits_{v{\in}U}p_{uv}\end{split}
\end{equation}
\sphinxAtStartPar
从而FL\sphinxhyphen{}Server聚合结果\(\overline{x}\)为：

\sphinxAtStartPar
上面的过程只是介绍了聚合算法的主要思想，基于MPC的聚合方案是精度无损的，代价是通讯轮次的增加。


\subsection{基于LDP\sphinxhyphen{}SignDS算法的安全聚合}
\label{\detokenize{chapter_federated_learning/privacy_encryption_algorithm:ldp-signds}}
\sphinxAtStartPar
对于先前的基于维度加噪的LDP算法，在隐私预算一定时，添加到每个维度的噪声规模基本上与模型参数的数量成正比。因此，对于高维模型，可能需要非常多的参与方来减轻噪音对模型收敛的影响。为了解决上述“维度依赖”问题，MindSpore
Federated 进一步提供了基于维度选择的\sphinxstylestrong{Sign\sphinxhyphen{}based Dimension Selection
(SignDS)}
\DUrole{bibtex}{{[}jiang2022signds{]}}算法。SignDS算法的主要思想是，对于每一条真实的本地更新\(\Delta\in\mathbb{R}^{d}\)，用户端首先选择一小部分更新最明显的维度构建topk集合\(S_k\)，并以此选择一个维度集合\(J\)返回给FL\sphinxhyphen{}Server。FL\sphinxhyphen{}Server根据维度集合\(J\)构建一条对应的稀疏更新\(\Delta^\prime\)，并聚合所有稀疏更新进用于更新全局模型。由于本地模型更新与本地数据信息相关联，直接选取真实的最大更新维度可能导致隐私泄露。对此，SignDS算法在两方面实现了隐私保证。一方面，算法使用了一种基于指数机制（Exponential
Mechanism， EM
\DUrole{bibtex}{{[}mcsherry2007mechanism{]}}）的维度选择算法\sphinxstylestrong{EM\sphinxhyphen{}MDS}，使得所选维度集满足严格的\(\epsilon\)\sphinxhyphen{}LDP保证；另一方面，在构建稀疏更新时，对所选维度分配一个常量值而不直接使用实际更新值，以保证稀疏更新和本地数据不再直接关联。由于维度选择满足\(\epsilon\)\sphinxhyphen{}LDP，且分配给所选维度的更新值与本地数据无关，根据差分隐私的传递性
\DUrole{bibtex}{{[}dwork2014algorithmic{]}}，所构建的稀疏更新同样满足\(\epsilon\)\sphinxhyphen{}LDP保证。\sphinxstylestrong{相较于之前基于维度加噪的LDP算法，SignDS算法可以显著提升高维模型的训练精度。同时，由于FL\sphinxhyphen{}Client只需上传一小部分的维度值而不是所有的模型权重，因此联邦学习的上行通信量也被大大降低。}

\sphinxAtStartPar
下面，我们分别对topk集合\(S_k\)的构建和EM\sphinxhyphen{}MDS维度选择算法进行详细介绍。

\sphinxAtStartPar
首先，由于实际更新值有正负，直接给所有选定的维度分配相同的常量值可能会明显改变模型更新方向，影响模型收敛。为了解决这个问题，SignDS提出了一种基于符号的topk集合构建策略。具体来讲，算法引入了一个额外的符号变量\(s\in\\{-1,1\\}\)。该变量由FL\sphinxhyphen{}Client以等概率随机采样，用于确定本地更新\(\Delta\)的topk集合\(S_k\)。如果\(s=1\)，我们将\(\Delta\)按\sphinxstylestrong{真实更新值}排序，并将\sphinxstylestrong{最大}的\(k\)个更新维度记为\(S_k\)。我们进一步从\(S_k\)中随机选择一部分维度，并将\(s=1\)作为这些维度的更新值用以构建稀疏更新。直觉上，\(S_k\)中维度的更新值很可能大于零。因此，将\(s=1\)分配给选定的维度不会导致模型更新方向的太大差异，从而减轻了对模型精度的影响。类似的，当\(s=-1\)时，我们选取\sphinxstylestrong{最小}的\(k\)个更新维度记为\(S_k\)，并将\(s=-1\)分配给所选维度。

\sphinxAtStartPar
下面，我们进一步介绍用于维度选择的EM\sphinxhyphen{}MDS算法。简单来说，EM\sphinxhyphen{}MDS算法的目的是从输出维度域\(\mathcal{J}\)中以一定概率\(\mathcal{P}\)随机选择一个维度集合\(J\in\mathcal{J}\)，不同维度集合对应的概率不同。我们假设\(J\)总共包含\(h\)个维度，其中有\(\nu\)个维度属于topk集合（即\(|S_k \cap J|=\nu\)，且\(\nu\in[0,h]\)），另外\(h-\nu\)个维度属于非topk集合。直观上，\(\nu\)越大，\(J\)中包含的topk维度越多，模型收敛越好。因此，我们希望给\(\nu\)较大的维度集合分配更高的概率。基于这个想法，我们将评分函数定义为：
\begin{equation}\label{equation:chapter_federated_learning/privacy_encryption_algorithm:chapter_federated_learning/privacy_encryption_algorithm:4}
\begin{split}u(S_{k}, J) = 𝟙(|S_k\cap J| \geq \nu_{th}) =  𝟙(\nu \geq \nu_{th})
:label: score_function\end{split}
\end{equation}
\sphinxAtStartPar
\(u(S_{k}, J)\)用来衡量输出维度集合\(J\)中包含的topk维度的数量是否超过某一阈值\(\nu_{th}\)（\(\nu_{th}\in[1,h]\)），超过则为1，否则为0。进一步，\(u(S_{k}, J)\)的敏感度可计算为：
\begin{equation}\label{equation:chapter_federated_learning/privacy_encryption_algorithm:chapter_federated_learning/privacy_encryption_algorithm:5}
\begin{split}\phi = \max_{J\in\mathcal{J}} ||u(S_{k}, J) - u(S^\prime_{k}, J)||= 1 - 0 = 1
:label: sensitivity\end{split}
\end{equation}
\sphinxAtStartPar
注意
\sphinxcode{\sphinxupquote{sensitivity}}对于任意一对不同的topk集合\(S_k\)和\(S_k^\prime\)均成立。

\sphinxAtStartPar
根据以上定义，EM\sphinxhyphen{}MDS算法描述如下：

\sphinxAtStartPar
\sphinxstyleemphasis{给定真实本地更新:math:`Deltainmathbb\{R\}\textasciicircum{}\{d\}`的topk集合:math:`S\_k`和隐私预算:math:`epsilon`，输出维度集合:math:`Jinmathcal\{J\}`的采样概率为：}
\begin{equation}\label{equation:chapter_federated_learning/privacy_encryption_algorithm:chapter_federated_learning/privacy_encryption_algorithm:6}
\begin{split}    \mathcal{P}=\frac{\mathrm{exp}(\frac{\epsilon}{\phi}\cdot u(S_{k}, J))}{\sum_{J^\prime\in\mathcal{J}}\mathrm{exp}(\frac{\epsilon}{\phi}\cdot u(S_{k}, J^\prime))}
    =
    \frac{\mathrm{exp}(\epsilon\cdot 𝟙(\nu \geq \nu_{th}))}{\sum_{\tau=0}^{\tau=h}\omega_{\tau}\cdot \mathrm{exp}(\epsilon\cdot 𝟙(\tau\geq\nu_{th}))}
    =
    \frac{\mathrm{exp}(\epsilon\cdot 𝟙(\nu \geq \nu_{th}))}{\sum_{\tau=0}^{\tau=\nu_{th}-1}\omega_{\tau} + \sum_{\tau=\nu_{th}}^{\tau=h}\omega_{\tau}\cdot \mathrm{exp}(\epsilon)}
:label: emmds\end{split}
\end{equation}
\sphinxAtStartPar
\sphinxstyleemphasis{其中，:math:`nu`是:math:`J`中包含的topk维度数量，:math:`nu\_\{th\}`是评分函数的阈值，:math:`J\textasciicircum{}prime`是任意一输出维度集合，:math:`omega\_\{tau\}=binom\{k\}\{tau\}binom\{d\sphinxhyphen{}k\}\{h\sphinxhyphen{}tau\}`是所有包含:math:`tau`个topk维度的集合数。}

\sphinxAtStartPar
我们进一步提供了EM\sphinxhyphen{}MDS算法的隐私证明:

\sphinxAtStartPar
对于每个FL\sphinxhyphen{}Client，给定随机采样的符号值\(x\)，任意两个本地更新\(\Delta\)，\(\Delta^\prime\)的topk集合记为\(S_k\)和\(S_k^\prime\)，对于任意输出维度集合\(J\in\mathcal{J}\)，令\(\nu=|S_k \cap J|\),
\(\nu^\prime=|S_k^\prime \cap J|\)为\(J\)与两组topk维度集的交集数量。根据
\sphinxcode{\sphinxupquote{emmds}}，以下不等式成立：
\begin{equation}\label{equation:chapter_federated_learning/privacy_encryption_algorithm:chapter_federated_learning/privacy_encryption_algorithm:7}
\begin{split}\frac{\mathrm{Pr}\[J|\Delta\]}{\mathrm{Pr}\[J|\Delta^\prime\]} = \frac{\mathrm{Pr}\[J|S_{k}\]}{\mathrm{Pr}\[J|S^\prime_{k}\]} = \frac{\frac{\mathrm{exp}(\frac{\epsilon}{\phi}\cdot u(S_{k}, J))}{\sum_{J^\prime\in\mathcal{J}}\mathrm{exp}(\frac{\epsilon}{\phi}\cdot u(S_{k}, J^\prime))}}{\frac{\mathrm{exp}(\frac{\epsilon}{\phi}\cdot u(S^\prime_{k}, J))}{\sum_{J^\prime\in\mathcal{J}}\mathrm{exp}(\frac{\epsilon}{\phi}\cdot u(S^\prime_{k}, J^\prime))}}
    = \frac{\frac{\mathrm{exp}(\epsilon\cdot 𝟙(\nu \geq \nu_{th}))}{\sum_{\tau=0}^{\tau=h}\omega_{\tau}\cdot \mathrm{exp}(\epsilon\cdot 𝟙(\tau\geq\nu_{th}))}}{\frac{
    \mathrm{exp}(\epsilon\cdot 𝟙(\nu^\prime \geq \nu_{th}))}{\sum_{\tau=0}^{\tau=h}\omega_{\tau}\cdot \mathrm{exp}(\epsilon\cdot 𝟙(\tau\geq\nu_{th}))}} \\
    = \frac{\mathrm{exp}(\epsilon\cdot 𝟙(\nu \geq \nu_{th}))}{
    \mathrm{exp}(\epsilon\cdot 𝟙(\nu^\prime \geq \nu_{th}))}
    \leq \frac{\mathrm{exp}(\epsilon\cdot 1)}{\mathrm{exp}(\epsilon\cdot 0)} = \mathrm{exp}(\epsilon)\end{split}
\end{equation}
\sphinxAtStartPar
\sphinxstyleemphasis{证明EM\sphinxhyphen{}MDS算法满足:math:`epsilon`\sphinxhyphen{}LDP保证。}

\sphinxAtStartPar
值得注意的是，计算
\sphinxcode{\sphinxupquote{emmds}}需要先确定topk维度数的阈值\(\nu_{th}\)。为此，我们首先推导在给定阈值\(\nu_{th}\)时，任意一组输出维度集合\(J\)包含的topk维度的概率分布和期望：
\begin{equation}\label{equation:chapter_federated_learning/privacy_encryption_algorithm:chapter_federated_learning/privacy_encryption_algorithm:8}
\begin{split}\mathrm{Pr}(\nu=\tau|\nu_{th})=
    \begin{cases}
        \omega_{\tau} / \Omega \quad \quad \quad \quad \quad \mathrm{ } &if \quad \tau\in\[0,\nu_{th}\) \\
        \omega_{\tau}\cdot\mathrm{exp}(\epsilon) / \Omega \quad \quad &if \quad \tau\in\[\nu_{th},h\]
    \end{cases}
:label: discrete-prob\end{split}
\end{equation}\begin{equation}\label{equation:chapter_federated_learning/privacy_encryption_algorithm:chapter_federated_learning/privacy_encryption_algorithm:9}
\begin{split}    \mathbb{E}\[\nu|\nu_{th}\] = \sum_{\tau=0}^{\tau=h}\tau\cdot \mathrm{Pr}(\nu=\tau|\nu_{th})
:label: expectation\end{split}
\end{equation}
\sphinxAtStartPar
这里，\(\Omega\)为
\sphinxcode{\sphinxupquote{emmds}}中\(\mathcal{P}\)的分母部分。直觉上，\(\mathbb{E}\[\nu|\nu_{th}\]\)越高，随机采样的\(J\)集合中包含的topk维度的概率就越大，从而模型效用就越好。因此，我们将\(\mathbb{E}\[\nu|\nu_{th}\]\)最高时的阈值确定为目标阈值\(\nu_{th}^{\*}\)，即：
\begin{equation}\label{equation:chapter_federated_learning/privacy_encryption_algorithm:chapter_federated_learning/privacy_encryption_algorithm:10}
\begin{split}\nu_{th}^{\*} = \underset{\nu_{th}\in\[1, h\]}{\operatorname{argmax}} \mathbb{E}\[\nu|\nu_{th}\]
:label: threshold\end{split}
\end{equation}
\sphinxAtStartPar
最后，我们在
\hyperref[\detokenize{chapter_federated_learning/privacy_encryption_algorithm:signds-workflow}]{图\ref{\detokenize{chapter_federated_learning/privacy_encryption_algorithm:signds-workflow}}}中描述了SignDS算法的详细流程。给定本地模型更新\(\Delta\)，我们首先随机采样一个符号值\(s\)并构建topk集合\(S_k\)。接下来，我们根据
\sphinxcode{\sphinxupquote{threshold}}确定阈值\(\nu_{th}^{\*}\)并遵循
\sphinxcode{\sphinxupquote{emmds}}定义的概率选择输出集合\(J\)。考虑到输出域\(\mathcal{J}\)包含\(\binom{d}{k}\)个可能的维度集合，以一定概率直接从\(\mathcal{J}\)中随机采样一个组合需要很大的计算成本和空间成本。因此，我们采用了逆采样算法以提升计算效率。具体来说，我们首先从标准均匀分布中采样一个随机值\(\beta\sim U(0,1)\)，并根据
\sphinxcode{\sphinxupquote{discrete\sphinxhyphen{}prob}}中\(p(\nu=\tau|\nu_{th})\)的累计概率分布\(CDF_{\tau}\)确定输出维度集合中包含的topk维度数\(\nu\)。最后，我们从topk集合\(S_k\)中随机选取\(\nu\)个维度，从非topk集合中随机采样\(h-\nu\)个维度，以构建最终的输出维度集合\(J\)。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch10-federated-learning-signds}.PNG}
\caption{SignDS工作流程}\label{\detokenize{chapter_federated_learning/privacy_encryption_algorithm:id5}}\label{\detokenize{chapter_federated_learning/privacy_encryption_algorithm:signds-workflow}}\end{figure}


\section{展望}
\label{\detokenize{chapter_federated_learning/outlook:id1}}\label{\detokenize{chapter_federated_learning/outlook::doc}}
\sphinxAtStartPar
为了实现联邦学习的大规模商用，我们仍然需要做许多的研究工作。比如我们无法查看联邦学习的分布式化的数据，那就很难选择模型的超参数以及设定优化器，只能采用一些基于模拟的方案来调测模型；比如用于移动设备时，单用户的标签数据很少，甚至无法获取数据的标签信息，联邦学习如何用于无监督学习；比如由于参与方的数据分布不一致，训练同一个全局模型，很难评价模型对于每个参与方的好坏；比如数据一直是公司的核心资产，不同的公司一直在致力于收集数据和创造数据孤岛，如何有效地激励公司或者机构参与联邦学习的系统中来。下面将介绍一些MindSpore
Federated在进行的一些尝试和业界的相关工作。

\sphinxAtStartPar
\sphinxstylestrong{异构场景下的联邦学习}

\sphinxAtStartPar
之前探讨的横向联邦和纵向联邦学习都是让不同的参与方共同建立一个共享的机器学习模型。然而，企业级联邦学习框架往往需要适应多种异构场景，如数据异构（不同客户端数据规模以及分布不一致），设备异构（不同客户端设备计算能力，通信效率不一致），以及模型异构（不同本地客户端模型学到的特征不一致）。

\sphinxAtStartPar
比较主流的两种联邦异构场景下的工作：

\sphinxAtStartPar
1）对异构数据具有高度鲁棒性的本地模型个性化联邦学习策略：

\sphinxAtStartPar
联邦学习训练的是一个全局模型，基于所有数据得到一个全局最优解，但是不同参与方的数据量和分布都是不同的，很多场景下全局模型无法在把握整体的同时又照顾到这种差异。当某一方的数据和整体偏离比较大时，联邦学习的效果确实有可能不如本地训练的效果。那么如何在所有参与方总体的收益最大化的同时，让个体的收益也能够最大化，这就是个性化联邦学习。

\sphinxAtStartPar
个性化联邦学习并不要求所有参与方最终使用的模型必须是一样的，比如允许每个参与方在参与联邦之后，根据自己的数据对模型进行微调，从而生成本方独特的个性化模型。在进行个性化微调之后，往往模型在本地测试集上的效果会更好。在这种方式下，不同参与方的模型结构是一样的，但是模型参数会有所不同。还有一些方案，是让所有的参与方拥有同样的特征提取层，但是任务分类层不同。还有的思路是将知识蒸馏引入联邦学习中，将联邦学习的全局模型作为teacher，将个性化模型作为student，可以缓解个性化过程中的过拟合问题。

\sphinxAtStartPar
2）对于异构模型进行模型聚合的策略研究：

\sphinxAtStartPar
一般在FedAvg的联邦聚合范式下，本地迭代训练次数越少、聚合地越频繁，模型收敛精度会越好，尤其是在不同参与客户端的数据是非iid的情况下。但是聚合会带来通信成本开销，联邦学习存在通信成本与模型精度的Trade\sphinxhyphen{}off。因此很多研究者聚焦于如何设计自适应聚合方案，要求在给定训练时间开销的前提下，找到本地更新和全局通信之间的最佳平衡，令全局模型的泛化误差最小。

\sphinxAtStartPar
\sphinxstylestrong{通信效率提升}

\sphinxAtStartPar
端云联邦学习流程中，每一个全局训练轮次里，每个参与方都需要给服务器发送完整的模型参数更新，然后服务器将聚合后的模型参数下发。现代的深度学习网络动辄有数百万甚至更大量级的参数，如此多的参数量传输将会带来巨大的通信开销带来性能瓶颈（用户流量、通信时延、客户端参与数量）。为了降低通信开销，MindSpore
Federated采取了一些改善通信效率的方法：

\sphinxAtStartPar
1）智能调频策略：通过改变全局模型聚合的轮次来提高联邦学习效率，减少训练任务达到收敛的通信开销。一种直觉是在模型训练初期，参数变化剧烈，因此设置较大的聚合频率，使得模型快速收敛；在模型训练中后期，参数趋于稳定，因此设置较小的聚合频率，减少通信成本。

\sphinxAtStartPar
2）通信压缩方案：对权重差进行量化以及稀疏化操作，即每次通信仅上传一小部分最重要参数的梯度，从而代替上传所有参数。之所以选择权重差做量化和稀疏，是因为它比权重值的分布更易拟合，而且稀疏性更高。量化就是将FP32的数据类型映射到INT8甚至更低比特表示的数值上，一方面降低存储和通信开销，另一方面可以更好地采用一些压缩编码方式进行传输（如哈夫曼编码、有限状态熵编码等）。比较常用的稀疏化方法有Top\sphinxhyphen{}k稀疏，即按梯度的绝对值从小到大排序，每轮只上传前k个参数，如何选取合适的(k)是一个有挑战性的问题。这个方案对于模型的联邦学习训练来说是有损的，在精度损失和通信压缩率之间也是trade\sphinxhyphen{}off。

\sphinxAtStartPar
\sphinxstylestrong{联邦生态}

\sphinxAtStartPar
在前面的章节中，我们介绍了面向隐私保护的联邦学习领域的一些技术与实践，然而随着探索地更加深入，联邦学习领域也变得更具包容性，它涵盖了分不是机器学习、模型压缩部署、信息安全、加密算法、博弈论等等。随着越来越多的公司、高校和机构参与进来，现在的联邦学习已经不仅仅是一种技术解决方案，还是一个隐私保护的生态系统，比如不同的参与方希望以可持续的方式加入联邦，如何设计激励机制，以确保利润可以相对公平地被联邦参与方共享，同时对于恶意的实施攻击或者破坏行为的参与方进行有效遏制。

\sphinxAtStartPar
另外，随着用户数据隐私保护和合理使用的法律法规越来越多的被推出，制定联邦学习的技术标准显得愈加重要，这一标准能够在法律监管部门和技术开发人员之间建立一座桥梁，让企业知道采用何种技术，能够在合乎法规的同时更好地进行信息的共享。

\sphinxAtStartPar
2020年底正式出版推行了由IEEE
标准委员会（SASB）通过的联邦学习国际标准（IEEE
P3652.1），该标准旨在提供一个搭建联邦学习的体系架构和应用的指导方针，主要内容包括：联邦学习的描述和定义、场景需求分类和安全测评、联邦学习个性指标的评估如何量化、联合管控的需求。这也是国际上首个针对人工智能协同技术框架订立的标准，标志着联邦学习开启大规模工业化应用的新篇章。


\section{小结}
\label{\detokenize{chapter_federated_learning/summary:id1}}\label{\detokenize{chapter_federated_learning/summary::doc}}
\sphinxAtStartPar
在这一章，我们简单介绍了联邦学习的背景、系统架构、联邦平均算法、隐私加密算法以及实际部署时的挑战。联邦学习是一个新起步的人工智能算法，可以在“数据保护”与“数据孤岛”这两大约束条件下，建立有效的机器学习模型。此外，由于联邦学习场景的特殊性（端侧数据不上传、安全隐私要求高和数据非独立同分布等特点），使得系统和算法的开发难度更高：如何平衡计算和通讯的开销？如何保证模型不会泄露隐私？算法如何在非独立同分布场景下收敛？这些难点都需要开发人员对实际的联邦学习场景有更深刻的认识。

\sphinxAtStartPar
在这一章，我们简单介绍了联邦学习的背景、系统架构、联邦平均算法、隐私加密算法以及实际部署时的挑战。联邦学习是一个新起步的人工智能算法，可以在“数据保护”与“数据孤岛”这两大约束条件下，建立有效的机器学习模型。

\sphinxAtStartPar
此外，由于联邦学习场景的特殊性（端侧数据不上传、安全隐私要求高和数据非独立同分布等特点），使得系统和算法的开发难度更高。如何平衡计算和通讯的开销，如何保证模型不会泄露隐私，以及算法如何在非独立同分布场景下收敛等等，都需要开发人员对实际的联邦学习场景有更深刻的认识。


\chapter{强化学习系统}
\label{\detokenize{chapter_reinforcement_learning/index:id1}}\label{\detokenize{chapter_reinforcement_learning/index::doc}}
\sphinxAtStartPar
在本章中，我们介绍深度学习的一个重要分支——强化学习及其在系统方面的知识。本章的学习目标包括：
\begin{itemize}
\item {} 
\sphinxAtStartPar
掌握强化学习基本知识。

\item {} 
\sphinxAtStartPar
掌握单节点和多节点强化学习系统设计思路。

\item {} 
\sphinxAtStartPar
掌握多智能体强化学习基础知识及系统设计简介。

\end{itemize}


\section{强化学习介绍}
\label{\detokenize{chapter_reinforcement_learning/rl_introduction:id1}}\label{\detokenize{chapter_reinforcement_learning/rl_introduction::doc}}
\sphinxAtStartPar
近年来，强化学习作为机器学习的一个分支受到越来越多的关注。从2013年起，DeepMind公司的研究人员就提出深度Q学习
\sphinxcite{chapter_reinforcement_learning/summary:mnih2013playing}（Deep
Q\sphinxhyphen{}learning）用于学习7个不同的电子游戏中对象的操作。自此以后，以DeepMind为首的科研机构推出了像AlphaGo下围棋这类的引人瞩目的强化学习成果，并在2016年与世界顶级围棋高手李世石的对战中取得胜利。自那以后，强化学习领域连续取得了一系列成就，如星际争霸游戏智能体AlphaStar、Dota
2游戏智能体OpenAI
Five、多人零和博弈德州扑克的Pluribus、机器狗运动控制算法等。在这一系列科研成就的背后，是整个强化学习领域算法在这些年内快速迭代进步的结果，基于模拟器产生的大量数据使得对数据“饥饿”（Data
Hungry）的深度神经网络能够表现出很好的拟合效果，从而将强化学习算法的能力充分发挥出来，在以上领域中达到或者超过人类专家的学习表现。目前，强化学习已经从电子游戏逐步走向更广阔的应用场景，如机器人控制、机械手灵巧操作、能源系统调度、网络负载分配、股票期货交易等一系列更加现实和富有意义的领域，对传统控制方法和启发式决策理论发起冲击。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=400\sphinxpxdimen]{{ch12-rl}.png}
\caption{强化学习框架}\label{\detokenize{chapter_reinforcement_learning/rl_introduction:id3}}\label{\detokenize{chapter_reinforcement_learning/rl_introduction:ch12-ch12-rl-framework}}\end{figure}

\sphinxAtStartPar
强化学习的核心是决策，即基于某个\sphinxstylestrong{状态}（State）下的\sphinxstylestrong{动作}（Action）的选择。进行这一决策的对象我们常称为\sphinxstylestrong{智能体}（Agent），而这一决策的影响将在\sphinxstylestrong{环境}（Environment）中体现。更具体地，不同的决策会影响环境的\sphinxstylestrong{状态转移}（State
Transition），以及\sphinxstylestrong{奖励}（Reward）。以上过程可以抽象为
\hyperref[\detokenize{chapter_reinforcement_learning/rl_introduction:ch12-ch12-rl-framework}]{图\ref{\detokenize{chapter_reinforcement_learning/rl_introduction:ch12-ch12-rl-framework}}}所示，这是文献中最常见的强化学习的模型描述。举例来说，当人在玩某个电子游戏的时候，需要逐渐熟悉游戏的操作以取得更好的游戏结果，那么人从刚接触到这个游戏到逐步掌握游戏技巧的这个过程为一个类似于强化学习的过程。该游戏从开始后的任一时刻，会处于一个特定的状态，而人通过观察这个状态（如观察游戏机显示屏的图像）会获得一个\sphinxstylestrong{观察量}（Observation），并基于这个观察量做出一个操作动作，这一动作将改变这个游戏状态，使其转移到下一个状态，并且玩家可以知道当前动作的效果（如产生了一个正或负的分数）。这时玩家再基于下一个状态的观察量做出新的动作选择，周而复始，直到游戏结束。通过反复的操作和观察，人能够逐步掌握这个游戏的技巧，一个强化学习智能体也是如此。这里注意，有几个比较关键的问题：（1）观察量未必等于状态，而通常观察量是状态的函数，从状态到观察量的映射可能有一定的信息损失。对于观察量等于状态或者根据观察量能够完全恢复环境状态的情况，我们称为\sphinxstylestrong{完全可观测}（Fully
Observable），否则我们称为\sphinxstylestrong{部分可观测}（Partially
Observable）环境；（2）玩家的每个动作未必会产生立即反馈，某个动作可能在许多步之后才产生效果，强化学习模型允许这种延迟反馈的存在；（3）这种反馈对人的学习过程而言未必是个数字，但是我们对强化学习智能体所得到的反馈进行数学抽象，将其转变为一个数字，称为奖励值。奖励值可以是状态的函数，也可以是状态和动作的函数，依具体问题而定。奖励值的存在是强化学习问题的一个基本假设，也是现有强化学习算法训练智能体与监督式深度学习的一个主要区别。

\sphinxAtStartPar
强化学习的决策过程通常由一个马尔可夫决策过程（Markov Decision
Process，MDP）（马尔可夫决策过程即一个后续状态只依赖当前状态和动作而不依赖于历史状态的函数）描述，可以用一个数组\((\mathcal{S}, \mathcal{A}, R, \mathcal{T}, \gamma)\)来表示。\(\mathcal{S}\)和\(\mathcal{A}\)分别是状态空间和动作空间，\(R\)是奖励函数，\(R(s,a)\):
\(\mathcal{S}\times \mathcal{A}\rightarrow \mathbb{R}\)为对于当前状态\(s\in\mathcal{S}\)和当前动作\(a\in\mathcal{A}\)的奖励值。从当前状态和动作到下一个状态的状态转移概率定义为\(\mathcal{T}(s^\prime|s,a)\):
\(\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow \mathbb{R}_+\)。\(\gamma\in(0,1)\)是奖励折扣因子（折扣因子可以乘到每个后续奖励值上，从而使无穷长序列有有限的奖励值之和）。强化学习的目标是最大化智能体的期望累计奖励值\(\mathbb{E}[\sum_t \gamma^t r_t]\)。

\sphinxAtStartPar
马尔可夫决策过程中的马尔可夫性质由以下定义
\begin{equation}\label{equation:chapter_reinforcement_learning/rl_introduction:chapter_reinforcement_learning/rl_introduction:0}
\begin{split}\mathcal{T}(s_{t+1}|s_t) = \mathcal{T}(s_{t+1}|s_0, s_1, s_2, \dots, s_t)\end{split}
\end{equation}
\sphinxAtStartPar
即当前状态转移只依赖于上一时刻状态，而不依赖于整个历史。这里的状态转移函数\(\mathcal{T}\)中省略了动作\(a\)，因为马尔可夫性质是独立于决策过程的环境转移过程的属性。

\sphinxAtStartPar
基于马尔可夫性质，可以进一步推导出在某一时刻最优策略不依赖于整个决策历史，而只依赖于当前最新状态的结论。这一结论在强化学习算法设计中有着重要意义，它简化了最优策略的求解过程。


\section{单节点强化学习系统}
\label{\detokenize{chapter_reinforcement_learning/single_node_rl:id1}}\label{\detokenize{chapter_reinforcement_learning/single_node_rl::doc}}
\sphinxAtStartPar
前面介绍了强化学习的基本知识和在系统层面的一般需求，这里我们介绍常见的单智能体强化学习系统中较为简单的一类，即单节点强化学习系统。这里，我们按照是否对模型训练和更新进行并行处理，将强化学习系统分为单节点和分布式强化学习系统。其中，单节点强化学习系统可以理解为只实例化一个类对象作为智能体，与环境交互进行采样和利用所采得的样本进行更新的过程分别视为这个类内的不同函数。除此之外的更为复杂的强化学习框架都可视为分布式强化学习系统。分布式强化学习系统的具体形式有很多，这也往往依赖于所实现的算法。从最简单的情况考虑，假设我们仍在同一个计算单元上实现算法，但是将强化学习的采样过程和更新过程实现为两个并行的进程，甚至各自实现为多个进程，以满足不同计算资源间的平衡。这时就需要进程间通信来协调采样和更新过程，这是一个最基础的分布式强化学习框架。更为复杂的情况是，整个算法的运行在多个计算设备上进行（如一个多机的计算集群），智能体的函数可能需要跨机跨进程间的通信来实现。对于多智能体系统，还需要同时对多个智能体的模型进行更新，则需要更为复杂的计算系统设计。我们将逐步介绍这些不同的系统内的实现机制。

\sphinxAtStartPar
我们先对单节点强化学习系统进行介绍。 在这里，我们以RLzoo
\sphinxcite{chapter_reinforcement_learning/summary:ding2020efficient}为例，讲解一个单节点强化学习系统构建所需要的基本模块。如
\hyperref[\detokenize{chapter_reinforcement_learning/single_node_rl:ch12-ch12-rlzoo}]{图\ref{\detokenize{chapter_reinforcement_learning/single_node_rl:ch12-ch12-rlzoo}}}所示，是RLzoo中采用的一个典型的单节点强化学习系统，它包括几个基本的组成部分：神经网络、适配器、策略网络和价值网络、环境实例、模型学习器、经验回放缓存（Experience
Replay Buffer）等。

\sphinxAtStartPar
我们先对前三个，神经网络、适配器、策略网络和价值网络进行介绍。神经网络即一般深度学习中使用的神经网络，用于实现基于数据的函数拟合，特点是可以用梯度下降的方法更新。我们在图中简单列出常见的三类神经网络：全连接网络，卷积网络和循环网络。策略网络和价值网络是一般深度强化学习的常见组成部分，分别是对策略函数和价值函数的近似表示。策略网络即一个由参数化深度神经网络表示的动作策略，而价值网络为神经网络表示的状态价值（State\sphinxhyphen{}Value）或状态\sphinxhyphen{}动作价值（State\sphinxhyphen{}Action
Value）函数。这里我们不妨称全连接网络，卷积网络和循环网络为一般神经网络，它们常作为基本构建模块而被用来搭建强化学习中的策略网络和价值网络。在RLzoo中，适配器则是为实现强化学习特定函数近似而选配一般神经网络的功能模块，每个适配器是一个根据网络输入输出格式决定的网络格式选择函数。如:numref:\sphinxtitleref{ch12/ch12\sphinxhyphen{}rlzoo}所示，RLzoo在实现中使用了三个不类型的适配器来使得强化学习算法构建过程有自适应能力。首先，根据不同的观察量类型，强化学习智能体所用的神经网络头部会有不同的结构，这一选择可以由一个基于观察量的适配器来实现；其次，根据所采用的强化学习算法类型，相应的策略网络尾部需要有不同的输出类型，包括确定性策略和随机性策略，RLzoo中使用一个策略适配器来进行选择；最后，根据不同的动作输出，如离散型、连续型、类别型等，需要使用一个动作适配器来选择。介绍完这些，我们已经有了可用的策略网络和价值网络，这构成了强化学习智能体核心学习模块。除此之外，还需要一个学习器（Learner）来更新这些学习模块，更新的规则就是强化学习算法给出的损失函数。而要想实现学习模块的更新，最重要的是输入的学习数据，即智能体跟环境交互过程中所采集的样本。对于\sphinxstylestrong{离线}（Off\sphinxhyphen{}Policy）强化学习，这些样本通常被存储于一个称为经验回放缓存的地方，学习器在需要更新模型时从该缓存中采得一些样本来进行更新。这里说到的离线强化学习是强化学习算法中的一类，强化学习算法可以分为\sphinxstylestrong{在线}（On\sphinxhyphen{}Policy）强化学习和离线强化学习两类，按照某个特定判据。这个判据是，用于更新的模型和用于采样的模型是否为同一个，如果是，则称在线强化学习算法，否则为离线强化学习算法。因而，离线强化学习通常允许与环境交互的策略采集的样本被存储于一个较大的缓存内，从而允许在许久之后再从这个缓存中抽取样本对模型进行更新。而对于在线强化学习，这个“缓存”有时其实也是存在的，只不过它所存储的是非常近期内采集的数据，从而被更新模型和用于采样的模型可以近似认为是同一个。从而，这里我们简单表示RLzoo的强化学习系统统一包括这个经验回放缓存模块。有了以上策略和价值网络、经验回放缓存、适配器、学习器，我们就得到了RLzoo中一个单节点的强化学习智能体，将这个智能体与环境实例交互，并采集数据进行模型更新，我们就得到了一个完整的单节点强化学习系统。这里的环境实例化我们允许多个环境并行采样。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch12-rlzoo}.png}
\caption{RLzoo算法库中使用的强化学习系统}\label{\detokenize{chapter_reinforcement_learning/single_node_rl:id4}}\label{\detokenize{chapter_reinforcement_learning/single_node_rl:ch12-ch12-rlzoo}}\end{figure}

\sphinxAtStartPar
近来研究人员发现，强化学习算法领域的发展瓶颈，可能不仅在于算法本身，而在于智能体采集数据的模拟器的模拟速度。Isaac
Gym
\sphinxcite{chapter_reinforcement_learning/summary:makoviychuk2021isaac}是Nvidia公司于2021年推出的基于GPU（Graphics
Processing Unit）的模拟引擎，在单GPU上实现2\sphinxhyphen{}3倍于之前基于CPU（Central
Processing
Unit）的模拟器的运行速度。关于GPU上运行加速我们已经在章节5中有所介绍。之所以GPU模拟能够对强化学习任务实现显著的加速效果，除了GPU本身多核心的并行运算能力之外，还在于这省却了CPU与GPU之间的数据传输和通信时间。传统的强化学习环境，如OpenAI
Gym（这是一个常用的强化学习基准测试环境）等，都是基于CPU进行的模拟计算，而深度学习方法的神经网络训练通常是在GPU（或TPU，Tensor
Processing
Unit）上进行的。从智能体与CPU上实例化的模拟环境交互过程所收集的数据样本，通常先暂时以CPU的数据格式存储，在使用的时候被转移到GPU上成为具有GPU数据类型的数据（如使用PyTorch时可通过tensor.to(device)的函数实现，只需将device设为“cuda”即可将一个类型为torch.Tensor的tensor转移到GPU上），然后来进行模型训练。同时，由于模型参数是以GPU上数据的类型存储的，调用模型进行前向传递的过程中也需要先将输入数据从CPU转移到GPU上，并且可能需要将模型输出的GPU数据再转移回CPU类型。这一系列冗余的数据转换操作都会显著增长模型学习的时间，并且也增加了算法实际使用过程中的工程量。Isaac
Gym模拟器的设计从底层上解决了这一困难，由于模拟器和模型双双实现在GPU上，他们之间的数据通信不再需要通过CPU来实现，从而绕过了CPU与GPU数据双向传输这一问题，实现了对强化学习任务中模拟过程的特定加速。


\section{分布式强化学习系统}
\label{\detokenize{chapter_reinforcement_learning/distributed_node_rl:id1}}\label{\detokenize{chapter_reinforcement_learning/distributed_node_rl::doc}}
\sphinxAtStartPar
分布式强化学习系统是比上面介绍的单节点强化学习系统更强大的一种。它能支持多环境多模型并行处理，主要是能同时在多个实际计算机系统上对多个模型进行更新，将大大提高强化学习系统的学习速度和整体表现。我们这里介绍分布式强化学习常见的算法和系统。

\sphinxAtStartPar
异步优势行动\sphinxhyphen{}批判者（Asynchronous Advantage
Actor\sphinxhyphen{}Critic，A3C）是由DeepMind研究人员
\sphinxcite{chapter_reinforcement_learning/summary:mnih2016asynchronous}于2016年提出的可以在多个计算设备上并行更新网络的学习算法。相比于
\hyperref[\detokenize{chapter_reinforcement_learning/single_node_rl:ch12-ch12-rlzoo}]{图\ref{\detokenize{chapter_reinforcement_learning/single_node_rl:ch12-ch12-rlzoo}}}中的单节点强化学习系统，A3C通过创建一组工作者（Worker），并将每个工作者分配到不同的计算设备上且为他们各自创建可以交互的环境来实现并行采样和模型更新，同时用一个主（Master）节点维护这些行动者（Actor）和批判者（Critic）网络的更新。行动者是策略网络，批判者是价值网络，
分别对应强化学习中的策略和价值函数。通过这样的设计，整个算法的各个工作者可以实时将所采集到样本计算出的梯度回传到主节点，来更新主节点的模型参数，并在主节点模型更新后随时下发到各个工作者进行模型更新。每个工作者可以单独在一个GPU上进行运算，从而整个算法可以在一个GPU集群上并行更新模型。算法结构由
\hyperref[\detokenize{chapter_reinforcement_learning/distributed_node_rl:ch12-ch12-a3c}]{图\ref{\detokenize{chapter_reinforcement_learning/distributed_node_rl:ch12-ch12-a3c}}}所示。研究表明，分布式强化学习训练除加速模型学习之外，由于其更新梯度是由多个计算节点各自对环境采样计算得到的，还有利于稳定学习表现。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch12-a3c}.png}
\caption{A3C分布式算法架构}\label{\detokenize{chapter_reinforcement_learning/distributed_node_rl:id10}}\label{\detokenize{chapter_reinforcement_learning/distributed_node_rl:ch12-ch12-a3c}}\end{figure}

\sphinxAtStartPar
重要性加权行动\sphinxhyphen{}学习者架构（Importance Weighted Actor\sphinxhyphen{}Learner
Architecture，IMPALA) 是由Lasse Espeholt等人于2018年
\sphinxcite{chapter_reinforcement_learning/summary:espeholt2018impala}提出的能够实现多机集群训练的强化学习框架。与A3C算法类似，IMPALA能够在多个GPU上并行进行梯度计算。具体地，IMPALA并行多个行动者（Actor）和学习者（Learner），每个行动者包含一个策略网络，并用它来和一个环境交互收集样本。所收集到的样本轨迹由行动者发送到各自的学习者，进行梯度计算。所有的学习者中有一个称为主学习者，它可以和其他所有学习者通信获取他们计算的梯度，从而在主学习者内部对模型进行更新，随后下发到各个学习者及行动者，做新一轮的采样和梯度计算。IMPALA被证明是比A3C更高效的分布式计算架构，它同时得益于一个特殊设计的学习者内的梯度计算函数，称为V\sphinxhyphen{}轨迹目标（V\sphinxhyphen{}trace
Target），通过重要性加权来稳定训练。我们这里侧重对分布式强化学习结构的介绍，对此不再赘述。感兴趣的读者可以参考原论文。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch12-impala}.png}
\caption{IMPALA分布式算法架构}\label{\detokenize{chapter_reinforcement_learning/distributed_node_rl:id11}}\label{\detokenize{chapter_reinforcement_learning/distributed_node_rl:ch12-ch12-impala}}\end{figure}

\sphinxAtStartPar
以上是两个著名的分布式强化学习算法A3C和IMPALA，最近研究中还有许多其他成果，如SEED
\sphinxcite{chapter_reinforcement_learning/summary:espeholt2019seed}、Ape\sphinxhyphen{}X
\sphinxcite{chapter_reinforcement_learning/summary:horgan2018distributed}等都对分布式强化学习有更好的效果，我们不再做过多介绍。下面我们将讨论几个典型的分布式强化学习算法库。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch12-rllib-arch}.svg}
\caption{RLlib系统架构}\label{\detokenize{chapter_reinforcement_learning/distributed_node_rl:id12}}\label{\detokenize{chapter_reinforcement_learning/distributed_node_rl:ch12-ch12-rllib}}\end{figure}

\sphinxAtStartPar
Ray
\sphinxcite{chapter_reinforcement_learning/summary:moritz2018ray}是由伯克利大学几名研究人员发起的一个分布式计算框架，基于Ray之上构建了一个专门针对强化学习的系统RLlib
\sphinxcite{chapter_reinforcement_learning/summary:liang2017ray}。RLlib是一个面向工业级应用的开源强化学习框架，同时包含了强化学习的算法库，它对非强化学习专家使用也很方便。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{ch12-rllib-distributed}.svg}
\caption{RLlib分布式训练}\label{\detokenize{chapter_reinforcement_learning/distributed_node_rl:id13}}\label{\detokenize{chapter_reinforcement_learning/distributed_node_rl:ch12-ch12-rllib-dist}}\end{figure}

\sphinxAtStartPar
RLlib的系统架构如
\hyperref[\detokenize{chapter_reinforcement_learning/distributed_node_rl:ch12-ch12-rllib}]{图\ref{\detokenize{chapter_reinforcement_learning/distributed_node_rl:ch12-ch12-rllib}}}所示，系统底层是构建在Ray的分布式计算和通信的基础组建之上，面向强化学习的领域概念，在Python层抽象了Trainer,
Environment,
Policy等基础组件，并为各个抽象组件提供了一些常用的内置实现，同时用户可以根据自己的算法场景对组件进行扩展，通过这些内置以及自定义的算法组件，研究人员可以方便快速地实现具体的强化学习算法。RLlib支持多种范式的分布式强化学习训练，如
\hyperref[\detokenize{chapter_reinforcement_learning/distributed_node_rl:ch12-ch12-rllib-dist}]{图\ref{\detokenize{chapter_reinforcement_learning/distributed_node_rl:ch12-ch12-rllib-dist}}}所示为基于同步采样的强化学习算法的分布式训练架构。其中每一个Rollout
Worker为一个独立进程，负责和对应的环境进行交互以完成经验采集，多个Rollout
Worker可以并行地完成环境交互；Trainer负责Rollout
Worker之间的协调，策略优化，以及将更新后的策略同步到Rollout Worker中。

\sphinxAtStartPar
强化学习中的策略通常可以采用深度神经网络，而基于深度神经网络的分布式强化学习训练，可以采用RLlib结合PyTorch或者TensorFlow等深度学习框架协同完成，深度学习框架负责策略网络的训练和更新，RLlib负责强化学习的算法计算。此外RLlib支持对环境交互使用向量化（Vectorized）的并行方式，允许外接模拟器，以及可以进行离线（Offline）强化学习。

\sphinxAtStartPar
对于分布式系统中样本回放缓冲池的管理，我们会提到另一个工作Reverb
\sphinxcite{chapter_reinforcement_learning/summary:cassirer2021reverb}。回忆本章开头，我们介绍了强化学习中的状态、动作、奖励等概念，实际强化学习算法进行训练所使用的数据正是存放在经验缓冲池中的这些数据元组，而每种数据自身的格式可能又有不同，实际使用时也需要对不同的数据做不同类型的操作。常见的数据操作类型如拼接、截取、乘积、转置、部分乘积、取均值、取极值等，而每种操作都可能需要对特定数据的特定维度进行，这常常给现有的强化学习框架在实践中产生一定的困难。为了方便强化学习过程中灵活使用不同的数据形式，Reverb设计了数据块的概念（Chunks），所有使用的训练数据在缓冲池中都使用数据块的格式进行管理和调用，这一设计基于数据是多维张量的特点，增大了数据使用的灵活性和访问速度。Acme
\sphinxcite{chapter_reinforcement_learning/summary:hoffman2020acme}是近年来由DeepMind提出的一个分布式强化学习框架，同样是针对学术界的研究和工业界的应用，它基于Reverb对样本缓冲池的数据管理，结合分布式采样的结构，给出了一个更快的分布式强化学习解决方案。Reverb帮助解决了数据管理和传输的效率问题，使得Acme得以将分布式计算的效力充分发挥，研究人员用Acme在大量强化学习基准测试中取得了显著的速度提升。


\section{多智能体强化学习}
\label{\detokenize{chapter_reinforcement_learning/marl:id1}}\label{\detokenize{chapter_reinforcement_learning/marl::doc}}
\sphinxAtStartPar
以上所讲述的强化学习内容都为单智能体强化学习，而在近来的强化学习研究中，多智能体强化学习越来越受到研究人员关注。回想在本小节初介绍的单智能体强化学习框架
\hyperref[\detokenize{chapter_reinforcement_learning/rl_introduction:ch12-ch12-rl-framework}]{图\ref{\detokenize{chapter_reinforcement_learning/rl_introduction:ch12-ch12-rl-framework}}}，其中我们只有单个智能体产生的单个动作对环境产生影响，环境也返回单个奖励值给智能体。这里我们把单智能体强化学习扩展到多智能体强化学习，可以得到至少两种可能的多智能体强化学习框架，如
\hyperref[\detokenize{chapter_reinforcement_learning/marl:ch12-ch12-marl}]{图\ref{\detokenize{chapter_reinforcement_learning/marl:ch12-ch12-marl}}}所示。
\hyperref[\detokenize{chapter_reinforcement_learning/marl:ch12-ch12-marl}]{图\ref{\detokenize{chapter_reinforcement_learning/marl:ch12-ch12-marl}}}(a)为多智能体同时执行动作的情况，他们相互之间观察不到彼此的动作，他们的动作一同对环境产生影响，并各自接受自己动作所产生的奖励。
\hyperref[\detokenize{chapter_reinforcement_learning/marl:ch12-ch12-marl}]{图\ref{\detokenize{chapter_reinforcement_learning/marl:ch12-ch12-marl}}}(b)为多智能体顺序执行动作的情况，后续智能体可能观察到前序智能体的动作，他们的动作一同对环境产生影响，并接受到各自的奖励值或共同的奖励值。除此之外，还有许多其他可能的多智能体框架，如更复杂的智能体间观察机制、智能体间通讯机制、多智能体合作与竞争等等。同时，这里假设多个智能体对环境的观察量都为环境的状态，这是最简单的一种，也是现实中最不可能出现的一种，实际情况下的多智能体往往对环境有各自不同的观察量。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch12-marl}.png}
\caption{两种可能的多智能体强化学习框架：（a）同步式多智能体决策；（b）异步式多智能体决策。}\label{\detokenize{chapter_reinforcement_learning/marl:id2}}\label{\detokenize{chapter_reinforcement_learning/marl:ch12-ch12-marl}}\end{figure}

\sphinxAtStartPar
这里我们可以根据前面对单智能体强化学习过程的马尔可夫决策过程描述，给出多智能体强化学习的马尔可夫决策过程，它可以用一个数组\((\mathcal{S}, N, \boldsymbol{\mathcal{A}}, \mathbf{R}, \mathcal{T}, \gamma)\)来表示。\(N\)是智能体个数，\(\mathcal{S}\)和\(\boldsymbol{\mathcal{A}}=(\mathcal{A}_1, \mathcal{A}_2, ..., \mathcal{A}_N)\)分别是环境状态空间和多智能体动作空间，其中\(A_i\)是第\(i\)个智能体的动作空间，\(\mathbf{R}=(R_1, R_2, ..., R_N)\)是多智能体奖励函数，\(\mathbf{R}(s,\mathbf{a})\):
\(\mathcal{S}\times \boldsymbol{\mathcal{A}}\rightarrow \mathbb{R}^N\)为对于当前状态\(s\in\mathcal{S}\)和当前多智能体动作\(\mathbf{a}\in\boldsymbol{\mathcal{A}}\)的奖励向量值，其中\(R_i\)是对第\(i\)个智能体的奖励值。从当前状态和动作到下一个状态的状态转移概率定义为\(\mathcal{T}(s^\prime|s,\mathbf{a})\):
\(\mathcal{S}\times\boldsymbol{\mathcal{A}}\times\mathcal{S}\rightarrow \mathbb{R}_+\)。\(\gamma\in(0,1)\)是奖励折扣因子（假设多个智能体采用相同的奖励折扣因子）。不同于单智能体强化学习，多智能体强化学习的目标除了常见的最大化每个智能体各自的期望累计奖励值\(\mathbb{E}[\sum_t \gamma^t r^i_t], i\in[N]\)之外，还有许多其他可能的学习目标，如达到纳什均衡、最大化团队奖励等等。

\sphinxAtStartPar
由上述介绍和定义可以发现，多智能体强化学习是一个比单智能体强化学习更加复杂的问题。而实际上，多个智能体的存在，对于每个智能体的决策而言，绝对不是简单的把每个单智能体决策累加的难度，实际情况要比单智能体决策问题复杂很多。多智能体系统的研究实际上是门古老的学科，它与博弈论（Game
Theory）密切相关，在深度强化学习盛行以前早已有大量研究和许多理论上未解的难题。其中一个典型的问题是纳什均衡在双人非零和博弈下没有多项式时间内可解的方法（实际上，这是一个PPAD（Polynomial
Parity Argument, Directed version）类的问题。（见论文Settling the
Complexity of Computing Two\sphinxhyphen{}Player Nash Equilibria. Xi Chen, et
al.）由于篇幅限制，我们这里无法对多智能体问题做深入探讨，我们可以用一个简单例子来介绍为什么多智能体强化学习问题无法简单地用单智能体强化学习算法来解。


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{剪刀\sphinxhyphen{}石头\sphinxhyphen{}布奖励值}\label{\detokenize{chapter_reinforcement_learning/marl:id3}}\label{\detokenize{chapter_reinforcement_learning/marl:tab-ch12-ch12-marl}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline

\sphinxAtStartPar

&\sphinxstyletheadfamily 
\sphinxAtStartPar
剪刀
&\sphinxstyletheadfamily 
\sphinxAtStartPar
石头
&\sphinxstyletheadfamily 
\sphinxAtStartPar
布
\\
\hline
\sphinxAtStartPar
剪刀
&
\sphinxAtStartPar
(0,0)
&
\sphinxAtStartPar
(\sphinxhyphen{}1,+1)
&
\sphinxAtStartPar
(+1,\sphinxhyphen{}1)
\\
\hline
\sphinxAtStartPar
石头
&
\sphinxAtStartPar
(+1,\sphinxhyphen{}1)
&
\sphinxAtStartPar
(0,0)
&
\sphinxAtStartPar
(\sphinxhyphen{}1,+1)
\\
\hline
\sphinxAtStartPar
布
&
\sphinxAtStartPar
(\sphinxhyphen{}1,+1)
&
\sphinxAtStartPar
(+1,\sphinxhyphen{}1)
&
\sphinxAtStartPar
(0,0)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
我们考虑一个大家都熟悉的游戏，
剪刀\sphinxhyphen{}石头\sphinxhyphen{}布，考虑两个玩家玩这个游戏的输赢情况，我们知道有这样的输赢关系：剪刀<石头<布<剪刀…这里的“<”即前一个纯策略被后一个纯策略完全压制，我们给予奖励值\sphinxhyphen{}1、+1到这两个玩家，当他们选择相同的纯策略时，奖励值均为0。于是我们得到一个奖励值表如
\hyperref[\detokenize{chapter_reinforcement_learning/marl:tab-ch12-ch12-marl}]{表\ref{\detokenize{chapter_reinforcement_learning/marl:tab-ch12-ch12-marl}}}所示，横轴为玩家1，纵轴为玩家2，表内的数组为玩家1和玩家2各自在相应动作下得到的奖励值。由于这个矩阵的反对称性，这个问题的纳什均衡策略对两个玩家相同，均为\((\frac{1}{3}, \frac{1}{3}, \frac{1}{3})\)的策略分布，即有各\(\frac{1}{3}\)的概率出剪刀、石头或布。如果我们把得到这个纳什均衡策略作为多智能体学习的目标，那么我们可以简单分析得到这个均衡策略无法通过简单的单智能体算法得到。考虑我们随机初始化两个玩家为任意两个纯策略，比如玩家1出剪刀，玩家2出石头。这时假设玩家2策略固定，可以把玩家2看做固定环境的一部分，于是可以使用任意单智能体强化学习算法对玩家1进行训练，使其最大化自己的奖励值。于是，玩家1会收敛到布的纯策略。这时再把玩家1固定，训练玩家2，玩家2又收敛到剪刀的纯策略。于是循环往复，整个训练过程始终无法收敛，玩家1和2各自在3个策略中循环却无法得到正确的纳什均衡策略。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{ch12-marl-sp}.png}
\caption{自学习算法示意图。}\label{\detokenize{chapter_reinforcement_learning/marl:id4}}\label{\detokenize{chapter_reinforcement_learning/marl:ch12-ch12-marl-sp}}\end{figure}

\sphinxAtStartPar
我们在上面这个例子中采用的学习方法其实是多智能体强化学习中最基础的一种，叫自学习（Selfplay），如
\hyperref[\detokenize{chapter_reinforcement_learning/marl:ch12-ch12-marl-sp}]{图\ref{\detokenize{chapter_reinforcement_learning/marl:ch12-ch12-marl-sp}}}所示。自学习的方法即固定当前对首次策略，按照单智能体优化的方法最大化一侧智能体的表现，这一过程称为最佳反应策略（Best
Response
Strategy）。之后再将这一最佳反应策略作为该智能体的固定策略，再来优化另一边的智能体策略，如此循环。我们可以看到自学习在特定的任务设置下可能无法收敛到我们想要的最终目标。正是由于多智能体学习过程中有类似循环结构的出现，我们需要更复杂的训练方法，和专门针对多智能体的学习方式来达到我们想要的目标。一般来讲，多智能体强化学习是比单智能体强化学习更复杂的一类，对于自学习的方法而言，单智能体强化学习的过程可以看做一个多智能体强化学习的子任务。从前面这一小游戏的角度来理解，当玩家1策略固定时，玩家1加游戏环境构成玩家2的实际学习环境，由于这个环境是固定的，玩家2可以通过单智能体强化学习来达到自身奖励值最大化；这时再固定玩家2的策略，玩家1又可以进行单智能体强化学习……这样，单智能体强化学习是多智能体任务的子任务。其他算法如虚构自学习（Fictitious
Self\sphinxhyphen{}play）
\hyperref[\detokenize{chapter_reinforcement_learning/marl:ch12-ch12-marl-fsp}]{图\ref{\detokenize{chapter_reinforcement_learning/marl:ch12-ch12-marl-fsp}}}，需要在每个单智能体强化学习的步骤中，对对手历史策略的平均策略求得最优应对策略，而对手的训练也是如此，进行循环，能够在上面剪刀\sphinxhyphen{}石头\sphinxhyphen{}布一类的游戏中保证收敛到纳什均衡策略。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{ch12-marl-fsp}.png}
\caption{虚构自学习算法示意图。}\label{\detokenize{chapter_reinforcement_learning/marl:id5}}\label{\detokenize{chapter_reinforcement_learning/marl:ch12-ch12-marl-fsp}}\end{figure}


\section{多智能体强化学习系统}
\label{\detokenize{chapter_reinforcement_learning/marl_sys:id1}}\label{\detokenize{chapter_reinforcement_learning/marl_sys::doc}}
\sphinxAtStartPar
上述的简单例子只是为了帮助读者理解强化学习在多智能体问题里的角色，而如今前沿的多智能体强化学习算法已经能够解决相当大规模的复杂多智能体问题，如星际争霸（StarCraft
II）、Dota
2等游戏，已相继被DeepMind、OpenAI等公司所研究的智能体AlphaStar
\sphinxcite{chapter_reinforcement_learning/summary:vinyals2019grandmaster}和OpenAI Five
\sphinxcite{chapter_reinforcement_learning/summary:berner2019dota}攻克，达到超越人类顶级玩家的水平。国内公司如腾讯、启元世界等也提出了星际争霸游戏的多智能体强化学习解决方案TStarBot\sphinxhyphen{}X
\sphinxcite{chapter_reinforcement_learning/summary:han2020tstarbot}和SCC
\sphinxcite{chapter_reinforcement_learning/summary:wang2021scc}。对于这类高度复杂的游戏环境，整个训练过程对分布式计算系统的要求更高，而整个训练过程可能需要分为多个阶段。以AlphaStar为例，它训练的智能体采用了监督学习与强化学习结合的方式。在训练早期，往往先采用大量的人类专业玩家标定数据进行有监督的学习，从而使智能体快速获得较好的能力，随后，训练会切换到强化学习过程，使用前面介绍的虚构自学习的算法进行训练，即自我博弈。为了得到一个表现最好的智能体，算法需要充分探索整个策略空间，从而在训练中不止对一个策略进行训练，而是对一个策略集群（League）进行训练，并通过类似演化算法的方式对策略集群进行筛选，得到大量策略中表现最好的策略。如
\hyperref[\detokenize{chapter_reinforcement_learning/marl_sys:ch12-ch12-marl-train}]{图\ref{\detokenize{chapter_reinforcement_learning/marl_sys:ch12-ch12-marl-train}}}所示，在训练过程中每个智能体往往需要和其他智能体以及剥削者（Exploiter）进行博弈，剥削者是专门针对某一个智能体策略的最佳对手策略，与之对抗可以提高策略自身的防剥削能力。通过对大量智能体策略进行训练并筛选的这类方法称为集群式训练（Population\sphinxhyphen{}based
Training/League
Training），是一种通过分布式训练提高策略种群多样性进而提升模型表现的方式。可见，在实践中这类方法自然需要分布式系统支持，来实现多个智能体的训练和相互博弈，这很好地体现了多智能体强化学习对分布式计算的依赖性。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch12-marl-train}.svg}
\caption{集群式多智能体强化学习训练示意图}\label{\detokenize{chapter_reinforcement_learning/marl_sys:id11}}\label{\detokenize{chapter_reinforcement_learning/marl_sys:ch12-ch12-marl-train}}\end{figure}

\sphinxAtStartPar
我们将对构建多智能体强化学习系统中的困难分为以下几点进行讨论：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{智能体个数带来的复杂度}：从单智能体系统到多智能体系统最直接的变化，就是智能体个数从1变为大于1个。对于一个各个智能体独立的\(N\)智能体系统而言，这种变化带来的策略空间表示复杂度是指数增加的，即\(\tilde{O}(e^N)\)。举个简单的例子，对于一个离散空间的单智能体系统，假设其状态空间大小为\(S\),
动作空间大小为\(A\)，游戏步长为\(H\)，那么这个离散策略空间的大小为\(O(HSA)\)；而直接将该游戏扩展为\(N\)玩家游戏后，在最一般的情况下，即所有玩家有对称的动作空间动作空间大小为\(A\)且不共享任何结构信息，所有玩家策略的联合分布空间大小为\(O(HSA^N)\)。这是因为每个独立玩家的策略空间构成联合策略空间是乘积关系\(\mathcal{A}=\mathcal{A}_1\times\dots\mathcal{A}_N\)。而这将直接导致算法搜索复杂度提升。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{游戏类型带来的复杂度}：从博弈论的角度，多智能系统所产生的游戏类型是复杂的。从最直接的分类角度，有竞争型、合作型、混合型。在竞争型游戏中，最典型的研究模型是二人零和博弈，如前一小结中提到的剪刀\sphinxhyphen{}石头\sphinxhyphen{}布的游戏。这类游戏中的纳什均衡策略一般为混合型策略，即无法通过单一纯策略达到均衡条件。纯策略纳什均衡存在于少数零和游戏中。合作型游戏即多个智能体需要通过合作来提升整体奖励。在这类问题研究中一般采用基于值分解的思路，将所有智能体得到的奖励值分配到单个智能体作为其奖励值。这一类的算法有VDN
\sphinxcite{chapter_reinforcement_learning/summary:sunehag2017value}, COMA
\sphinxcite{chapter_reinforcement_learning/summary:foerster2018counterfactual}, QMIX
\sphinxcite{chapter_reinforcement_learning/summary:rashid2018qmix}等。在混合型游戏中，部分智能体之间为合作关系，部分智能体或智能体的集合间为竞争关系。一般的非零和博弈且非纯合作型游戏为混合型游戏，举个简单的例子如囚徒困境（Prisoner’s
Dilemma）， 其奖励值表如
\hyperref[\detokenize{chapter_reinforcement_learning/marl_sys:tab-ch12-ch12-marl-prison}]{表\ref{\detokenize{chapter_reinforcement_learning/marl_sys:tab-ch12-ch12-marl-prison}}}所示。囚徒困境的两个玩家各有两个动作，沉默和背叛。可以用警察审查两名罪犯来理解，奖励值的绝对值即他们将被判处的年数。纯所有玩家的奖励值之和非常数，故其为非零和博弈型游戏。因此这一游戏不能被认为是纯竞争型或纯合作型游戏，因为当他们中的一方选择沉默一方选择背叛时，二者没有有效合作，而一方拿到了0的奖励，另一方为\sphinxhyphen{}3。而两者都选择沉默时是一种合作策略，各自拿到\sphinxhyphen{}1的奖励值。尽管这一策略看起来优于其他策略，但是这并不是这个游戏的纳什均衡策略，因为纳什均衡策略假设玩家间策略需要单独制定，无法形成联合策略分布。这实际上切断了玩家间的信息沟通和潜在合作的可能。因此，囚徒困境的纳什均衡策略是两个玩家都选择背叛对方。诸如此类的博弈论游戏类型，导致单智能体强化学习不能被直接用来优化多智能体系统中的各个智能体的策略。单智能体强化学习一般是找极值的过程，而多智能体系统求解纳什均衡策略往往是找极大\sphinxhyphen{}极小值即鞍点的过程，从优化的角度看这也是不同的。复杂的关系需要更普适的系统进行表达，这也对多智能体系统的构建提出了挑战。多智能体游戏类型也有许多其他的分类角度，如单轮进行的游戏、多轮进行的游戏、多智能体同时决策的、多智能体序贯决策等等，每一类不同的游戏都有相应不同的算法。而现有的多智能体系统往往针对单一类型游戏或者单一算法，缺少普适性多智能体强化学习系统，尤其是分布式的系统。

\end{itemize}


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{囚徒困境奖励值}\label{\detokenize{chapter_reinforcement_learning/marl_sys:id12}}\label{\detokenize{chapter_reinforcement_learning/marl_sys:tab-ch12-ch12-marl-prison}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline

\sphinxAtStartPar

&\sphinxstyletheadfamily 
\sphinxAtStartPar
沉默
&\sphinxstyletheadfamily 
\sphinxAtStartPar
背叛
\\
\hline
\sphinxAtStartPar
沉默
&
\sphinxAtStartPar
(\sphinxhyphen{}1,\sphinxhyphen{}1)
&
\sphinxAtStartPar
(\sphinxhyphen{}3,0)
\\
\hline
\sphinxAtStartPar
背叛
&
\sphinxAtStartPar
(0,\sphinxhyphen{}3)
&
\sphinxAtStartPar
(\sphinxhyphen{}2,\sphinxhyphen{}2)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{算法的异构}：从前面介绍的几个简单的多智能体算法，如自学习、虚构自学习等可以看出，多智能体算法有时由许多轮单智能体强化学习过程组成。而对不同的游戏类型，算法的类型也不相同。比如，对合作型游戏，许多算法是基于奖励分配（Credit
Assignment）的思想，如何将多个智能体获得的共同奖励合理分配给单个智能体是这类算法的核心。而这里面按照具体算法执行方式，也可以分为集成训练统一执行的（Centralized
Training Centralized Execution）、集成训练分别执行的（Centralized
Training Decentralized Execution）、分别训练并分别执行（Decentralized
Training Decentralized
Execution）的几类，来描述不同智能体训练过程和执行过程的统一性。对于竞争型游戏，往往采用各种计算纳什均衡的近似方法，如前面提到的虚构自学习、Double
Oracle、Mirror
Descent等等，将获取单个最优策略的单智能体强化学习过程看做一个“动作”，而对这些“动作”组成的元问题上进行纳什均衡近似。现有的算法在类似问题上有很大的差异性，使得构建一个统一的多智能体强化学习系统比较困难。

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{学习方法组合}：在前面提到的AlphaStar
\sphinxcite{chapter_reinforcement_learning/summary:vinyals2019grandmaster}等工作中，多智能体系统中优化得到一个好的策略往往不只需要强化学习算法，还需要其他学习方法如模仿学习等的辅助。比如从一些顶级人类玩家的游戏记录中形成有标签的训练样本，来预训练智能体。由于这些大规模游戏的复杂性，这往往是一个在训练前期快速提升智能体表现的有效方式。而对于整个学习系统而言，这就需要对不同学习范式进行结合，如合理地在模仿学习和强化学习之间进行切换等。这也使得大规模多智能体系统不单一是构建强化学习系统的问题，而需要许多其他学习机制和协调机制的配合实现。

\end{itemize}

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_reinforcement_learning/marl_sys:ch12-ch12-marl-sys}]{图\ref{\detokenize{chapter_reinforcement_learning/marl_sys:ch12-ch12-marl-sys}}}所示，为一个分布式多智能体强化学习系统。图中的两个智能体可以类似扩展到多个智能体。每个智能体包含多个行动者（Actor）用于采样和学习者（Learner）用于更新模型，这些行动者和学习者可以并行处理来加速训练过程，具体方法可以参考单智能体分布式系统章节介绍的A3C和IMPALA架构。训练好的模型被统一存储和管理在模型存储器中，是否对各个智能体的模型分别存储取决于各个智能体是否对称。存储器中的模型可以被模型评估器用来打分，从而为下一步模型选择器做准备。模型选择器根据模型评估器或者元学习者（如PSRO算法
\sphinxcite{chapter_reinforcement_learning/summary:lanctot2017unified}）以及均衡求解器等进行模型选择，并将选出的模型分发到各个智能体的行动者上。这一处理过程，我们称为联盟型管理（League\sphinxhyphen{}based
Management）。对于与环境交互的部分，分布式系统可以通过一个推理服务器（Inference
Server）对各个并行进程中的模型进行集中推理，将基于观察量（Observation）的动作（Action）发送给环境。环境部分也可以是并行的。推理服务器将采集到的交互轨迹发送给各个智能体进行模型训练。以上为一个分布式多智能体系统的例子，实际中根据不同的游戏类型和算法结构可能会有不同的设计。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ch12-marl-sys}.png}
\caption{分布式多智能体强化学习系统}\label{\detokenize{chapter_reinforcement_learning/marl_sys:id13}}\label{\detokenize{chapter_reinforcement_learning/marl_sys:ch12-ch12-marl-sys}}\end{figure}


\section{小结}
\label{\detokenize{chapter_reinforcement_learning/summary:id1}}\label{\detokenize{chapter_reinforcement_learning/summary::doc}}
\sphinxAtStartPar
在这一章，我们简单介绍了强化学习的基本概念，包括单智能体和多智能体强化学习算法、单节点和分布式强化学习系统等，给读者对强化学习问题的基本认识。当前，强化学习是一个快速发展的深度学习分支，许多实际问题都有可能通过强化学习算法的进一步发展得到解决。另一方面，由于强化学习问题设置的特殊性（如需要与环境交互进行采样等），也使得相应算法对计算系统的要求更高：如何更好地平衡样本采集和策略训练过程？如何均衡CPU和GPU等不同计算硬件的能力？如何在大规模分布式系统上有效部署强化学习智能体？等等，都需要对计算机系统的设计和使用有更好的理解。


\section{参考文献}
\label{\detokenize{chapter_reinforcement_learning/summary:id2}}
\sphinxAtStartPar



\chapter{可解释性AI系统}
\label{\detokenize{chapter_explainable_AI/index:ai}}\label{\detokenize{chapter_explainable_AI/index::doc}}
\sphinxAtStartPar
近10年来，籍由算力与数据规模的性价比突破临界点，以深度神经网络为代表的联结主义模型架构及统计学习范式（以后简称深度学习）在特征表征能力上取得了跨越级别的突破，大大推动了人工智能的发展，在很多场景中达到令人难以置信的效果。比如：人脸识别准确率达到97\%以上；谷歌智能语音助手回答正确率，在2019年的测试中达到92.9\%。在这些典型场景下，深度学习在智能表现上的性能已经超过了普通人类（甚至专家），从而到了撬动技术更替的临界点。在过去几年间，在某些商业逻辑对技术友好，或者伦理法规暂时稀缺的领域，如安防、实时调度、流程优化、竞技博弈、信息流分发等，人工智能和深度学习取得了技术和商业上快速突破。

\sphinxAtStartPar
食髓知味，技术发展的甜头自然每个领域都不愿放过。而当对深度学习的商业化运用来到某些对技术敏感、与人的生存或安全关系紧密的领域，如自动驾驶、金融、医疗和司法等高风险应用场景时，原有的商业逻辑在进行技术更替的过程中就会遇到阻力，从而导致商业化变现速度的减缓甚至失败。究其原因，以上场景的商业逻辑及背后伦理法规的中枢之一是稳定的、可追踪的责任明晰与责任分发；而深度学习得到的模型是个黑盒，我们无法从模型的结构或权重中获取模型行为的任何信息，从而使这些场景下责任追踪和分发的中枢无法复用，导致人工智能在业务应用中遇到技术上和结构上的困难。此外，模型的可解释性问题也引起了国家层面的关注，相关机构对此推出了相关的政策和法规。

\sphinxAtStartPar
因此，从商业推广层面以及从法规层面，我们都需要打开黑盒模型，对模型进行解释，可解释AI正是解决该类问题的技术。

\sphinxAtStartPar
本章的学习目标包括：
\begin{itemize}
\item {} 
\sphinxAtStartPar
掌握可解释AI的目标和应用场景

\item {} 
\sphinxAtStartPar
掌握常见的可解释AI方法类型及其对应的典型方法

\item {} 
\sphinxAtStartPar
思考可解释AI方法的未来发展

\end{itemize}


\section{背景}
\label{\detokenize{chapter_explainable_AI/explainable_ai:id1}}\label{\detokenize{chapter_explainable_AI/explainable_ai::doc}}
\sphinxAtStartPar
在人类历史上，技术进步、生产关系逻辑和伦理法规的发展是动态演进的。当一种新的技术在实验室获得突破后，其引发的价值产生方式的变化会依次对商品形态、生产关系等带来冲击。而同时当新技术带来的价值提升得到认可后，商业逻辑的组织形态在自发的调整过程中，也会对技术发展的路径、内容甚至速度提出诉求，并当诉求得到满足时适配以新型的伦理法规。在这样的相互作用中，技术系统与社会体系会共振完成演进，是谓技术革命。

\sphinxAtStartPar
近10年来，籍由算力与数据规模的性价比突破临界点，以深度神经网络为代表的联结主义模型架构及统计学习范式（以后简称深度学习）在特征表征能力上取得了跨越级别的突破，大大推动了人工智能的发展，在很多场景中达到令人难以置信的效果。比如：人脸识别准确率达到97\%以上；谷歌智能语音助手回答正确率，在2019年的测试中达到92.9\%。在这些典型场景下，深度学习在智能表现上的性能已经超过了普通人类（甚至专家），从而到了撬动技术更替的临界点。在过去几年间，在某些商业逻辑对技术友好，或者伦理法规暂时稀缺的领域，如安防、实时调度、流程优化、竞技博弈、信息流分发等，人工智能和深度学习取得了技术和商业上快速突破。

\sphinxAtStartPar
食髓知味，技术发展的甜头自然每个领域都不愿放过。而当对深度学习商业化运用来到某些对技术敏感、与人的生存或安全关系紧密的领域，如自动驾驶、金融、医疗和司法等高风险应用场景时，原有的商业逻辑在进行技术更替的过程中就会遇到阻力，从而导致商业化变现速度的减缓甚至失败。究其原因，以上场景的商业逻辑及背后伦理法规的中枢之一是稳定的、可追踪的责任明晰与责任分发；而深度学习得到的模型是个黑盒，我们无法从模型的结构或权重中获取模型行为的任何信息，从而使这些场景下责任追踪和分发的中枢无法复用，导致人工智能在业务应用中遇到技术上和结构上的困难。

\sphinxAtStartPar
举2个具体的例子：例1，在金融风控场景，通过深度学习模型识别出来小部分用户有欺诈嫌疑，但是业务部门不敢直接使用这个结果进行处理。因为人们难以理解结果是如何得到的，从而无法判断结果是否准确。而且该结果缺乏明确的依据，如果处理了，也无法向监管机构交代；
例2，在医疗领域，深度学习模型根据患者的检测数据，判断患者有肺结核，但是医生不知道诊断结果是怎么来的，不敢直接采用，而是根据自己的经验，仔细查看相关检测数据，然后给出自己的判断。从这2个例子可以看出，黑盒模型严重影响模型在实际场景的应用和推广。

\sphinxAtStartPar
此外，模型的可解释性问题也引起了国家层面的关注，相关机构对此推出了相关的政策和法规。
\begin{itemize}
\item {} 
\sphinxAtStartPar
2017年7月，国务院印发《新一代人工智能发展规划》，首次涵盖可解释AI。

\item {} 
\sphinxAtStartPar
2021年3月，中国人民银行发布金融行业标准《人工智能算法金融应用评价规范》，对金融行业AI模型可解释性提出了明确要求。

\item {} 
\sphinxAtStartPar
2021年8月，网信办《互联网信息服务算法推荐管理规定》,
提出对互联网行业算法推荐可解释性的要求。

\item {} 
\sphinxAtStartPar
2021年9月，科技部发布《新一代人工智能伦理规范》。

\end{itemize}

\sphinxAtStartPar
因此，从商业推广层面以及从法规层面，我们都需要打开黑盒模型，对模型进行解释，可解释AI正是解决该类问题的技术。


\section{可解释AI定义}
\label{\detokenize{chapter_explainable_AI/explainable_ai:ai}}
\sphinxAtStartPar
按DARPA（美国国防部先进研究项目局）的描述，如
\hyperref[\detokenize{chapter_explainable_AI/explainable_ai:xai-concept}]{图\ref{\detokenize{chapter_explainable_AI/explainable_ai:xai-concept}}}所示，
可解释AI的概念在于：区别于现有的AI系统，可解释AI系统可以解决用户面对黑盒模型时遇到的问题，使得用户知其然并知其所以然。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{img/ch11/xai_concept}.png}
\caption{可解释AI概念（图片来源于Broad Agency Announcement Explainable
Artificial Intelligence (XAI) DARPA\sphinxhyphen{}BAA\sphinxhyphen{}16–53）}\label{\detokenize{chapter_explainable_AI/explainable_ai:id23}}\label{\detokenize{chapter_explainable_AI/explainable_ai:xai-concept}}\end{figure}

\sphinxAtStartPar
然而，不论是学术界还是工业界，对于可解释AI (eXplainable
AI(XAI))都没有一个统一的定义。这里列举3种典型定义，供大家参考讨论：
\begin{itemize}
\item {} 
\sphinxAtStartPar
可解释性就是希望寻求对模型工作机理的直接理解，打破人工智能的黑盒子。

\item {} 
\sphinxAtStartPar
可解释AI是为AI算法所做出的决策提供人类可读的以及可理解的解释。

\item {} 
\sphinxAtStartPar
可解释AI是确保人类可以轻松理解和信任人工智能代理做出的决策的一组方法。

\end{itemize}

\sphinxAtStartPar
我们根据自身的实践经验和理解，将可解释AI定义为：一套面向机器学习（主要是深度神经网络）的技术合集，包括可视化、数据挖掘、逻辑推理、知识图谱等，目的是通过此技术合集，使深度神经网络呈现一定的可理解性，以满足相关使用者对模型及应用服务产生的信息诉求（如因果或背景信息），从而为使用者对人工智能服务建立认知层面的信任。


\section{可解释AI算法现状介绍}
\label{\detokenize{chapter_explainable_AI/explainable_ai:id2}}
\sphinxAtStartPar
随着可解释AI概念的提出，可解释AI越来越受到学术界及工业界的关注，下图展示了人工智能领域顶级学术会议中可解释AI关键字的趋势。为了让读者更好的对现有可解释AI算法有一个整体认知，我们这里参考
\sphinxcite{chapter_explainable_AI/explainable_ai:tkde-li}总结归纳了可解释AI的算法类型，如
\hyperref[\detokenize{chapter_explainable_AI/explainable_ai:xai-methods}]{图\ref{\detokenize{chapter_explainable_AI/explainable_ai:xai-methods}}}所示。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{XAI_methods}.PNG}
\caption{可解释AI（XAI）算法分支}\label{\detokenize{chapter_explainable_AI/explainable_ai:id24}}\label{\detokenize{chapter_explainable_AI/explainable_ai:xai-methods}}\end{figure}

\sphinxAtStartPar
对模型进行解释有多种多样的方法，这里依据解释过程是否引入数据集以外的外部知识，将其分为数据驱动的解释方法和知识感知的解释方法。

\sphinxAtStartPar
\sphinxstylestrong{数据驱动的解释}

\sphinxAtStartPar
数据驱动的解释是指纯粹从数据本身生成解释的方法，而不需要先验知识等外部信息。为了提供解释，数据驱动的方法通常从选择数据集（具有全局或局部分布）开始。然后，将选定的数据集或其变体输入到黑盒模型（在某些情况下，选取数据集不是所必需的。例如，
\sphinxcite{chapter_explainable_AI/explainable_ai:erhan2009visualizing}提出的最大激活值方法），通过对黑盒模型的相应预测进行一定的分析(例如，对预测w.r.t.输入特征进行求导）来生成解释。根据可解释性的范围，这些方法可以进一步分为全局方法或局部方法，即它们是解释所有数据点的全局模型行为还是预测子集行为。特别地，基于实例的方法提供了一种特殊类型的解释–它们直接返回数据实例作为解释。虽然从解释范围的分类来看，基于实例的方法也可以适合全局方法（代表性样本）或局部方法（反事实），但我们单独列出它们，以强调它们提供解释的特殊方式。

\sphinxAtStartPar
全局方法旨在提供对模型逻辑的理解以及所有预测的完整推理，基于对其特征、学习到的组件和结构的整体视图等等。有几个方向可以探索全局可解释性。为了便于理解，我们将它们分为以下三个子类：
(i)
模型提取——从原始黑盒模型中提取出一个可解释的模型，比如通过模型蒸馏的方式将原有黑盒模型蒸馏到可解释的决策树
\sphinxcite{chapter_explainable_AI/explainable_ai:frosst2017distilling}
\sphinxcite{chapter_explainable_AI/explainable_ai:zhang2019interpreting}，从而使用决策树中的规则解释该原始模型；
(ii) 基于特征的方法——估计特征的重要性或相关性，如
\hyperref[\detokenize{chapter_explainable_AI/explainable_ai:xai-global-feature-importance}]{图\ref{\detokenize{chapter_explainable_AI/explainable_ai:xai-global-feature-importance}}}所示,
该类型解释可提供如“信用逾期记录是模型依赖的最重要特征”的解释，从而协助判定模型是否存在偏见.
一种典型的全局特征解释方法是SHAP（其仅能针对树模型输出全局解释）:cite:\sphinxtitleref{lundberg2017unified}。
(iii)
透明模型设计——修改或重新设计黑盒模型以提高其可解释性。这类方法目前也逐渐成为探索热点，近期的相关工作包括ProtoPNet
\sphinxcite{chapter_explainable_AI/explainable_ai:chen2019looks}, Interpretable CNN
\sphinxcite{chapter_explainable_AI/explainable_ai:zhang2018interpretable}, ProtoTree
\sphinxcite{chapter_explainable_AI/explainable_ai:nauta2021neural}等。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{xai_global_feature_importance}.png}
\caption{全局特征重要性解释}\label{\detokenize{chapter_explainable_AI/explainable_ai:id25}}\label{\detokenize{chapter_explainable_AI/explainable_ai:xai-global-feature-importance}}\end{figure}

\sphinxAtStartPar
全局解释可以提供黑盒模型的整体认知。但由于黑盒模型的高复杂性，在实践中往往很难通过模型提取/设计得到与原模型行为相近的简单透明模型，也往往很难对整个数据集抽象出统一的特征重要性。此外，在为单个观察生成解释时，全局解释也缺乏局部保真度，因为全局重要的特征可能无法准确解释单个样例的决定。因此，局部方法成为了近些年领域内重要的研究方向。局部方法尝试为单个实例或一组实例检验模型行为的合理性。当仅关注局部行为时，复杂模型也可以变得简单，因此即使是简单的函数也有可以为局部区域提供可信度高的解释。基于获得解释的过程，局部方法可以分为两类：局部近似和基于传播的方法。

\sphinxAtStartPar
局部近似是通过在样本近邻区域模拟黑盒模型的行为生成可理解的子模型。相比于全局方法中的模型提取，局部近似仅需关注样本临近区域，因此更容易获得精确描述局部行为的子模型。如
\hyperref[\detokenize{chapter_explainable_AI/explainable_ai:xai-lime}]{图\ref{\detokenize{chapter_explainable_AI/explainable_ai:xai-lime}}}所示，通过在关注数据点\(x\)附近生成\(m\)个数据点\((x_i^\prime, f(x_i^\prime)), for\  i=1,2, ...m\)（这里\(f\)为黑盒模型决策函数）,用线性拟合这些数据点，可以得到一个线性模型\(g=\sum_i^k w_ix^i\)，这里\(k\)表示数据的特征维度。那么线性模型中的权重\(w_i\)即可用于表示数据\(x\)中第\(i\)个特征对于模型\(f\)的重要性。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{xai_lime}.png}
\caption{局部近似方法示例}\label{\detokenize{chapter_explainable_AI/explainable_ai:id26}}\label{\detokenize{chapter_explainable_AI/explainable_ai:xai-lime}}\end{figure}

\sphinxAtStartPar
基于传播的方法通常是传播某些信息直接定位相关特征，这些方法包含了基于反向传播的方法和基于前向传播的方法。基于反向传播的方法通过梯度回传将输出的贡献归因于输入特征。如
\hyperref[\detokenize{chapter_explainable_AI/explainable_ai:xai-gradient-based}]{图\ref{\detokenize{chapter_explainable_AI/explainable_ai:xai-gradient-based}}}所示,通过梯度回传，计算模型输出对输入的梯度\(\frac{d(f(x))}{dx}\)
作为模型解释。常见的基于梯度传播的方法有基本Gradient方法，GuidedBackprop
\sphinxcite{chapter_explainable_AI/explainable_ai:zeiler2014visualizing}, GradCAM
\sphinxcite{chapter_explainable_AI/explainable_ai:selvaraju2017grad}等. 而基于前向传播的方法通过扰动特征后,
进行前向推理的输出差异来量化输出与特征的相关性。其中，常见的几种方法有RISE
\sphinxcite{chapter_explainable_AI/explainable_ai:petsiuk2018rise}，ScoreCAM \sphinxcite{chapter_explainable_AI/explainable_ai:wang2020score}等。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{xai_gradient_based}.PNG}
\caption{局部近似方法示例}\label{\detokenize{chapter_explainable_AI/explainable_ai:id27}}\label{\detokenize{chapter_explainable_AI/explainable_ai:xai-gradient-based}}\end{figure}

\sphinxAtStartPar
\sphinxstylestrong{知识感知的解释}

\sphinxAtStartPar
数据驱动的解释方法能够从数据集或输入和输出之间的关系提供全面的解释。在此基础上，还可以利用外部知识来丰富解释并使其更加人性化。没有机器学习背景知识的门外汉可能很难直接理解特征的重要性，以及特征和目标之间的联系。借助外部领域知识，我们不仅可以生成表明特征重要性的解释，还可以描述某些特征比其他特征更重要的原因。因此，在过去几年中，基于知识感知的可解释AI方法引起了越来越多的关注。与从多种情景中收集的原始数据集相比，知识通常被视为人类根据生活经验或严格的理论推理得出的实体或关系。一般来说，知识可以有多种形式。它可以保留在人的头脑中，也可以用自然语言、音频或规则记录，具有严格的逻辑。为了对这些方法进行系统回顾，我们在此根据知识来源将它们分为两类：通用知识方法和知识库（KB）方法。前者以非结构化数据为知识源来构建解释，后者以结构化知识库为基础来构建解释。

\sphinxAtStartPar
提供知识的一个相对直接的方法是通过人类的参与。事实上，随着人工智能研究和应用的爆炸式增长，人类在人工智能系统中的关键作用已经慢慢显现。这样的系统被称为以人为中心的人工智能系统。
\sphinxcite{chapter_explainable_AI/explainable_ai:riedl2019human}认为，以人为中心的人工智能不仅能让人工智能系统从社会文化的角度更好地了解人类，还能让人工智能系统帮助人类了解自己。为了实现这些目标，人工智能需要满足可解释性和透明度等几个属性。

\sphinxAtStartPar
具体来说，人类能够通过提供相当多的人类定义的概念来在人工智能系统中发挥作用。
\DUrole{bibtex}{{[}kim2018interpretability{]}}利用概念激活向量（CAV）来测试概念在分类任务中的重要性（TCAV）。CAV是与感兴趣目标概念的激活与否决策边界垂直的矢量，该矢量可以这样获取:
输入目标概念的正负样本, 进行线性回归, 得到决策边界,
从而得到CAV。以“斑马”的“条纹”概念为例，用户首先收集包含有“条纹”的数据样本及不含“条纹”的数据样本，输入到网络中，获取中间层的激活值，基于正负样本的标签（\(1\)代表含有概念，\(0\)代表不含概念）对中间层激活值进行拟合，获取决策边界，CAV即为该决策边界的垂直向量。

\sphinxAtStartPar
如
\hyperref[\detokenize{chapter_explainable_AI/explainable_ai:xai-tcav}]{图\ref{\detokenize{chapter_explainable_AI/explainable_ai:xai-tcav}}}所示，为了计算TCAV评分，代表第\(l\)层概念对类\(k\)预测的重要性的“概念敏感度”可以首先计算为方向导数\(S_{C,k,l}(\mathbf{x})\)：
\begin{equation}\label{equation:chapter_explainable_AI/explainable_ai:chapter_explainable_AI/explainable_ai:0}
\begin{split}\begin{split}
S_{C,k,l}(\mathbf{x}) =  &\lim_{\epsilon\rightarrow 0}\frac{h_{l,k}(f_{l}(\mathbf{x})+\epsilon \mathbf{v}^{l}_{C})-h_{l,k}(f_{l}(\mathbf{x}))}{\epsilon} \\ = &\nabla h_{l,k}(f_{l}(\mathbf{x})) \cdot \mathbf{v}^{l}_{C}
\end{split}
\label{eq:TCAV_score}\end{split}
\end{equation}
\sphinxAtStartPar
其中\(f_{l}(\mathbf{x})\)是在第\(l\)、\(h_{l,k}(\cdot)\)是类\(k\)的logit,\(\nabla h_{l,k}(\cdot)\)是\(h_{l，k}\)
w.r.t层\(l\)的激活的梯度。\(\mathbf{v}^{l}_{C}\)是用户旨在探索的概念\(C\)的CAV。正（或负）敏感性表明概念\(C\)对输入的激活有正（或负）影响。

\sphinxAtStartPar
基于\(S_{C,k,l}\),
TCAV就可以通过计算类\(k\)的具有正\(S_{C,k,l}\)’s的样本的比率来获得：
\begin{equation}\label{equation:chapter_explainable_AI/explainable_ai:chapter_explainable_AI/explainable_ai:1}
\begin{split}\textbf{TCAV}_{Q_{C,k,l}}=\frac{\vert \{\mathbf{x}\in X_{k}:S_{C,k,l}(\mathbf{x})>0\}\vert}{\vert X_{k}\vert}
\label{eq:TCAV}\end{split}
\end{equation}
\sphinxAtStartPar
结合\(t\)\sphinxhyphen{}分布假设方法，如果\(\textbf{TCAV}_{Q_{C,k,l}}\)大于0.5，则表明概念\(C\)对类\(k\)有重大影响。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{xai_tcav}.png}
\caption{TCAV流程(图片来源于 \sphinxcite{chapter_explainable_AI/explainable_ai:tkde-li})}\label{\detokenize{chapter_explainable_AI/explainable_ai:id28}}\label{\detokenize{chapter_explainable_AI/explainable_ai:xai-tcav}}\end{figure}

\sphinxAtStartPar
人类的知识可以是主观的，而KB可以是客观的。在当前研究中，KB通常被建模为知识图谱(KG)。以下以MindSpore支持的可解释推荐模型TB\sphinxhyphen{}Net为例，讲解如何使用知识图谱构建可解释模型。知识图谱可以捕捉实体之间丰富的语义关系。TB\sphinxhyphen{}Net的目的之一就是确定哪一对实体（即，物品\sphinxhyphen{}物品）对用户产生最重大的影响，并通过什么关系和关键节点进行关联。不同于现有的基于KG嵌入的方法（RippleNet使用KG补全方法预测用户与物品之间的路径），TB\sphinxhyphen{}Net提取真实路径，以达到推荐结果的高准确性和优越的可解释性。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{tb_net}.png}
\caption{TB\sphinxhyphen{}Net网络训练框架}\label{\detokenize{chapter_explainable_AI/explainable_ai:id29}}\label{\detokenize{chapter_explainable_AI/explainable_ai:tb-net}}\end{figure}

\sphinxAtStartPar
TB\sphinxhyphen{}Net的框架如
\hyperref[\detokenize{chapter_explainable_AI/explainable_ai:tb-net}]{图\ref{\detokenize{chapter_explainable_AI/explainable_ai:tb-net}}}所示：其中，\(i_c\)代表待推荐物品，\(h_n\)代表历史记录中用户交互的物品，\(r\)和\(e\)代表图谱中的关系（relation）和实体（entity），它们的向量化表达拼接在一起形成关系矩阵和实体矩阵。首先，TB\sphinxhyphen{}Net通过\(i_c\)和\(h_n\)的相同特征值来构建用户\(u\)的子图谱，每一对\(i_c\)和\(h_n\)都由关系和实体所组成的路径来连接。然后，TB\sphinxhyphen{}Net的路径双向传导方法将物品、实体和关系向量的计算从路径的左侧和右侧分别传播到中间节点，即计算左右两个流向的向量汇集到同一中间实体的概率。该概率用于表示用户对中间实体的喜好程度，并作为解释的依据。最后，TB\sphinxhyphen{}Net识别子图谱中关键路径（即关键实体和关系），输出推荐结果和具有语义级别的解释。

\sphinxAtStartPar
以游戏推荐为场景，随机对一个用户推荐新的游戏，如
\hyperref[\detokenize{chapter_explainable_AI/explainable_ai:xai-kg-recommendation}]{图\ref{\detokenize{chapter_explainable_AI/explainable_ai:xai-kg-recommendation}}}所示，其中Half\sphinxhyphen{}Life, DOTA 2, Team
Fortress 2等为游戏名称。关系属性中，game.year
代表游戏发行年份，game.genres代表游戏属性，game.developer代表游戏的开发商，game.categories代表游戏分类。属性节点中，MOBA代表多人在线战术竞技游戏，Valve代表威尔乌游戏公司，Action代表动作类，Multi\sphinxhyphen{}player代表多人游戏，Valve
Anti\sphinxhyphen{}Cheat
enabled代表威尔乌防作弊类，Free代表免费，Cross\sphinxhyphen{}Platform代表跨平台。右边的游戏是用户历史记录中玩过的游戏。而测试数据中正确推荐的游戏是“Team
Fortress 2”。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{xai_kg_recommendation}.png}
\caption{Steam游戏推荐可解释示例 （用户玩过的游戏: Half\sphinxhyphen{}Life, DOAT 2.
推荐命中的游戏: “Team Fortress 2”。具有属性信息的节点如，game.geners:
Action, free\sphinxhyphen{}to\sphinxhyphen{}play; game.developer: Valve; game.categories:
Multiplayer, MOBA.）}\label{\detokenize{chapter_explainable_AI/explainable_ai:id30}}\label{\detokenize{chapter_explainable_AI/explainable_ai:xai-kg-recommendation}}\end{figure}

\sphinxAtStartPar
在
\hyperref[\detokenize{chapter_explainable_AI/explainable_ai:xai-kg-recommendation}]{图\ref{\detokenize{chapter_explainable_AI/explainable_ai:xai-kg-recommendation}}}中，有两个突出显示的相关概率（38.6\%,
21.1\%），它们是在推荐过程中模型计算的关键路径被激活的概率。红色箭头突出显示从“Team
Fortress
2”到历史项目“Half\sphinxhyphen{}Life”之间的关键路径。它表明TB\sphinxhyphen{}Net能够通过各种关系连接向用户推荐物品，并找出关键路径作为解释。因此，将“Team
Fortress 2”推荐给用户的解释可以翻译成固定话术：“Team Fortress
2”是游戏公司“Valve”开发的一款动作类、多人在线、射击类电子游戏。这与用户历史玩过的游戏“Half\sphinxhyphen{}Life”有高度关联。


\section{可解释AI系统及实践}
\label{\detokenize{chapter_explainable_AI/explainable_ai:id17}}
\sphinxAtStartPar
随着各领域对可解释的诉求快速增长，越来越多企业集成可解释AI工具包，为广大用户提供快速便捷的可解释实践，业界现有的主流工具包有:
\sphinxhyphen{} TensorFlow团队的What\sphinxhyphen{}if
Tool，用户不需编写任何程序代码就能探索学习模型，让非开发人员也能参与模型调校工作。
\sphinxhyphen{}
IBM的AIX360，提供了多种的解释及度量方法去评估模型在各个不同维度上的可解释及可信性能。
\sphinxhyphen{} Facebook
Torch团队的captum，针对图像及文本场景，提供了多种主流解释方法。 \sphinxhyphen{}

\phantomsection\label{\detokenize{chapter_explainable_AI/explainable_ai:mindspore-xai}}
\sphinxAtStartPar
微软的InterpretML，用户可以训练不同的白盒模型及解释黑盒模型。 \sphinxhyphen{}
SeldonIO的Alibi，专注于查勘模型内部状况及决策解释，提供各种白盒、黑盒模型、单样本及全局解释方法的实现。
\sphinxhyphen{}
华为MindSpore的XAI工具，提供数据工具、解释方法、白盒模型以及度量方法，为用户提供不同级别的解释（局部，全局，语义级别等）。

\sphinxAtStartPar
本节将以MindSpore
XAI工具为例，讲解在实践中如何使用可解释AI工具为图片分类模型和表格数据分类模型提供解释，从而协助用户理解模型进行进一步的调试调优。
MindSpore
XAI工具的架构如下，其为基于MindSpore深度学习框架的一个可解释工具，可在Ascend及GPU设备上部署。
\begin{center}\sphinxincludegraphics{{mindspore_xai}.png}\end{center}
\begin{quote}
\begin{quote}\begin{description}
\item[{width}] \leavevmode
\sphinxAtStartPar
800px

\end{description}\end{quote}
\end{quote}

\sphinxAtStartPar
要使用MindSpore可解释AI，读者首先要通过pip安装MindSpore
XAI包（支持MindSpore1.7 或以上，GPU及Ascend
处理器，推荐配合JupyterLab使用）:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pip install mindspore\PYGZhy{}xai
\end{sphinxVerbatim}

\sphinxAtStartPar
在MindSpore
XAI的\sphinxhref{https://www.mindspore.cn/xai/docs/zh-CN/r1.8/index.html}{官网教程}%
\begin{footnote}[85]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.mindspore.cn/xai/docs/zh-CN/r1.8/index.html}
%
\end{footnote}中，详细介绍了如何安装和使用提供的解释方法,
读者可自行查阅。


\subsection{MindSpore XAI工具为图片分类场景提供解释}
\label{\detokenize{chapter_explainable_AI/explainable_ai:id18}}
\sphinxAtStartPar
下面结合MindSpore XAI1.8版本中已支持的显着图可视方法 GradCAM
作为一个代码演示例子。读者可参阅\sphinxhref{https://www.mindspore.cn/xai/docs/zh-CN/1.8/using\_cv\_explainers.html}{官方教程}%
\begin{footnote}[86]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.mindspore.cn/xai/docs/zh-CN/1.8/using\_cv\_explainers.html}
%
\end{footnote}以取得演示用的数据集,
模型和完整脚本代码。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{mindspore\PYGZus{}xai}\PYG{n+nn}{.}\PYG{n+nn}{explainer} \PYG{k+kn}{import} \PYG{n}{GradCAM}

\PYG{c+c1}{\PYGZsh{} 通常指定最后一层的卷积层}
\PYG{n}{grad\PYGZus{}cam} \PYG{o}{=} \PYG{n}{GradCAM}\PYG{p}{(}\PYG{n}{net}\PYG{p}{,} \PYG{n}{layer}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{layer4}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} 3 是\PYGZsq{}boat\PYGZsq{}类的ID}
\PYG{n}{saliency} \PYG{o}{=} \PYG{n}{grad\PYGZus{}cam}\PYG{p}{(}\PYG{n}{boat\PYGZus{}image}\PYG{p}{,} \PYG{n}{targets}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
如果输入的是一个维度为 \(1*3*224*224\)
的图片Tensor，那返回的saliency就是一个 \(1*1*224*224\)
的显著图Tensor。下面我们将几个例子展示如何使用可解释AI能力来更好理解图片分类模型的预测结果，获取作为分类预测依据的关键特征区域，从而判断得到分类结果的合理性和正确性，加速模型调优。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=400\sphinxpxdimen]{{correct_correct}.png}
\caption{预测结果正确，依据的关键特征合理的例子}\label{\detokenize{chapter_explainable_AI/explainable_ai:id31}}\label{\detokenize{chapter_explainable_AI/explainable_ai:correct-correct}}\end{figure}

\sphinxAtStartPar
上图预测标签是“bicycle”，解释结果给出依据的关键特征
在车轮上，说明这个分类判断依据是合理的, 可以初步判定模型为可信的。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=400\sphinxpxdimen]{{correct_wrong}.png}
\caption{预测结果正确，依据的关键特征不合理的例子}\label{\detokenize{chapter_explainable_AI/explainable_ai:id32}}\label{\detokenize{chapter_explainable_AI/explainable_ai:correct-wrong}}\end{figure}

\sphinxAtStartPar
上图在预测标签中有1个标签是“person”，这个结果是对的；但是解释的时候，高亮区域在马头的上，那么这个关键特征依据很可能是错误的,
这个模型的可靠性还需进一步验证。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=400\sphinxpxdimen]{{wrong_wrong}.png}
\caption{预测结果错误，依据的关键特征不合理的例子}\label{\detokenize{chapter_explainable_AI/explainable_ai:id33}}\label{\detokenize{chapter_explainable_AI/explainable_ai:wrong-wrong}}\end{figure}

\sphinxAtStartPar
在上图中，预测标签为“boat”，但是原始图像中并没有船只存在，通过图中右侧解释结果可以看到模型将水面作为分类的关键依据，得到预测结果“boat”，这个依据是错误的。通过对训练数据集中标签为“boat”的数据子集进行分析，发现绝大部分标签为“boat”的图片中，都有水面，这很可能导致模型训练的时候，误将水面作为“boat”类型的关键依据。基于此，按比例补充有船没有水面的图片集，从而大幅消减模型学习的时候误判关键特征的概率。


\subsection{MindSpore XAI工具为表格分类场景提供解释}
\label{\detokenize{chapter_explainable_AI/explainable_ai:tabular-lime}}\label{\detokenize{chapter_explainable_AI/explainable_ai:id19}}
\sphinxAtStartPar
MindSpore XAI
1.8版本支持了三个业界比较常见的表格数据模型解释方法：LIMETabular、SHAPKernel和SHAPGradient。

\sphinxAtStartPar
以LIMETabular为例针对一个复杂难解释的模型，提供一个局部可解释的模型来对单个样本进行解释：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{mindspore\PYGZus{}xai}\PYG{n+nn}{.}\PYG{n+nn}{explainer} \PYG{k+kn}{import} \PYG{n}{LIMETabular}

\PYG{c+c1}{\PYGZsh{} 将特征转换为特征统计数据}
\PYG{n}{feature\PYGZus{}stats} \PYG{o}{=} \PYG{n}{LIMETabular}\PYG{o}{.}\PYG{n}{to\PYGZus{}feat\PYGZus{}stats}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{feature\PYGZus{}names}\PYG{o}{=}\PYG{n}{feature\PYGZus{}names}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} 初始化解释器}
\PYG{n}{lime} \PYG{o}{=} \PYG{n}{LIMETabular}\PYG{p}{(}\PYG{n}{net}\PYG{p}{,} \PYG{n}{feature\PYGZus{}stats}\PYG{p}{,} \PYG{n}{feature\PYGZus{}names}\PYG{o}{=}\PYG{n}{feature\PYGZus{}names}\PYG{p}{,} \PYG{n}{class\PYGZus{}names}\PYG{o}{=}\PYG{n}{class\PYGZus{}names}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} 解释}
\PYG{n}{lime\PYGZus{}outputs} \PYG{o}{=} \PYG{n}{lime}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{,} \PYG{n}{targets}\PYG{p}{,} \PYG{n}{show}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
解释器会显示出把该样本分类为setosa这一决定的决策边界，返回的
lime\_outputs 是代表决策边界的一个结构数据。 可视化解释,可得到
\begin{center}\sphinxincludegraphics{{tabular}.png}\end{center}
\begin{quote}
\begin{quote}\begin{description}
\item[{width}] \leavevmode
\sphinxAtStartPar
400px

\end{description}\end{quote}
\end{quote}

\sphinxAtStartPar
上述解释说明针对setosa这一决策,最为重要的特征为petal length。


\subsection{MindSpore XAI工具提供白盒模型}
\label{\detokenize{chapter_explainable_AI/explainable_ai:id20}}
\sphinxAtStartPar
除了针对黑盒模型的事后解释方法,XAI工具同样提供业界领先的白盒模型,使得用户可基于这些白盒模型进行训练,在推理过程中模型可同时输出推理结果及解释结果。以TB\sphinxhyphen{}Net为例(可参考:numref:\sphinxtitleref{tb\_net}及其\sphinxhref{https://e.gitee.com/mind\_spore/repos/mindspore/xai/tree/master/models/whitebox/tbnet}{官网教程}%
\begin{footnote}[87]\sphinxAtStartFootnote
\sphinxnolinkurl{https://e.gitee.com/mind\_spore/repos/mindspore/xai/tree/master/models/whitebox/tbnet}
%
\end{footnote}进行使用)，该方法已上线商用，为百万级客户提供带有语义级解释的理财产品推荐服务。TB\sphinxhyphen{}Net利用知识图谱对理财产品的属性和客户的历史数据进行建模。在图谱中，具有共同属性值的理财产品会被连接起来，待推荐产品与客户的历史购买或浏览的产品会通过共同的属性值连接成路径，构成该客户的子图谱。然后，TB\sphinxhyphen{}Net对图谱中的路径进行双向传导计算，从而识别关键产品和关键路径，作为推荐和解释的依据。

\sphinxAtStartPar
一个可解释推荐的例子如下：在历史数据中，该客户近期曾购买或浏览了理财产品A、B和N等等。通过TB\sphinxhyphen{}Net的路径双向传导计算可知，路径（产品P，年化利率\_中等偏高，产品A）和路径（产品P，风险等级\_中等风险，产品N）的权重较高，即为关键路径。此时，TB\sphinxhyphen{}Net输出的解释为：“推荐理财产品P给该客户，是因为它的年化利率\_中等偏高，风险等级\_中等风险，分别与该客户近期购买或浏览的理财产品A和B一致。”

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{tbnet_finance}.png}
\caption{TBNet应用金融理财场景}\label{\detokenize{chapter_explainable_AI/explainable_ai:id34}}\label{\detokenize{chapter_explainable_AI/explainable_ai:tbnet-finance}}\end{figure}

\sphinxAtStartPar
除了上面介绍的解释方法外，MindSpore
XAI还会提供一系列的度量方法用以评估不同解释方法的优劣，另外也会陆续增加自带解释的白盒模型，用户可直接取用成熟的模型架构以快速构建自己的可解释AI系统。


\section{未来可解释AI}
\label{\detokenize{chapter_explainable_AI/explainable_ai:id21}}
\sphinxAtStartPar
为了进一步推动可解释AI的研究，我们在此总结了一些值得注意的研究方向。

\sphinxAtStartPar
首先，知识感知型XAI仍有很大的研究扩展空间。然而，要有效地利用外部知识，仍有许多悬而未决的问题。其中一个问题是如何在如此广阔的知识空间中获取或检索有用的知识。例如,
维基百科上记载了各式各样各领域相关的知识, 但如果要解决医学图像分类问题,
维基百科上大部分词条都是无关或存在噪音的,
这样便很难准确地寻找到合适的知识引入到XAI系统中。

\sphinxAtStartPar
此外，XAI系统的部署也非常需要一个更加标准和更加统一的评估框架。为了构建标准统一的评估框架，我们可能需要同时利用不同的指标，相互补充。不同的指标可能适用于不同的任务和用户。统一的评价框架应具有相应的灵活性。

\sphinxAtStartPar
最后，我们相信跨学科合作将是有益的。XAI的发展不仅需要计算机科学家来开发先进的算法，还需要物理学家、生物学家和认知科学家来揭开人类认知的奥秘，以及特定领域的专家来贡献他们的领域知识。


\section{参考文献}
\label{\detokenize{chapter_explainable_AI/explainable_ai:id22}}
\sphinxAtStartPar



\chapter{机器人系统}
\label{\detokenize{chapter_rl_sys/index:id1}}\label{\detokenize{chapter_rl_sys/index::doc}}
\sphinxAtStartPar
在本章中，我们介绍机器学习的一个重要分支——机器人及其在系统方面的知识。本章的学习目标包括：
\begin{itemize}
\item {} 
\sphinxAtStartPar
掌握机器人系统基本知识。

\item {} 
\sphinxAtStartPar
掌握通用机器人操作系统。

\item {} 
\sphinxAtStartPar
掌握感知系统、规划系统、控制系统。

\end{itemize}


\section{概述}
\label{\detokenize{chapter_rl_sys/rl_sys_intro:id1}}\label{\detokenize{chapter_rl_sys/rl_sys_intro::doc}}
\sphinxAtStartPar
机器人学是一个交叉学科，它涉及了计算机科学、机械工程、电气工程、生物医学工程、数学等多种学科，并有诸多应用，比如自动驾驶汽车、机械臂、无人机、医疗机器人等。机器人能够自主地完成一种或多种任务或者辅助人类完成指定任务。通常，人们把机器人系统划分为感知系统、决策（规划）和控制系统等组成部分。

\sphinxAtStartPar
近些年，随着机器学习的兴起，经典机器人技术出现和机器学习技术结合的趋势，称为机器人学习（Robot
Learning）。机器人学习包含了计算机视觉、自然语言处理、语音处理、强化学习和模仿学习等人工智能技术在机器人上的应用，让机器人通过学习，自主地执行各种决策控制任务。

\sphinxAtStartPar
机器人系统按照涉及的机器人数量，可以划分为单机器人系统和多机器人系统。多机器人系统协作和沟通中涉及的安全和隐私问题，也会是一个值得研究的方向。最近机器人系统在室内自主移动
\sphinxcite{chapter_rl_sys/summary:id4,chapter_rl_sys/summary:huang2018navigationnet}，道路自动驾驶
\sphinxcite{chapter_rl_sys/summary:pmlr-v155-huang21a,chapter_rl_sys/summary:pmlr-v155-sun21a,chapter_rl_sys/summary:sun2022selfsupervisedta}，机械臂工业操作等行业场景得到充分应用和发展。一些机器人基础设施项目也在进行中，如具备从公开可用的互联网资源、计算机模拟和
真实机器人试验中学习能力的大规模的计算系统RobotBrain。在自动驾驶领域，受联网的自动驾驶汽车
(CAV) 对传统交通运输行业的影响，“车辆计算”(Vehicle Computing) (如
\hyperref[\detokenize{chapter_rl_sys/rl_sys_intro:vehicle-computing}]{图\ref{\detokenize{chapter_rl_sys/rl_sys_intro:vehicle-computing}}})概念引起广泛关注，并激发了如何让计算能力有限使用周围的CAV计算平台来执行复杂的计算任务的研究。最近，有很多自动驾驶系统的模拟器，代表性的比如CARLA，支持安全RL、MARL、真实地图数据导入、泛化性测试等任务的MetaDrive
\sphinxcite{chapter_rl_sys/summary:li2021metadrive}，还有CarSim和
TruckSim，它们可以作为各种自动驾驶算法的训练场并对算法效果进行评估。另外针对自动驾驶的系统开发平台也不断涌现，如ERDOS,
D3 (Dynamic
Deadline\sphinxhyphen{}Driven)和强调模块化思想的Pylot，可以让模型训练与部署系统与这些平台对接。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{vehicle_computing}.png}
\caption{车辆计算框架图 \sphinxcite{chapter_rl_sys/summary:id3}}\label{\detokenize{chapter_rl_sys/rl_sys_intro:id9}}\label{\detokenize{chapter_rl_sys/rl_sys_intro:vehicle-computing}}\end{figure}

\sphinxAtStartPar
\hyperref[\detokenize{chapter_rl_sys/rl_sys_intro:learning-decision-module}]{图\ref{\detokenize{chapter_rl_sys/rl_sys_intro:learning-decision-module}}}是一个典型的感知、规划、控制的模块化设计的自动驾驶系统框架图，接下来，我们也将按照这个顺序依次介绍通用框架、感知系统、规划系统和控制系统。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{idm}.png}
\caption{通过模仿学习进行自动驾驶框架图。
绿线表示自主驾驶系统的模块化流程。橙色实线表示神经判别器的训练。而橙色虚线表示规划和控制模块是不可微的。但是决策策略可以通过判别器对控制行动的奖励，重新参数化技术进行训练，如蓝色虚线所示。}\label{\detokenize{chapter_rl_sys/rl_sys_intro:id10}}\label{\detokenize{chapter_rl_sys/rl_sys_intro:learning-decision-module}}\end{figure}


\section{通用机器人操作系统}
\label{\detokenize{chapter_rl_sys/ros:id1}}\label{\detokenize{chapter_rl_sys/ros::doc}}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ROS2_arch}.png}
\caption{ROS/ROS2架构概述 \sphinxcite{chapter_rl_sys/summary:maruyama2016exploring}}\label{\detokenize{chapter_rl_sys/ros:id6}}\label{\detokenize{chapter_rl_sys/ros:ros2-arch}}\end{figure}

\sphinxAtStartPar
机器人操作系统(ROS)起源于斯坦福大学人工智能实验室的一个机器人项目。它是一个自由、开源的框架，提供接口、工具来构建先进的机器人。由于机器人领域的快速发展和复杂化，代码复用和模块化的需求日益强烈，ROS适用于机器人这种多节点多任务的复杂场景。目前也有一些机器人、无人机甚至无人车都开始采用ROS作为开发平台。在机器人学习方面，ROS/ROS2可以与深度学习结合，有开发人员为ROS/ROS2开发了的深度学习节点，并支持NVIDIA
Jetson和TensorRT。NVIDIA
Jetson是NVIDIA为自主机器开发的一个嵌入式系统，包括CPU、GPU、PMIC、DRAM
和闪存的一个模组化系统，可以将自主机器软件运作系统运行速率提升。TensorRT
是由 Nvidia 发布的机器学习框架，用于在其硬件上运行机器学习推理。

\sphinxAtStartPar
作为一个适用于机器人编程的框架，ROS把原本松散的零部件耦合在了一起，为他们提供了通信架构。虽然叫做“操作系统”，ROS更像是一个中间件，给各种基于ROS的应用程序建立起了沟通的桥梁，通过这个中间件，机器人的感知、决策、控制算法可以组织和运行。ROS采用了分布式的设计思想，支持C++、Pyhton等多种编程语言，方便移植。对ROS来讲，最小的进程单元是节点，由节点管理器来管理。参数配置存储在参数服务器中。ROS的通信方式包含：主题（Topic）、服务（Service）、参数服务器（Parameter
Server）、动作库（ActionLib）这四种。

\sphinxAtStartPar
ROS提供了很多内置工具，比如三维可视化器rviz，用于可视化机器人、它们工作的环境和传感器数据。它是一个高度可配置的工具，具有许多不同类型的可视化和插件。catkin是ROS
构建系统（类似于Linux下的CMake），Catkin
Workspace是创建、修改、编译catkin软件包的目录。roslaunch可用于在本地和远程启动多个ROS
节点以及在ROS参数服务器上设置参数的工具。此外还有机器人仿真工具Gazebo和移动操作软件和规划框架MoveIt!。ROS为机器人开发者提供了不同编程语言的接口，比如C++语言ROS接口roscpp，python语言的ROS接口rospy。ROS中提供了许多机器人的统一机器人描述格式URDF（Unified
Robot Description
Format）文件，URDF使用XML格式描述机器人文件。ROS也有一些需要提高的地方，比如它的通信实时性能有限，与工业级要求的系统稳定性还有一定差距。

\sphinxAtStartPar
ROS2项目在ROSCon 2014上被宣布，第一个ROS2发行版 Ardent Apalone
是于2017年发布。ROS2增加了对多机器人系统的支持，提高了多机器人之间通信的网络性能，而且支持微控制器和跨系统平台，不仅可以运行在现有的X86和ARM系统上，还将支持MCU等嵌入式微控制器，不止能运行在Linux系统之上，还增加了对Windows、MacOS、RTOS等系统的支持。更重要的是，ROS
2还加入了实时控制的支持，可以提高控制的时效性和整体机器人的性能。ROS
2的通信系统基于DDS（Data Distribution Service），即数据分发服务,如
\hyperref[\detokenize{chapter_rl_sys/ros:ros2-arch}]{图\ref{\detokenize{chapter_rl_sys/ros:ros2-arch}}}所示。

\sphinxAtStartPar
ROS2依赖于使用shell环境组合工作区。“工作区”（Workspace）是一个ROS术语，表示使用ROS2进行开发的系统位置。核心ROS2
工作区称为Underlay。随后的工作区称为Overlays。使用ROS2
进行开发时，通常会同时有多个工作区处于活动状态。


\subsection{ROS还是ROS2}
\label{\detokenize{chapter_rl_sys/ros:rosros2}}
\sphinxAtStartPar
简单来说，除非你的项目已经搭建于ROS之上，或者你需要使用某个只在ROS框架下存在的功能库，否则大多数情况下都应该直接选择ROS2。

\sphinxAtStartPar
ROS是一个很好的框架。它有着成熟活跃的社区，种类繁多的功能库，同时也有十分成熟的商业项目（例如Mobile
Industrial Robots公司的MiR robot）。
但是，ROS也有很多缺陷，例如工业项目中ROS的稳定性一直是个问题，必须进行额外的系统设计来增强到能够接受的程度。
这也是为什么ROS2会对框架的很多部分都进行重新设计的原因，因此ROS2也能更好的支持工业机器人项目。
除此之外，ROS的社区也在积极拥抱ROS2。官方和社区都提供了详细的教程来帮助开发者将ROS的库迁移到ROS2的框架内。
同时，官方也宣布了ROS Noetic
Ninjemys是最后一版ROS，并且预计于2025年五月停止官方支持（End of Life
date）。

\sphinxAtStartPar
本书将跟随官方的建议，使用ROS2作为讲解的框架。接下来我们详细介绍一下ROS2的核心概念（这一部分我们参考了文献  \sphinxstepexplicit %
\begin{footnote}[1]\phantomsection\label{\thesphinxscope.1}%
\sphinxAtStartFootnote
\sphinxurl{https://docs.ros.org/en/foxy/Tutorials/Understanding-ROS2-Nodes.html}
%
\end{footnote}）。


\subsubsection{ROS2 Nodes}
\label{\detokenize{chapter_rl_sys/ros:ros2-nodes}}
\sphinxAtStartPar
ROS
Graph是一个由ROS2元素组成的网络，在同一时间一起处理数据。它包括所有的可执行文件和它们之间的联系。ROS2
中的每个节点都应负责一个单一的模块用途（例如，一个节点用于控制车轮马达，一个节点用于控制激光测距仪等）。每个节点都可以通过主题、服务、动作或参数向其他节点发送和接收数据。一个完整的机器人系统由许多协同工作的节点组成。在
ROS 2 中，单个可执行文件（C++ 程序、Python
程序等）可以包含一个或多个节点，如 \hyperref[\detokenize{chapter_rl_sys/ros:ros2-graph}]{图\ref{\detokenize{chapter_rl_sys/ros:ros2-graph}}}。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ros2_graph}.png}
\caption{一个完整的机器人系统由许多协同工作的节点组成。在ROS 2
中，单个可执行文件（C++ 程序、Python 程序等）可以包含一个或多个节点}\label{\detokenize{chapter_rl_sys/ros:id7}}\label{\detokenize{chapter_rl_sys/ros:ros2-graph}}\end{figure}

\sphinxAtStartPar
节点之间的互相发现是通过ROS2底层的中间件实现的。 过程总结如下
\begin{itemize}
\item {} 
\sphinxAtStartPar
当一个节点启动后， 它会向其他拥有相同ROS域名(ROS domain，
可以通过设置ROS\_DOMAIN\_ID环境变量来设
置)的节点进行广播，说明它已经上线。
其他节点在收到广播后返回自己的相关信息，这样节点间的连接就可以建
立了，之后就可以通信了。

\item {} 
\sphinxAtStartPar
节点会定时广播它的信息，这样即使它已经错过了最初的发现过程，它也可以和新上线的节点进行连接。

\item {} 
\sphinxAtStartPar
节点在下线前它也会广播其他节点自己要下线了。

\end{itemize}


\subsubsection{ROS2 Topics}
\label{\detokenize{chapter_rl_sys/ros:ros2-topics}}
\sphinxAtStartPar
ROS2将复杂系统分解为许多模块化节点。主题（Topics）是 ROS
Graph的重要元素，它充当节点交换消息的总线。一个节点可以向任意数量的主题发布数据，同时订阅任意数量的主题，如图:numref:\sphinxtitleref{ros2\_topics}所示。主题是数据在节点之间以及因此在系统的不同部分之间移动的主要方式之一，例如，移动机器人的激光测距仪的相关节点可以将读数发布到lidar这个主题上，然后导航模块和安全模块的相关节点都可以订阅这个主题从而持续的获得想要的信息。

\sphinxAtStartPar
rqt是ROS的一个软件框架，以插件的形式实现了各种 GUI 工具。可以在 rqt
中将所有现有的GUI工具作为可停靠窗口运行。这些工具仍然可以以传统的独立方法运行，但rqt可以更轻松地同时管理屏幕上的所有各种窗口。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ros2_topics}.png}
\caption{一个节点可以向任意数量的主题发布数据，同时订阅任意数量的主题}\label{\detokenize{chapter_rl_sys/ros:id8}}\label{\detokenize{chapter_rl_sys/ros:id4}}\end{figure}


\subsubsection{ROS 2 Services}
\label{\detokenize{chapter_rl_sys/ros:ros-2-services}}
\sphinxAtStartPar
服务（Services）是 ROS
图中节点的另一种通信方式。服务基于调用和响应模型，而不是主题的发布者\sphinxhyphen{}订阅者模型。虽然主题允许节点订阅数据流并获得持续更新，但服务仅在客户端专门调用它们时才提供数据。节点可以使用ROS2中的服务进行通信。与主题那种单向通信模式，节点发布可由一个或多个订阅者使用的信息的方式不同
服务是客户端向节点发出请求的请求/响应模式提供服务，服务处理请求并生成响应。

\sphinxAtStartPar
例如，当一个服务机器人想要尝试识别一个条形码时，控制模块的相关节点（Service客户端）可以发送照相机截图到条形码识别节点（Service服务端），后者尝试识别并返回结果给控制模块。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ros2_services}.png}
\caption{ROS2服务}\label{\detokenize{chapter_rl_sys/ros:id9}}\label{\detokenize{chapter_rl_sys/ros:ros2-services}}\end{figure}


\subsubsection{ROS 2 Parameters}
\label{\detokenize{chapter_rl_sys/ros:ros-2-parameters}}
\sphinxAtStartPar
参数（Parameters）是节点的配置值。您可以将参数视为节点设置。节点可以将参数存储为整数、浮点数、布尔值、字符串和列表。在ROS2
中，每个节点都维护自己的参数。


\subsubsection{ROS 2 Actions}
\label{\detokenize{chapter_rl_sys/ros:ros-2-actions}}
\sphinxAtStartPar
动作（Actions）是ROS2中的一种通信类型，适用于长时间运行的任务。它们由三个部分组成：目标、反馈和结果。动作建立在主题和服务之上。它们的功能类似于服务，除了可以取消动作。它们还提供稳定的反馈，而不是返回单一响应的服务。动作使用客户端\sphinxhyphen{}服务器模型，类似于发布者\sphinxhyphen{}订阅者模型（在主题教程中描述）。“动作客户端”节点将目标发送到“动作服务器”节点，该节点确认目标并返回反馈流和结果。动作类似于允许您执行长时间运行的任务、提供定期反馈并且可以取消的服务。机器人系统可能会使用动作进行导航。动作目标可以告诉机器人前往某个位置。当机器人导航到该位置时，它可以沿途发送更新（即反馈），然后在到达目的地后发送最终结果消息。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{ros2_actions}.png}
\caption{ROS2动作}\label{\detokenize{chapter_rl_sys/ros:id10}}\label{\detokenize{chapter_rl_sys/ros:ros2-actions}}\end{figure}


\section{机器人操作系统（ROS）的入门案例}
\label{\detokenize{chapter_rl_sys/ros_code_ex:ros}}\label{\detokenize{chapter_rl_sys/ros_code_ex::doc}}
\sphinxAtStartPar
在这一章节中，我们将带领大家安装ROS2并配置好使用环境，然后再通过一些简单的代码示例来让大家更深入的了解如何使用ROS2和上一章节所介绍的概念。

\sphinxAtStartPar
在本章节以及本章后续的案例章节中，我们将使用ROS2 Foxy
Fitzroy（笔者撰写时的最新的ROS2 LTS版本），Ubuntu Focal（20.04）和Ubuntu
Focal系统所带的Python 3.8（笔者的Ubuntu Focal所带的是3.8.10）。 其中ROS2
Foxy Fitzroy和Ubuntu
Focal是官方的搭配，而如果你采用debian安装的方式（官方推荐方式）来安装ROS2的话，则Python必须使用Ubuntu所带的Python3版本。
这是因为debian安装方式会将很多ROS2的Python依赖库以\sphinxcode{\sphinxupquote{apt install}}（而非\sphinxcode{\sphinxupquote{pip install}}）的方式安装到Ubuntu自带的Python3路径中去。
这也就是说，当你选定ROS2版本后，你所需的Ubuntu版本和Python版本也就随之确定了。

\sphinxAtStartPar
如果想要使用Python虚拟环境（virtual
env）的话，也必须指定使用Ubuntu系统所带的Python解释器（interpreter），并在创建时加上\sphinxcode{\sphinxupquote{site\sphinxhyphen{}packages}}选项。添加这个选项是因为我们需要那些安装在系统Python3路径中的ROS2的依赖库。如果读者感兴趣的话，可以阅读这篇\sphinxhref{https://docs.ros.org/en/foxy/How-To-Guides/Using-Python-Packages.html}{英文教程}%
\begin{footnote}[89]\sphinxAtStartFootnote
\sphinxnolinkurl{https://docs.ros.org/en/foxy/How-To-Guides/Using-Python-Packages.html}
%
\end{footnote}来了解更多细节。

\sphinxAtStartPar
举例来说，对于\sphinxcode{\sphinxupquote{pipenv}}用户，可以通过下面这条命令来创建一个使用系统Python3并添加了\sphinxcode{\sphinxupquote{site\sphinxhyphen{}packages}}的虚拟环境。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pipenv \PYGZhy{}\PYGZhy{}python \PYG{k}{\PYGZdl{}(}/usr/bin/python3 \PYGZhy{}V \PYG{p}{|} cut \PYGZhy{}d\PYG{l+s+s2}{\PYGZdq{} \PYGZdq{}} \PYGZhy{}f2\PYG{k}{)} \PYGZhy{}\PYGZhy{}site\PYGZhy{}packages
\end{sphinxVerbatim}

\sphinxAtStartPar
因为要使用系统Python3的原因，用\sphinxcode{\sphinxupquote{conda}}创建的虚拟环境可能会出现各种不兼容的问题。

\sphinxAtStartPar
对于其它版本的ROS2，安装过程和使用方式基本相同。

\sphinxAtStartPar
在本章节以及本章后续的案例章节中，我们在合适的场合将用ROS2，Ubuntu和Python来分别指代ROS2
Foxy Fitzroy，Ubuntu Focal和Ubuntu Focal所带的Python 3.8。

\sphinxAtStartPar
本章节中的案例有参考ROS2的\sphinxhref{https://docs.ros.org/en/foxy/Tutorials.html}{官方教程}%
\begin{footnote}[90]\sphinxAtStartFootnote
\sphinxnolinkurl{https://docs.ros.org/en/foxy/Tutorials.html}
%
\end{footnote}。这个官方教程讲解的非常详细，非常适合初学者入门ROS2。如果读者对英文有自信的话，可以尝试阅读官方教程来了解更多ROS2的细节。

\sphinxAtStartPar
另外，本章节的案例所使用的代码可以在本书相关的\sphinxhref{https://github.com/openmlsys/openmlsys-ros2}{ROS2案例代码库}%
\begin{footnote}[91]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/openmlsys/openmlsys-ros2}
%
\end{footnote}中的\sphinxcode{\sphinxupquote{src/my\_hello\_world}}和\sphinxcode{\sphinxupquote{src/my\_interfaces}}文件夹内找到。


\subsection{安装ROS2 Foxy Fitzroy}
\label{\detokenize{chapter_rl_sys/ros_code_ex:ros2-foxy-fitzroy}}
\sphinxAtStartPar
在Ubuntu上安装ROS2相对简单，绝大多数情况跟随官方教程安装即可。
例如对于ROS2 Foxy Fitzroy和Ubuntu
Focal，对自己英文水平较为自信的读者也可以跟随\sphinxhref{https://docs.ros.org/en/foxy/Installation/Ubuntu-Install-Debians.html}{这篇官方教程}%
\begin{footnote}[92]\sphinxAtStartFootnote
\sphinxnolinkurl{https://docs.ros.org/en/foxy/Installation/Ubuntu-Install-Debians.html}
%
\end{footnote}。
本章节关于ROS2的安装的部分主要也是这篇教程相关部分的转述。


\subsubsection{系统区域（locale）需要支持UTF\sphinxhyphen{}8}
\label{\detokenize{chapter_rl_sys/ros_code_ex:locale-utf-8}}
\sphinxAtStartPar
在开始安装之前，我们需要先确保我们Ubuntu系统的区域（locale）已经设置成了支持UTF\sphinxhyphen{}8的值。
我们可以通过\sphinxcode{\sphinxupquote{locale}}命令来查看目前的区域（locale）设置。
如果\sphinxcode{\sphinxupquote{LANG}}的值是以\sphinxcode{\sphinxupquote{.UTF\sphinxhyphen{}8}}结尾的话，则代表系统已经是支持UTF\sphinxhyphen{}8的区域（locale）设置了。
否则，可以使用下面的命令来将系统的区域（locale）设置为支持UTF\sphinxhyphen{}8的美式英语。
想设置成其它语言只需更改相应的语言代码即可。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
sudo apt update \PYG{o}{\PYGZam{}\PYGZam{}} sudo apt install locales
sudo locale\PYGZhy{}gen en\PYGZus{}US en\PYGZus{}US.UTF\PYGZhy{}8
sudo update\PYGZhy{}locale \PYG{n+nv}{LC\PYGZus{}ALL}\PYG{o}{=}en\PYGZus{}US.UTF\PYGZhy{}8 \PYG{n+nv}{LANG}\PYG{o}{=}en\PYGZus{}US.UTF\PYGZhy{}8
\PYG{n+nb}{export} \PYG{n+nv}{LANG}\PYG{o}{=}en\PYGZus{}US.UTF\PYGZhy{}8
\end{sphinxVerbatim}


\subsubsection{设置软件源}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id1}}
\sphinxAtStartPar
我们还需要将ROS2的软件源加入到系统中。我们可以通过下面这些命令完成这点。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
sudo apt update \PYG{o}{\PYGZam{}\PYGZam{}} sudo apt install curl gnupg2 lsb\PYGZhy{}release
sudo curl \PYGZhy{}sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key  \PYGZhy{}o /usr/share/keyrings/ros\PYGZhy{}archive\PYGZhy{}keyring.gpg

\PYG{n+nb}{echo} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{deb [arch=}\PYG{k}{\PYGZdl{}(}dpkg \PYGZhy{}\PYGZhy{}print\PYGZhy{}architecture\PYG{k}{)}\PYG{l+s+s2}{ signed\PYGZhy{}by=/usr/share/keyrings/ros\PYGZhy{}archive\PYGZhy{}keyring.gpg] http://packages.ros.org/ros2/ubuntu }\PYG{k}{\PYGZdl{}(}\PYG{n+nb}{source} /etc/os\PYGZhy{}release \PYG{o}{\PYGZam{}\PYGZam{}} \PYG{n+nb}{echo} \PYG{n+nv}{\PYGZdl{}UBUNTU\PYGZus{}CODENAME}\PYG{k}{)}\PYG{l+s+s2}{ main}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{|} sudo tee /etc/apt/sources.list.d/ros2.list \PYGZgt{} /dev/null
\end{sphinxVerbatim}


\subsubsection{安装ROS2}
\label{\detokenize{chapter_rl_sys/ros_code_ex:ros2}}
\sphinxAtStartPar
现在我们可以开始安装ROS2了。我们可以先更新软件源缓存，然后再安装ROS2
Desktop版。这个版本包含了ROS2框架和大部分ROS2开发常用的软件库，如RViz等，因此是首选的版本。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
sudo apt update
sudo apt install ros\PYGZhy{}foxy\PYGZhy{}desktop
\end{sphinxVerbatim}

\sphinxAtStartPar
另外，让我们再来安装两个额外的软件，\sphinxcode{\sphinxupquote{colcon}}和\sphinxcode{\sphinxupquote{rosdep}}。前者是ROS2的编译工具，后者可以帮助我们迅速安装一个ROS2工程所需的依赖库。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
sudo apt\PYGZhy{}get install python3\PYGZhy{}colcon\PYGZhy{}common\PYGZhy{}extensions python3\PYGZhy{}rosdep
\end{sphinxVerbatim}

\sphinxAtStartPar
到此，我们已经安装好了ROS2。但是，如果想要使用它，我们还需要一个额外的环境设置步骤。


\subsubsection{环境设置}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id2}}
\sphinxAtStartPar
对于任意安装好的ROS2（和ROS）版本，我们需要source对应的setup脚本来为对应的版本设置好所需环境，然后才能开始使用其版本。

\sphinxAtStartPar
例如，对于刚安装好的ROS2 Foxy
Fitzroy，我们可以在终端中执行下面的命令来设置好ROS2所需的环境。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{source} /opt/ros/foxy/setup.bash
\end{sphinxVerbatim}

\sphinxAtStartPar
如果你用的是bash以外的shell，你可以尝试将setup的文件扩展名改为对应shell的名字。例如zsh的用户可以尝试使用\sphinxcode{\sphinxupquote{source /opt/ros/foxy/setup.zsh}}命令。

\sphinxAtStartPar
如果你不想每次使用ROS2之前都要输入上述命令，可以尝试将这条命令加入到你的\sphinxcode{\sphinxupquote{.bashrc}}文件中去（或者是\sphinxcode{\sphinxupquote{.zshrc}}或其它对应的shell文件）。这样，你以后的每个新命令行终端都会自动设置到ROS2所需的环境。

\sphinxAtStartPar
这种环境设置方式的好处在于你可以放心的安装多个不同版本的ROS2（和ROS），然后只需在需要时\sphinxcode{\sphinxupquote{source}}对应版本的\sphinxcode{\sphinxupquote{setup.bash}}文件，从而使用这个版本的ROS2并不受其它版本的干扰。

\sphinxAtStartPar
如果你是一个Python的重度用户，上面这种将\sphinxcode{\sphinxupquote{setup.bash}}加入到\sphinxcode{\sphinxupquote{.bashrc}}的方式可能会对你造成一些困扰。因为你的所有virtual
env从此都会自动引入ROS2的环境设置，并且ROS2所包含的python
libraries也会加入到你的virtual env的路径里面去。
我相信，你可能对于virtual
env会检测到ROS2的库这种情况不会感到特别开心，即使这些库并不会被用到或破坏你virtual
env中程序的运行。

\sphinxAtStartPar
解决这个问题的方法也很简单。当你准备主要用Python来开发一个ROS2项目时，你可以为这个项目新建一个virtual
env，然后将\sphinxcode{\sphinxupquote{source /opt/ros/foxy/setup.bash}}这条命令加入到这个virtual
env的\sphinxcode{\sphinxupquote{activate}}脚本中去。

\sphinxAtStartPar
注意！你可能需要将这条\sphinxcode{\sphinxupquote{source}}命令添加到脚本结尾前一些的位置或脚本最开头，要不然当你进入（activate）virtual
env时你有可能会遇到下面这个错误（例如，对于\sphinxcode{\sphinxupquote{pipenv}}的用户就需要添加到脚本结尾处的\sphinxcode{\sphinxupquote{hash \sphinxhyphen{}r 2>/dev/null}}这条命令之前而不是最末尾）。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Shell \PYG{k}{for} UNKNOWN\PYGZus{}VIRTUAL\PYGZus{}ENVIRONMENT already activated.
No action taken to avoid nested environments.
\end{sphinxVerbatim}


\subsubsection{测试安装成功}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id3}}
\sphinxAtStartPar
当我们执行了上述的\sphinxcode{\sphinxupquote{source}}命令之后，我们可以测试ROS2的安装以及环境设置时成功的。

\sphinxAtStartPar
我们只需在执行了\sphinxcode{\sphinxupquote{source}}命令的命令行中执行\sphinxcode{\sphinxupquote{printenv | grep \sphinxhyphen{}i \textasciicircum{}ROS}}。输出的结果应该包含以下三个环境变量。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nv}{ROS\PYGZus{}VERSION}\PYG{o}{=}\PYG{l+m}{2}
\PYG{n+nv}{ROS\PYGZus{}PYTHON\PYGZus{}VERSION}\PYG{o}{=}\PYG{l+m}{3}
\PYG{n+nv}{ROS\PYGZus{}DISTRO}\PYG{o}{=}foxy
\end{sphinxVerbatim}

\sphinxAtStartPar
此外，我们可以新开两个执行了\sphinxcode{\sphinxupquote{source}}命令的终端窗口，然后分别执行以下两条命令。

\sphinxAtStartPar
终端1:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ros2 run demo\PYGZus{}nodes\PYGZus{}cpp talker
\end{sphinxVerbatim}

\sphinxAtStartPar
终端2：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ros2 run demo\PYGZus{}nodes\PYGZus{}py listener
\end{sphinxVerbatim}

\sphinxAtStartPar
如果成功安装并执行了\sphinxcode{\sphinxupquote{source}}命令的话，我们将会看到\sphinxcode{\sphinxupquote{talker}}显示它正在发布消息，同时\sphinxcode{\sphinxupquote{listener}}显示它听到了这些消息。

\sphinxAtStartPar
恭喜！您已经成功安装好了ROS2并配置到了环境。下面我们将会通过几个简单的案例来展示上章节中介绍过的ROS2的核心概念。


\subsection{ROS2节点和Hello World}
\label{\detokenize{chapter_rl_sys/ros_code_ex:ros2hello-world}}
\sphinxAtStartPar
在这一小节中，我们将会创建一个ROS2项目，并使用Python来编写一个Hello
World案例，以便展示ROS2 Node的基本结构。


\subsubsection{新建一个ROS2项目}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id4}}
\sphinxAtStartPar
首先，在一个合适的位置新建一个文件夹。这个文件夹将是我们ROS2项目的根目录，同时也是上一章节中介绍过的“工作区”（Workspace）。这个工作区是我们自己创建的，所以它是一个Overlay
Workspace。相对的，我们之前执行的\sphinxcode{\sphinxupquote{source}}命令会帮我们准备好这个Overlay所基于的核心工作区（Underlay
Workspace）。

\sphinxAtStartPar
假设我们创建了名为\sphinxcode{\sphinxupquote{openmlsys\sphinxhyphen{}ros2}}的工作区。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mkdir openmlsys\PYGZhy{}ros2
\PYG{n+nb}{cd} openmlsys\PYGZhy{}ros2
\end{sphinxVerbatim}

\sphinxAtStartPar
然后让我们为这个工作区创建一个Python的虚拟环境（virtual
env）并依照上面\sphinxstyleemphasis{环境设置}小节中所介绍的那样将\sphinxcode{\sphinxupquote{source}}命令添加到虚拟环境对应的\sphinxcode{\sphinxupquote{activate}}脚本中去。

\sphinxAtStartPar
\sphinxstylestrong{我们默认之后所有案例章节的命令都是在这个新建的虚拟环境中执行的。}

\sphinxAtStartPar
不同的虚拟环境管理工具会有不同的指令，因此这一步笔者没有提供可执行命令的示例，而是留给读者自行处理。

\sphinxAtStartPar
接下来，我们要在这个工作区文件夹内新建一个名为\sphinxcode{\sphinxupquote{src}}的子文件夹。在这个子文件夹内，我们将会创建不同的ROS2的程序库（package）。这些程序库相互独立，但又会互相调用其他库的功能来达成整个ROS2项目想要达成的各种目的。

\sphinxAtStartPar
在创建好\sphinxcode{\sphinxupquote{src}}文件夹后，我们可以尝试调用\sphinxcode{\sphinxupquote{colcon build}}命令。\sphinxcode{\sphinxupquote{colcon}}是ROS2项目常用的一个编译工具（build
tool）。这个命令会尝试编译整个ROS2项目（即目前工作区内的所有的程序库）。在成功运行完命令后，我们可以发现工作区内多出了三个新文件夹：\sphinxcode{\sphinxupquote{build}}，\sphinxcode{\sphinxupquote{install}}和\sphinxcode{\sphinxupquote{log}}。其中\sphinxcode{\sphinxupquote{build}}内是编译过程的中间产物，\sphinxcode{\sphinxupquote{install}}内是编译的最终产物（即编译好的库），而\sphinxcode{\sphinxupquote{log}}内是编译过程的日志。

\sphinxAtStartPar
到此，我们已经新建好了一个ROS2项目的框架，可以开始编写具体的代码了。


\subsubsection{新建一个ROS2框架下的Python库}
\label{\detokenize{chapter_rl_sys/ros_code_ex:ros2python}}
\sphinxAtStartPar
下面，让我们在\sphinxcode{\sphinxupquote{src}}文件夹内新建一个ROS2的程序库。我们将在这个程序库内编写我们的Hello
World案例。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} src
ros2 pkg create \PYGZhy{}\PYGZhy{}build\PYGZhy{}type ament\PYGZus{}python \PYGZhy{}\PYGZhy{}dependencies rclpy std\PYGZus{}msgs \PYGZhy{}\PYGZhy{}node\PYGZhy{}name hello\PYGZus{}world\PYGZus{}node my\PYGZus{}hello\PYGZus{}world
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{ros2}}命令的\sphinxcode{\sphinxupquote{pkg create}}子项可以帮助我们快速的创建一个ROS2程序库的框架。\sphinxcode{\sphinxupquote{build\sphinxhyphen{}type}}参数指明了这是一个纯Python库，\sphinxcode{\sphinxupquote{dependencies}}参数指明了这个库将会使用\sphinxcode{\sphinxupquote{rclpy}}和\sphinxcode{\sphinxupquote{std\_msgs}}这两个依赖库，\sphinxcode{\sphinxupquote{node\sphinxhyphen{}name}}参数指明了我们创建的程序库中会有一个名为hello\_world\_node的ROS2节点，而最后的my\_hello\_world则是新建程序库的名字。

\sphinxAtStartPar
进入新建好的程序库文件夹\sphinxcode{\sphinxupquote{my\_hello\_world}}，我们可以看到刚运行的命令已经帮我们建好一个Python库文件夹\sphinxcode{\sphinxupquote{my\_hello\_world}}。其与程序库同名，且内含\sphinxcode{\sphinxupquote{\_\_init\_\_.py}}文件和\sphinxcode{\sphinxupquote{hello\_world\_node.py}}文件。后者的存在是由于我们使用了\sphinxcode{\sphinxupquote{node\_name}}参数的原因。我们将在这个Python库文件夹内编写我们的Python代码。

\sphinxAtStartPar
除此之外，还有\sphinxcode{\sphinxupquote{resource}}和\sphinxcode{\sphinxupquote{test}}这两个文件夹。前者帮助ROS2来定位Python程序库，因此我们不需要管它。后者用来包含所有的测试代码，并且我们可以看到里面已经有了三个测试文件。

\sphinxAtStartPar
除了这三个文件夹外，还有三个文件，\sphinxcode{\sphinxupquote{package.xml}}，\sphinxcode{\sphinxupquote{setup.cfg}}和\sphinxcode{\sphinxupquote{setup.py}}。

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{package.xml}}是ROS2程序库的标准配置文件。打开后我们可以发现很多内容已经预生成好了，但是我们还需填写或更新\sphinxcode{\sphinxupquote{version}}，\sphinxcode{\sphinxupquote{description}}，\sphinxcode{\sphinxupquote{maintainer}}和\sphinxcode{\sphinxupquote{license}}这几项的内容。在此笔者推荐大家每次新建一个ROS2库的时候都第一时间将这些信息补全。除了这些项，我们还能看到\sphinxcode{\sphinxupquote{rclpy}}和\sphinxcode{\sphinxupquote{std\_msgs}}已经被列为依赖库了，这是因为我们使用了\sphinxcode{\sphinxupquote{dependencies}}参数的原因。如果我们要添加或修改依赖库，可以直接在\sphinxcode{\sphinxupquote{package.xml}}内的\sphinxcode{\sphinxupquote{depend}}列表处修改。除了最常用的\sphinxcode{\sphinxupquote{depend}}（同时针对build，export和execution），我们还有\sphinxcode{\sphinxupquote{build\_depend}}，\sphinxcode{\sphinxupquote{build\_export\_depend}}，\sphinxcode{\sphinxupquote{exec\_depend}}，\sphinxcode{\sphinxupquote{test\_depend}}，\sphinxcode{\sphinxupquote{buildtool\_depend}}和\sphinxcode{\sphinxupquote{dec\_depend}}。关于\sphinxcode{\sphinxupquote{package.xml}}的具体介绍可以参考此英文\sphinxhref{http://wiki.ros.org/catkin/package.xml}{Wiki
Page}%
\begin{footnote}[93]\sphinxAtStartFootnote
\sphinxnolinkurl{http://wiki.ros.org/catkin/package.xml}
%
\end{footnote}。

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{setup.cfg}}和\sphinxcode{\sphinxupquote{setup.py}}都是Python库的相关文件，但是ROS2也会通过这两个文件来了解怎么安装这个Python库至\sphinxcode{\sphinxupquote{install}}文件夹以及有哪些需要注册的entry
points，即可以直接用ROS2命令行命令来直接调用的程序。我们可以看到在\sphinxcode{\sphinxupquote{setup.py}}中的\sphinxcode{\sphinxupquote{entry\_points}}项的\sphinxcode{\sphinxupquote{console\_scripts}}子项中已经将\sphinxcode{\sphinxupquote{hello\_world\_node}}这个名字设置为\sphinxcode{\sphinxupquote{my\_hello\_world/hello\_world\_node.py}}这个Python文件中\sphinxcode{\sphinxupquote{main()}}函数的别名。我们后续就可以使用ROS2命令行命令和这个名字来直接调用这个函数。具体方式如下:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} ros2 run \PYGZlt{}package\PYGZus{}name\PYGZgt{} \PYGZlt{}entry\PYGZus{}point\PYGZgt{}}
ros2 run my\PYGZus{}hello\PYGZus{}world hello\PYGZus{}world\PYGZus{}node
\end{sphinxVerbatim}

\sphinxAtStartPar
后续如果需要添加新的entry point的话可以直接在此位置添加。

\sphinxAtStartPar
除了entry
point需要关注之外，我们也需要及时将\sphinxcode{\sphinxupquote{setup.py}}中的\sphinxcode{\sphinxupquote{version}}，\sphinxcode{\sphinxupquote{maintainer}}，\sphinxcode{\sphinxupquote{maintainer\_email}}，\sphinxcode{\sphinxupquote{description}}和\sphinxcode{\sphinxupquote{license}}项都更新好。


\subsubsection{第一个ROS2节点}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id5}}
\sphinxAtStartPar
让我们打开\sphinxcode{\sphinxupquote{my\_hello\_world/hello\_world\_node.py}}这个Python文件，清空里面全部内容，以便于编写我们需要的代码。

\sphinxAtStartPar
首先，让我们引入必要的库：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{rclpy}
\PYG{k+kn}{from} \PYG{n+nn}{rclpy}\PYG{n+nn}{.}\PYG{n+nn}{node} \PYG{k+kn}{import} \PYG{n}{Node}

\PYG{k+kn}{from} \PYG{n+nn}{std\PYGZus{}msgs}\PYG{n+nn}{.}\PYG{n+nn}{msg} \PYG{k+kn}{import} \PYG{n}{String}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{rclpy}}（ROS Client Library for
Python）让我们能够通过Python来使用ROS2框架内的各种功能。而\sphinxcode{\sphinxupquote{Node}}类则是所有ROS2节点的基类（Base
Class），我们的节点类也需要继承这个基类。\sphinxcode{\sphinxupquote{std\_msgs}}则包含了ROS2预定义的一些用于框架内通信的标准信息格式，我们需要使用\sphinxcode{\sphinxupquote{String}}这种消息格式来传递字符串信息。

\sphinxAtStartPar
接下来让我们定义我们自己的ROS2节点：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{HelloWorldNode}\PYG{p}{(}\PYG{n}{Node}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{my\PYGZus{}hello\PYGZus{}world\PYGZus{}node}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{msg\PYGZus{}publisher} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{create\PYGZus{}publisher}\PYG{p}{(}\PYG{n}{String}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hello\PYGZus{}world\PYGZus{}topic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}
        \PYG{n}{timer\PYGZus{}period} \PYG{o}{=} \PYG{l+m+mf}{1.}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{timer} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{create\PYGZus{}timer}\PYG{p}{(}\PYG{n}{timer\PYGZus{}period}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{timer\PYGZus{}callback}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{count} \PYG{o}{=} \PYG{l+m+mi}{0}

    \PYG{k}{def} \PYG{n+nf}{timer\PYGZus{}callback}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{msg} \PYG{o}{=} \PYG{n}{String}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{msg}\PYG{o}{.}\PYG{n}{data} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hello World: }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{count}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{msg\PYGZus{}publisher}\PYG{o}{.}\PYG{n}{publish}\PYG{p}{(}\PYG{n}{msg}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Publishing: }\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{msg}\PYG{o}{.}\PYG{n}{data}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{count} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
\end{sphinxVerbatim}

\sphinxAtStartPar
如上所述，我们的节点类\sphinxcode{\sphinxupquote{HelloWorldNode}}继承于\sphinxcode{\sphinxupquote{Node}}基类。

\sphinxAtStartPar
在\sphinxcode{\sphinxupquote{\_\_init\_\_()}}方法中，我们先调用基类的初始化方法，并通过这个调用将我们的节点命名为\sphinxcode{\sphinxupquote{my\_hello\_world\_node}}。接着我们创建一个信息发布者，它可以将字符串类型的信息发布到\sphinxcode{\sphinxupquote{hello\_world\_topic}}这个主题上，并且会维持一个大小为10的缓冲区。再接着我们创建一个计时器，它会每秒钟调用一次\sphinxcode{\sphinxupquote{timer\_callback()}}方法。最后，我们初始化一个计数器，来统计总共有多少条信息被发布了。

\sphinxAtStartPar
在\sphinxcode{\sphinxupquote{timer\_callback()}}方法中，我们简单的创建一条带计数器的Hello
World信息，并通过信息发布者发送出去。然后我们在日志中记录这次操作并将计数器加一。

\sphinxAtStartPar
定义好我们的Hello
World节点类后，我们可以开始定义\sphinxcode{\sphinxupquote{main()}}函数。这个函数就是我们之前在\sphinxcode{\sphinxupquote{setup.py}}中看到的那个entry
point。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{main}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{init}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{n}{args}\PYG{p}{)}
    \PYG{n}{hello\PYGZus{}world\PYGZus{}node} \PYG{o}{=} \PYG{n}{HelloWorldNode}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{spin}\PYG{p}{(}\PYG{n}{hello\PYGZus{}world\PYGZus{}node}\PYG{p}{)}
    \PYG{n}{hello\PYGZus{}world\PYGZus{}node}\PYG{o}{.}\PYG{n}{destroy\PYGZus{}node}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{shutdown}\PYG{p}{(}\PYG{p}{)}


\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{n}{main}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
这个\sphinxcode{\sphinxupquote{main()}}也比较简单。我们先通过\sphinxcode{\sphinxupquote{rclpy.init()}}方法来启动ROS2框架。然后我们创建一个\sphinxcode{\sphinxupquote{HelloWorldNode}}的实例。接着我们通过\sphinxcode{\sphinxupquote{rclpy.spin()}}方法将这个实例加入到运行的ROS2框架中去，让其参与ROS2的事件循环并正确运行。\sphinxcode{\sphinxupquote{rclpy.spin()}}是一个阻碍方法，它会一直运行一直到被阻止（例如ROS2框架停止运行）。这时候我们就会摧毁我们的节点，并且确保关闭ROS2框架。如果我们忘记了摧毁不再使用的节点，不用慌，garbage
collector也会帮忙摧毁这个节点。

\sphinxAtStartPar
到此，我们创建了第一个ROS2节点！


\subsubsection{第一次编译和运行}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id6}}
\sphinxAtStartPar
让我们尝试编译新编写的这个库。这里，我们并不是真的要编译一个Python项目，而是将我们写的Python库安装到一个ROS2能找到的地方。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} cd \PYGZlt{}workspace\PYGZgt{}}
\PYG{n+nb}{cd} openmlsys\PYGZhy{}ros2
colcon build \PYGZhy{}\PYGZhy{}symlink\PYGZhy{}install
\end{sphinxVerbatim}

\sphinxAtStartPar
通过在运行这个编译命令，我们会编译工作区内\sphinxcode{\sphinxupquote{src}}文件夹下所有的Python和C++库，并将编译好的C++库和Python库安装到\sphinxcode{\sphinxupquote{install}}文件夹下。
通过指定\sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}symlink\sphinxhyphen{}install}}这个选项，我们要求\sphinxcode{\sphinxupquote{colcon}}对于Python库用生成symlink的方式来代替复制安装。这样一来，我们在\sphinxcode{\sphinxupquote{src}}中做的后续改动都会直接反应到\sphinxcode{\sphinxupquote{install}}中去，而不用一直反复执行编译命令。

\sphinxAtStartPar
在编译成功之后，编译好的库还不能直接使用。例如你现在执行\sphinxcode{\sphinxupquote{ros2 run my\_hello\_world hello\_world\_node}}的话很有可能会得到\sphinxcode{\sphinxupquote{Package 'my\_hello\_world' not found}}这样一个结果。

\sphinxAtStartPar
为了使用编译好的库，我们需要让ROS2知道\sphinxcode{\sphinxupquote{install}}文件夹。具体来说，我们需要\sphinxcode{\sphinxupquote{source}}在\sphinxcode{\sphinxupquote{install}}文件夹下的\sphinxcode{\sphinxupquote{local\_setup.bash}}文件。即：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{source} install/local\PYGZus{}setup.bash
\end{sphinxVerbatim}

\sphinxAtStartPar
有些机敏的读者可能会想到我们可以像之前添加那个\sphinxcode{\sphinxupquote{setup.bash}}一样将这个\sphinxcode{\sphinxupquote{install/local\_setup.bash}}也加入到虚拟环境的\sphinxcode{\sphinxupquote{activate}}脚本中去，这样我们就不用每次都单独\sphinxcode{\sphinxupquote{source}}这个文件了。很可惜，这样会带来一些问题。

\sphinxAtStartPar
具体来说，一方面我们需要将这两个文件都\sphinxcode{\sphinxupquote{source}}了（不管是通过\sphinxcode{\sphinxupquote{activate}}脚本还是手动输入）才能顺利运行编译好的ROS2程序，但另一方面我们必须只\sphinxcode{\sphinxupquote{source}}第一个\sphinxcode{\sphinxupquote{setup.bash}}而不\sphinxcode{\sphinxupquote{source}}第二个\sphinxcode{\sphinxupquote{local\_setup.bash}}才能顺利编译带有C++依赖项的纯Python的ROS2库。
在稍后面一点的案例中我们会看到，对于一个使用了自定义消息接口库（自己编写的C++库）的纯Python的ROS2程序库来说，必须只\sphinxcode{\sphinxupquote{source}}第一个\sphinxcode{\sphinxupquote{setup.bash}}而不\sphinxcode{\sphinxupquote{source}}第二个\sphinxcode{\sphinxupquote{local\_setup.bash}}才能顺利编译。

\sphinxAtStartPar
在成功\sphinxcode{\sphinxupquote{source}}了\sphinxcode{\sphinxupquote{install/local\_setup.bash}}之后，我们就可以尝试调用写好的节点了。

\sphinxAtStartPar
从现在开始，除非特殊说明，\sphinxstylestrong{新开一个终端窗口}都是指\sphinxstyleemphasis{新开一个确保``setup.bash``和``install/local\_setup.bash``都已经被``source``了的终端窗口}，而在\sphinxstylestrong{工作区执行``colcon build``命令}则都是\sphinxstyleemphasis{在一个只``source``了``setup.bash``而忽略了``install/local\_setup.bash``的终端窗口中执行此编译命令}。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ros2 run my\PYGZus{}hello\PYGZus{}world hello\PYGZus{}world\PYGZus{}node
\end{sphinxVerbatim}

\sphinxAtStartPar
我们应该会看到类似下面这样的信息：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{[}INFO\PYG{o}{]} \PYG{o}{[}\PYG{l+m}{1653270247}.805815900\PYG{o}{]} \PYG{o}{[}my\PYGZus{}hello\PYGZus{}world\PYGZus{}node\PYG{o}{]}: Publishing: \PYG{l+s+s2}{\PYGZdq{}Hello World: 0\PYGZdq{}}
\PYG{o}{[}INFO\PYG{o}{]} \PYG{o}{[}\PYG{l+m}{1653270248}.798165800\PYG{o}{]} \PYG{o}{[}my\PYGZus{}hello\PYGZus{}world\PYGZus{}node\PYG{o}{]}: Publishing: \PYG{l+s+s2}{\PYGZdq{}Hello World: 1\PYGZdq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
我们还可以再新开一个终端窗口，然后执行\sphinxcode{\sphinxupquote{ros2 topic echo /hello\_world\_topic}}。我们应该能看到类似下面的信息：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
data: \PYG{l+s+s1}{\PYGZsq{}Hello World: 23\PYGZsq{}}
\PYGZhy{}\PYGZhy{}\PYGZhy{}
data: \PYG{l+s+s1}{\PYGZsq{}Hello World: 24\PYGZsq{}}
\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\sphinxAtStartPar
这代表着我们的信息确实被发布到了目标主题上。因为\sphinxcode{\sphinxupquote{ros2 topic echo <topic\_name>}}这条命令输出的就是给定名字的主题所接收到的信息。

\sphinxAtStartPar
恭喜！您已成功运行了您的第一个ROS2节点！


\subsubsection{一个消息订阅者节点}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id7}}
\sphinxAtStartPar
只是发布消息并不能组成一个完整的流程，我们还需要一个消息订阅者来消费我们发布的信息。

\sphinxAtStartPar
让我们在\sphinxcode{\sphinxupquote{hello\_world\_node.py}}所在的文件夹内新建一个名为\sphinxcode{\sphinxupquote{message\_subscriber.py}}的文件，并添加以下内容：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{rclpy}
\PYG{k+kn}{from} \PYG{n+nn}{rclpy}\PYG{n+nn}{.}\PYG{n+nn}{node} \PYG{k+kn}{import} \PYG{n}{Node}

\PYG{k+kn}{from} \PYG{n+nn}{std\PYGZus{}msgs}\PYG{n+nn}{.}\PYG{n+nn}{msg} \PYG{k+kn}{import} \PYG{n}{String}


\PYG{k}{class} \PYG{n+nc}{MessageSubscriber}\PYG{p}{(}\PYG{n}{Node}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{my\PYGZus{}hello\PYGZus{}world\PYGZus{}subscriber}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{msg\PYGZus{}subscriber} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{create\PYGZus{}subscription}\PYG{p}{(}
            \PYG{n}{String}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hello\PYGZus{}world\PYGZus{}topic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{subscriber\PYGZus{}callback}\PYG{p}{,} \PYG{l+m+mi}{10}
        \PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{subscriber\PYGZus{}callback}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{msg}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Received }\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{msg}\PYG{o}{.}\PYG{n}{data}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}


\PYG{k}{def} \PYG{n+nf}{main}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{init}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{n}{args}\PYG{p}{)}
    \PYG{n}{message\PYGZus{}subscriber} \PYG{o}{=} \PYG{n}{MessageSubscriber}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{spin}\PYG{p}{(}\PYG{n}{message\PYGZus{}subscriber}\PYG{p}{)}
    \PYG{n}{message\PYGZus{}subscriber}\PYG{o}{.}\PYG{n}{destroy\PYGZus{}node}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{shutdown}\PYG{p}{(}\PYG{p}{)}


\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
    \PYG{n}{main}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
这个新添加的文件以及其中的消息订阅者节点类和上面的HelloWorld节点类十分相似，甚至更为简单些。我们只需要在初始化时通过基类初始化方法赋予节点\sphinxcode{\sphinxupquote{my\_hello\_world\_subscriber}}这个名字，然后创建一个消息订阅者来订阅\sphinxcode{\sphinxupquote{hello\_world\_topic}}主题下的消息，并指定\sphinxcode{\sphinxupquote{subscriber\_callback()}}方法来处理接收到的消息。而在\sphinxcode{\sphinxupquote{subscriber\_callback()}}中，我们将接收到的消息记录进日志。\sphinxcode{\sphinxupquote{main()}}方法则和HelloWorld节点类的基本一样。

\sphinxAtStartPar
在能正式使用这个新节点之前，我们需要将其添加成为一个entry
point。为此，我们只需在\sphinxcode{\sphinxupquote{setup.py}}的对应位置添加下面这行：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{message\PYGZus{}subscriber = my\PYGZus{}hello\PYGZus{}world.message\PYGZus{}subscriber:main}\PYG{l+s+s1}{\PYGZsq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
但是，添加完成之后在终端窗口运行\sphinxcode{\sphinxupquote{ros2 run my\_hello\_world message\_subscriber}}还是会得到\sphinxcode{\sphinxupquote{No executable found}}这样的错误反馈。这是因为我们新增了一个entry
point，必须重新编译整个ROS2项目才能让ROS2知道这个新增点。

\sphinxAtStartPar
让我们再次在工作区目录执行\sphinxcode{\sphinxupquote{colcon build \sphinxhyphen{}\sphinxhyphen{}symlink\sphinxhyphen{}install}}。在成功编译后，让我们新建两个终端窗口，都分别确保\sphinxcode{\sphinxupquote{source}}好了两个\sphinxcode{\sphinxupquote{setup}}文件。然后分别用\sphinxcode{\sphinxupquote{ros2}}命令调用它们：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} in terminal 1}
ros2 run my\PYGZus{}hello\PYGZus{}world hello\PYGZus{}world\PYGZus{}node
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} in terminal 2}
ros2 run my\PYGZus{}hello\PYGZus{}world message\PYGZus{}subscriber
\end{sphinxVerbatim}

\sphinxAtStartPar
我们应该可以看到终端窗口1中会不断显示发布了第N号Hello
World消息，而终端窗口2中则不断显示收到了第N号Hello World消息。

\sphinxAtStartPar
恭喜！你完成了一对ROS2节点，一个负责发送信息，一个负责订阅接受信息。


\subsection{ROS2参数}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id8}}
\sphinxAtStartPar
顺利完成上面的消息发布者和消息订阅者是个很好的开始，但是实际项目的节点不会这么简单。
至少，实际项目的节点会是参数化的。下面，就让我们一起看看怎样让一个节点读取一个参数。

\sphinxAtStartPar
让我们在\sphinxcode{\sphinxupquote{hello\_world\_node.py}}所在的文件夹内新建一个名为\sphinxcode{\sphinxupquote{parametrised\_hello\_world\_node.py}}的文件，并添加以下内容：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{rclpy}
\PYG{k+kn}{from} \PYG{n+nn}{rclpy}\PYG{n+nn}{.}\PYG{n+nn}{node} \PYG{k+kn}{import} \PYG{n}{Node}

\PYG{k+kn}{from} \PYG{n+nn}{std\PYGZus{}msgs}\PYG{n+nn}{.}\PYG{n+nn}{msg} \PYG{k+kn}{import} \PYG{n}{String}


\PYG{k}{class} \PYG{n+nc}{ParametrisedHelloWorldNode}\PYG{p}{(}\PYG{n}{Node}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{parametrised\PYGZus{}hello\PYGZus{}world\PYGZus{}node}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{msg\PYGZus{}publisher} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{create\PYGZus{}publisher}\PYG{p}{(}\PYG{n}{String}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hello\PYGZus{}world\PYGZus{}topic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}
        \PYG{n}{timer\PYGZus{}period} \PYG{o}{=} \PYG{l+m+mf}{1.}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{timer} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{create\PYGZus{}timer}\PYG{p}{(}\PYG{n}{timer\PYGZus{}period}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{timer\PYGZus{}callback}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{count} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{declare\PYGZus{}parameter}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{world}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{timer\PYGZus{}callback}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{name} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}parameter}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{get\PYGZus{}parameter\PYGZus{}value}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{string\PYGZus{}value}
        \PYG{n}{msg} \PYG{o}{=} \PYG{n}{String}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{msg}\PYG{o}{.}\PYG{n}{data} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hello }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{name}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{count}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{msg\PYGZus{}publisher}\PYG{o}{.}\PYG{n}{publish}\PYG{p}{(}\PYG{n}{msg}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Publishing: }\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{msg}\PYG{o}{.}\PYG{n}{data}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{count} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}


\PYG{k}{def} \PYG{n+nf}{main}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{init}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{n}{args}\PYG{p}{)}
    \PYG{n}{hello\PYGZus{}world\PYGZus{}node} \PYG{o}{=} \PYG{n}{ParametrisedHelloWorldNode}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{spin}\PYG{p}{(}\PYG{n}{hello\PYGZus{}world\PYGZus{}node}\PYG{p}{)}
    \PYG{n}{hello\PYGZus{}world\PYGZus{}node}\PYG{o}{.}\PYG{n}{destroy\PYGZus{}node}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{shutdown}\PYG{p}{(}\PYG{p}{)}


\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{n}{main}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
我们可以看到，这个新的参数化HelloWorld节点类和之前的HelloWorld节点类基本相同。
唯二的区别在于：1）这个新类在初始化方法中额外通过\sphinxcode{\sphinxupquote{self.declare\_parameter()}}方法来向ROS2框架声明新的节点实例会有一个名为\sphinxcode{\sphinxupquote{name}}的参数，并且这个参数的初始值为\sphinxcode{\sphinxupquote{world}}；2）这个新类在\sphinxcode{\sphinxupquote{timer\_callback()}}回调函数中尝试获取这个\sphinxcode{\sphinxupquote{name}}参数的实际值，并以这个实际值来组成要发送的信息的内容。

\sphinxAtStartPar
让我们先将这个新文件的\sphinxcode{\sphinxupquote{main()}}方法注册为一个新的entry point。
同样的，在\sphinxcode{\sphinxupquote{setup.py}}中的相应位置加入下面这行即可。
然后别忘了在工作区根目录下执行\sphinxcode{\sphinxupquote{colcon build \sphinxhyphen{}\sphinxhyphen{}symlink\sphinxhyphen{}install}}来重新编译项目。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{parametrised\PYGZus{}hello\PYGZus{}world\PYGZus{}node = my\PYGZus{}hello\PYGZus{}world.parametrised\PYGZus{}hello\PYGZus{}world\PYGZus{}node:main}\PYG{l+s+s1}{\PYGZsq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
在编译完成之后，如果我们在终端中执行\sphinxcode{\sphinxupquote{ros2 run my\_hello\_world parametrised\_hello\_world\_node}}，我们将看到这个参数化HelloWorld节点将正常运行，并持续发布“\sphinxstyleemphasis{Hello
World: N}”这样的信息。此时节点使用的是\sphinxcode{\sphinxupquote{world}}这个初始值。

\sphinxAtStartPar
让我们在一个新的终端中执行\sphinxcode{\sphinxupquote{ros2 param list}}，我们将看到下面的信息：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/parametrised\PYGZus{}hello\PYGZus{}world\PYGZus{}node:
  name
  use\PYGZus{}sim\PYGZus{}time
\end{sphinxVerbatim}

\sphinxAtStartPar
这个信息表示\sphinxcode{\sphinxupquote{parametrised\_hello\_world\_node}}这个节点的确申明并使用一个\sphinxcode{\sphinxupquote{name}}参数。
另外一个名为\sphinxcode{\sphinxupquote{use\_sim\_time}}的参数是ROS2默认给与的一个参数，用来表示这个节点是否使用ROS2框架内部的模拟时间，而不是电脑的系统时间。

\sphinxAtStartPar
我们可以继续在这个终端中输入下面这个命令来将值\sphinxcode{\sphinxupquote{ROS2}}赋予给\sphinxcode{\sphinxupquote{name}}这个参数。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ros2 param \PYG{n+nb}{set} /parametrised\PYGZus{}hello\PYGZus{}world\PYGZus{}node name \PYG{l+s+s2}{\PYGZdq{}ROS2\PYGZdq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
如果赋值成功的话，这个命令会返回\sphinxcode{\sphinxupquote{Set parameter successful}}，并且我们可以在持续运行参数化HelloWorld节点的那个终端窗口内看到其发布的信息变为了“\sphinxstyleemphasis{Hello
ROS2: N}”。

\sphinxAtStartPar
恭喜！你现在掌握了如何让ROS2节点（和其它类型的ROS2程序）使用参数的方法。


\subsection{ROS2服务}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id9}}
\sphinxAtStartPar
在上一章节中我们知道了ROS2框架除了发布者\sphinxhyphen{}订阅者这种通信模式，还有服务端\sphinxhyphen{}客户端这种模式。
在这一小节中，我们将通过一个简单的串联两个字符串的服务来演示如何使用这种模式。


\subsubsection{自定义的服务接口}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id10}}
\sphinxAtStartPar
在正式开始编写服务端和客户端的代码之前，我们需要先定义好它们之间进行沟通的信息接口。

\sphinxAtStartPar
ROS2框架内有三种类型的信息接口：
\begin{itemize}
\item {} 
\sphinxAtStartPar
发布者\sphinxhyphen{}订阅者模式下的节点所用的\sphinxstylestrong{消息}类型接口（message/msg）：这种接口只负责单向的消息传递，也只用定义单向传递的信息的格式。

\item {} 
\sphinxAtStartPar
服务端\sphinxhyphen{}客户端模式下的服务节点所用的\sphinxstylestrong{服务}类型接口（service/srv）：这种接口需要负责双向的消息传递，即需要定义客户端发给服务端的请求的格式和服务端发给客户端的响应的格式。

\item {} 
\sphinxAtStartPar
动作模式下的动作节点所用的\sphinxstylestrong{动作}类型接口（action）：这种接口需要负责双向的消息传递以及中间的进展反馈，即需要定义动作发起节点发给动作节点的请求的格式，动作节点发给发起节点的结果的格式，以及动作节点发给发起节点的中间进展反馈的格式。

\end{itemize}

\sphinxAtStartPar
对于前面定义的那些HelloWorld节点，我们使用的是已经预定义好的\sphinxcode{\sphinxupquote{std\_msgs}}库内的\sphinxcode{\sphinxupquote{std\_msgs.msg.String}}类型的消息类型接口。
实际上，因为消息类型接口只负责定义单向的信息格式，我们很容易找到现成的符合我们需求的类型。
但是对于服务（service）和动作（action）来说，因为涉及到定义双向沟通的格式，很多时候我们需要自己定义一个接口类型。接下来，就让我们自行定义我们的字符串串联服务将要使用的服务类型接口。

\sphinxAtStartPar
首先，让我们在工作区的\sphinxcode{\sphinxupquote{src}}文件夹内新建一个库来专门维护自定义的消息，服务和动作类型接口。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} openmlsys\PYGZhy{}ros2/src
ros2 pkg create \PYGZhy{}\PYGZhy{}build\PYGZhy{}type ament\PYGZus{}cmake my\PYGZus{}interfaces
\end{sphinxVerbatim}

\sphinxAtStartPar
这个新建的库是一个C++库，而不是Python库。这是因为ROS2的自定义接口类型只能以C++库的方式存在。新建好库之后，记得更新\sphinxcode{\sphinxupquote{package.xml}}中的相关项。

\sphinxAtStartPar
下面，让我们在新建的\sphinxcode{\sphinxupquote{src/my\_interfaces}}文件夹内新建三个子文件夹：\sphinxcode{\sphinxupquote{msg}}，\sphinxcode{\sphinxupquote{srv}}和\sphinxcode{\sphinxupquote{action}}。这是因为一般会将自定义的接口放到相对应的子文件夹中去，以方便维护。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} my\PYGZus{}interfaces
mkdir msg srv action
\end{sphinxVerbatim}

\sphinxAtStartPar
接着，让我们在\sphinxcode{\sphinxupquote{srv}}子目录下创建我们想要定义的服务类型接口。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} srv
touch ConcatTwoStr.srv
\end{sphinxVerbatim}

\sphinxAtStartPar
然后，让我们将以下内容添加到\sphinxcode{\sphinxupquote{ConcatTwoStr.srv}}中去：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{string} \PYG{n}{str1}
\PYG{n}{string} \PYG{n}{str2}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\PYG{n}{string} \PYG{n}{ret}
\end{sphinxVerbatim}

\sphinxAtStartPar
其中，\sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}}}之上的是客户端发给服务端的请求的格式，而之下的是服务端发给客户端的响应的格式。

\sphinxAtStartPar
定义好了接口后，我们还需要更改\sphinxcode{\sphinxupquote{CMakeLists.txt}}以便让编译器知道有自定义接口需要编译并能找到它们。让我们打开\sphinxcode{\sphinxupquote{my\_interfaces/CMakeLists.txt}}并在\sphinxcode{\sphinxupquote{if(BUILD\_TESTING)}}这行之前添加下面的内容。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
find\PYGZus{}package(rosidl\PYGZus{}default\PYGZus{}generators REQUIRED)

rosidl\PYGZus{}generate\PYGZus{}interfaces(\PYGZdl{}\PYGZob{}PROJECT\PYGZus{}NAME\PYGZcb{}
  \PYGZdq{}srv/ConcatTwoStr.srv\PYGZdq{}
)
\end{sphinxVerbatim}

\sphinxAtStartPar
上面这两段代码的主要作用是告诉编译器需要\sphinxcode{\sphinxupquote{rosidl\_default\_generators}}这个库并生成我们指明的自定义接口。

\sphinxAtStartPar
在更新好\sphinxcode{\sphinxupquote{CMakeLists.txt}}之后，我们还需要把\sphinxcode{\sphinxupquote{rosidl\_default\_generators}}添加到\sphinxcode{\sphinxupquote{package.xml}}中作为自定义接口库的依赖项。打开\sphinxcode{\sphinxupquote{package.xml}}，在\sphinxcode{\sphinxupquote{<test\_depend>ament\_lint\_auto</test\_depend>}}这行前添加下面内容。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{\PYGZlt{}build\PYGZus{}depend}\PYG{n+nt}{\PYGZgt{}}rosidl\PYGZus{}default\PYGZus{}generators\PYG{n+nt}{\PYGZlt{}/build\PYGZus{}depend\PYGZgt{}}
\PYG{n+nt}{\PYGZlt{}exec\PYGZus{}depend}\PYG{n+nt}{\PYGZgt{}}rosidl\PYGZus{}default\PYGZus{}runtime\PYG{n+nt}{\PYGZlt{}/exec\PYGZus{}depend\PYGZgt{}}
\PYG{n+nt}{\PYGZlt{}member\PYGZus{}of\PYGZus{}group}\PYG{n+nt}{\PYGZgt{}}rosidl\PYGZus{}interface\PYGZus{}packages\PYG{n+nt}{\PYGZlt{}/member\PYGZus{}of\PYGZus{}group\PYGZgt{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
更新好\sphinxcode{\sphinxupquote{package.xml}}后，我们就可以编译这个自定义接口库了。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} openmlsys\PYGZhy{}ros2
colcon build \PYGZhy{}\PYGZhy{}packages\PYGZhy{}select my\PYGZus{}interfaces
\end{sphinxVerbatim}

\sphinxAtStartPar
上述命令中我们通过\sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}packages\sphinxhyphen{}select}}选项指定了只编译\sphinxcode{\sphinxupquote{my\_interfaces}}这一个库从而节省时间，因为\sphinxcode{\sphinxupquote{my\_hello\_world}}这个库目前并没有任何更改。另外，我们没有使用\sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}symlink\sphinxhyphen{}install}}选项是因为这个自定义接口库是一个C++库，每次更改后必须重新编译。

\sphinxAtStartPar
在运行这次的编译命令时，读者有可能会遇到\sphinxcode{\sphinxupquote{ModuleNotFoundError: No module named 'XXX'}}这类的错误（\sphinxcode{\sphinxupquote{XXX}}可以是\sphinxcode{\sphinxupquote{em}}，\sphinxcode{\sphinxupquote{catkin\_pkg}}，\sphinxcode{\sphinxupquote{lark}}，\sphinxcode{\sphinxupquote{numpy}}或其它Python库）。
遇到这类错误多半是因为所使用的Python虚拟环境并不是指向Ubuntu系统Python3或\sphinxcode{\sphinxupquote{site\sphinxhyphen{}packages}}并没有被包含在虚拟环境中。
读者可能需要删除当前的虚拟环境并按照本章节开头所讲解的那样重新创建一个符合要求的虚拟环境。

\sphinxAtStartPar
我们可以通过在新的终端窗口运行\sphinxcode{\sphinxupquote{ros2 interface show my\_interfaces/srv/ConcatTwoStr}}来验证是否已经编译成功了。成功的话终端会显示自定义服务接口\sphinxcode{\sphinxupquote{ConcatTwoStr}}的具体定义。

\sphinxAtStartPar
现在，我们定义好了需要使用的服务接口，下面可以开始编写我们的服务段和客户端了。


\subsubsection{ROS2服务端}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id11}}
\sphinxAtStartPar
让我们在\sphinxcode{\sphinxupquote{hello\_world\_node.py}}所在的文件夹内新建一个名为\sphinxcode{\sphinxupquote{concat\_two\_str\_service.py}}的文件，并添加以下内容：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{my\PYGZus{}interfaces}\PYG{n+nn}{.}\PYG{n+nn}{srv} \PYG{k+kn}{import} \PYG{n}{ConcatTwoStr}

\PYG{k+kn}{import} \PYG{n+nn}{rclpy}
\PYG{k+kn}{from} \PYG{n+nn}{rclpy}\PYG{n+nn}{.}\PYG{n+nn}{node} \PYG{k+kn}{import} \PYG{n}{Node}


\PYG{k}{class} \PYG{n+nc}{ConcatTwoStrService}\PYG{p}{(}\PYG{n}{Node}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}service}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{srv} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{create\PYGZus{}service}\PYG{p}{(}\PYG{n}{ConcatTwoStr}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{concat\PYGZus{}two\PYGZus{}str}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}callback}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}callback}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{request}\PYG{p}{,} \PYG{n}{response}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{response}\PYG{o}{.}\PYG{n}{ret} \PYG{o}{=} \PYG{n}{request}\PYG{o}{.}\PYG{n}{str1} \PYG{o}{+} \PYG{n}{request}\PYG{o}{.}\PYG{n}{str2}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Incoming request}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{str1: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{request}\PYG{o}{.}\PYG{n}{str1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{str2: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{request}\PYG{o}{.}\PYG{n}{str2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

        \PYG{k}{return} \PYG{n}{response}


\PYG{k}{def} \PYG{n+nf}{main}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{init}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{n}{args}\PYG{p}{)}
    \PYG{n}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}service} \PYG{o}{=} \PYG{n}{ConcatTwoStrService}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{spin}\PYG{p}{(}\PYG{n}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}service}\PYG{p}{)}
    \PYG{n}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}service}\PYG{o}{.}\PYG{n}{destroy\PYGZus{}node}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{shutdown}\PYG{p}{(}\PYG{p}{)}


\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{n}{main}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
我们可以发现，编写一个服务（Service）和编写一个一般的节点（Node）很相似，甚至它们都是继承自同一个基类\sphinxcode{\sphinxupquote{rclpy.node.Node}}。在这个文件中，我们先从编译好的\sphinxcode{\sphinxupquote{my\_interfaces}}库中引入自定义的服务接口\sphinxcode{\sphinxupquote{ConcatTwoStr}}。然后在服务端节点的初始化方法中通过\sphinxcode{\sphinxupquote{self.create\_service()}}创建一个服务器对象，并指明服务接口类型是\sphinxcode{\sphinxupquote{ConcatTwoStr}}，服务名字是\sphinxcode{\sphinxupquote{concat\_two\_str}}，处理服务请求的回调函数是\sphinxcode{\sphinxupquote{self.concat\_two\_str\_callback}}。而在回调函数\sphinxcode{\sphinxupquote{self.concat\_two\_str\_callback()}}中，我们通过\sphinxcode{\sphinxupquote{request}}对象取得请求的\sphinxcode{\sphinxupquote{str1}}和\sphinxcode{\sphinxupquote{str2}}，计算出结果并赋值到\sphinxcode{\sphinxupquote{response}}对象的\sphinxcode{\sphinxupquote{ret}}上，并进行日志记录。我们可以看到，\sphinxcode{\sphinxupquote{request}}和\sphinxcode{\sphinxupquote{response}}对象的结构符合我们在\sphinxcode{\sphinxupquote{ConcatTwoStr.srv}}中的定义。

\sphinxAtStartPar
另外别忘记了将此文件的\sphinxcode{\sphinxupquote{main()}}方法作为一个entry
point添加到\sphinxcode{\sphinxupquote{setup.py}}中去。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}service = my\PYGZus{}hello\PYGZus{}world.concat\PYGZus{}two\PYGZus{}str\PYGZus{}service:main}\PYG{l+s+s1}{\PYGZsq{}}
\end{sphinxVerbatim}


\subsubsection{ROS2客户端}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id12}}
\sphinxAtStartPar
让我们在\sphinxcode{\sphinxupquote{hello\_world\_node.py}}所在的文件夹内新建一个名为\sphinxcode{\sphinxupquote{concat\_two\_str\_client\_async.py}}的文件，并添加以下内容：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{sys}

\PYG{k+kn}{from} \PYG{n+nn}{my\PYGZus{}interfaces}\PYG{n+nn}{.}\PYG{n+nn}{srv} \PYG{k+kn}{import} \PYG{n}{ConcatTwoStr}
\PYG{k+kn}{import} \PYG{n+nn}{rclpy}
\PYG{k+kn}{from} \PYG{n+nn}{rclpy}\PYG{n+nn}{.}\PYG{n+nn}{node} \PYG{k+kn}{import} \PYG{n}{Node}


\PYG{k}{class} \PYG{n+nc}{ConcatTwoStrClientAsync}\PYG{p}{(}\PYG{n}{Node}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}client\PYGZus{}async}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cli} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{create\PYGZus{}client}\PYG{p}{(}\PYG{n}{ConcatTwoStr}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{concat\PYGZus{}two\PYGZus{}str}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{k}{while} \PYG{o+ow}{not} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cli}\PYG{o}{.}\PYG{n}{wait\PYGZus{}for\PYGZus{}service}\PYG{p}{(}\PYG{n}{timeout\PYGZus{}sec}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{)}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service not available, waiting again...}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{req} \PYG{o}{=} \PYG{n}{ConcatTwoStr}\PYG{o}{.}\PYG{n}{Request}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{send\PYGZus{}request}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{req}\PYG{o}{.}\PYG{n}{str1} \PYG{o}{=} \PYG{n}{sys}\PYG{o}{.}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{req}\PYG{o}{.}\PYG{n}{str2} \PYG{o}{=} \PYG{n}{sys}\PYG{o}{.}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{future} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cli}\PYG{o}{.}\PYG{n}{call\PYGZus{}async}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{req}\PYG{p}{)}


\PYG{k}{def} \PYG{n+nf}{main}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{init}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{n}{args}\PYG{p}{)}

    \PYG{n}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}client\PYGZus{}async} \PYG{o}{=} \PYG{n}{ConcatTwoStrClientAsync}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}client\PYGZus{}async}\PYG{o}{.}\PYG{n}{send\PYGZus{}request}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{while} \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{ok}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{spin\PYGZus{}once}\PYG{p}{(}\PYG{n}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}client\PYGZus{}async}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}client\PYGZus{}async}\PYG{o}{.}\PYG{n}{future}\PYG{o}{.}\PYG{n}{done}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{try}\PYG{p}{:}
                \PYG{n}{response} \PYG{o}{=} \PYG{n}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}client\PYGZus{}async}\PYG{o}{.}\PYG{n}{future}\PYG{o}{.}\PYG{n}{result}\PYG{p}{(}\PYG{p}{)}
            \PYG{k}{except} \PYG{n+ne}{Exception} \PYG{k}{as} \PYG{n}{e}\PYG{p}{:}
                \PYG{n}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}client\PYGZus{}async}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}
                    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Service call failed }\PYG{l+s+si}{\PYGZpc{}r}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{p}{(}\PYG{n}{e}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}
            \PYG{k}{else}\PYG{p}{:}
                \PYG{n}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}client\PYGZus{}async}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}
                    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Result of concat\PYGZus{}two\PYGZus{}str: (}\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s1}{, }\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s1}{) \PYGZhy{}\PYGZgt{} }\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}}
                    \PYG{p}{(}\PYG{n}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}client\PYGZus{}async}\PYG{o}{.}\PYG{n}{req}\PYG{o}{.}\PYG{n}{str1}\PYG{p}{,} \PYG{n}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}client\PYGZus{}async}\PYG{o}{.}\PYG{n}{req}\PYG{o}{.}\PYG{n}{str2}\PYG{p}{,} \PYG{n}{response}\PYG{o}{.}\PYG{n}{ret}\PYG{p}{)}\PYG{p}{)}
            \PYG{k}{break}

    \PYG{n}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}client\PYGZus{}async}\PYG{o}{.}\PYG{n}{destroy\PYGZus{}node}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{shutdown}\PYG{p}{(}\PYG{p}{)}


\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{n}{main}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
相比于服务端，这个客户端较为复杂一点。在客户端节点的初始化方法中，我们先创建一个客户端对象，并指明服务接口类型是\sphinxcode{\sphinxupquote{ConcatTwoStr}}，服务名字为\sphinxcode{\sphinxupquote{concat\_two\_str}}。然后通过一个\sphinxcode{\sphinxupquote{while}}循环，这个客户端将一直等待知道对应服务上线才会进行下一步。这个循环等待的技巧是很多客户端都会使用的。当服务端上线以后，初始化方法将创建一个服务请求对象的模板并暂存于客户端节点的\sphinxcode{\sphinxupquote{req}}属性上。除了初始化方法，客户端节点还定义了另一个方法\sphinxcode{\sphinxupquote{send\_request()}}来读取程序启动时命令行的前两个参数，然后存入服务请求对象并异步发送给服务端。

\sphinxAtStartPar
而在\sphinxcode{\sphinxupquote{main()}}方法中，我们先创建一个客户端并发送服务请求，然后通过一个\sphinxcode{\sphinxupquote{while}}循环来等待服务返回结果并记录进日志。其中，\sphinxcode{\sphinxupquote{rclpy.ok()}}是用来检测ROS2是否还在正常运行，以保证当ROS2在服务结束前就停止运行了的话，客户端这边不会陷入死循环。而\sphinxcode{\sphinxupquote{rclpy.spin\_once()}}和\sphinxcode{\sphinxupquote{rclpy.spin()}}略有不同，后者会不断执行事件循环直到ROS2停止，而前者则只会执行一次事件循环。这也是为什么前者更适合用在这里，因为我们已经有了一个while循环了。另外我们可以看到，\sphinxcode{\sphinxupquote{concat\_two\_str\_client.future}}对象提供了很多方法来帮助我们确定目前服务请求的状态。

\sphinxAtStartPar
同样的，别忘记了将此文件的\sphinxcode{\sphinxupquote{main()}}方法作为一个entry
point添加到\sphinxcode{\sphinxupquote{setup.py}}中去。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{concat\PYGZus{}two\PYGZus{}str\PYGZus{}client\PYGZus{}async = my\PYGZus{}hello\PYGZus{}world.concat\PYGZus{}two\PYGZus{}str\PYGZus{}client\PYGZus{}async:main}\PYG{l+s+s1}{\PYGZsq{}}
\end{sphinxVerbatim}


\subsubsection{确认正常运行}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id13}}
\sphinxAtStartPar
我们现在编写好了我们的服务端和客户端，让我们在工作区根目录下重新编译一边\sphinxcode{\sphinxupquote{my\_hello\_world}}库。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} openmlsys\PYGZhy{}ros2
colcon build \PYGZhy{}\PYGZhy{}packages\PYGZhy{}select my\PYGZus{}hello\PYGZus{}world \PYGZhy{}\PYGZhy{}symlink\PYGZhy{}install
\end{sphinxVerbatim}

\sphinxAtStartPar
然后让我们在两个新的终端窗口中分别运行以下命令。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} in terminal 1}
ros2 run my\PYGZus{}hello\PYGZus{}world concat\PYGZus{}two\PYGZus{}str\PYGZus{}client\PYGZus{}async Hello World
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} in terminal 2}
ros2 run my\PYGZus{}hello\PYGZus{}world concat\PYGZus{}two\PYGZus{}str\PYGZus{}service
\end{sphinxVerbatim}

\sphinxAtStartPar
如果一切正常的话，我们应该看到类似以下的信息。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} in terminal 1}
\PYG{o}{[}INFO\PYG{o}{]} \PYG{o}{[}\PYG{l+m}{1653525569}.843701600\PYG{o}{]} \PYG{o}{[}concat\PYGZus{}two\PYGZus{}str\PYGZus{}client\PYGZus{}async\PYG{o}{]}: Result of concat\PYGZus{}two\PYGZus{}str: \PYG{o}{(}Hello, World\PYG{o}{)} \PYGZhy{}\PYGZgt{} HelloWorld
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} in terminal 2}
\PYG{o}{[}INFO\PYG{o}{]} \PYG{o}{[}\PYG{l+m}{1653516701}.306543500\PYG{o}{]} \PYG{o}{[}concat\PYGZus{}two\PYGZus{}str\PYGZus{}service\PYG{o}{]}: Incoming request
str1: Hello
str2: World
\end{sphinxVerbatim}

\sphinxAtStartPar
恭喜！您现在已经了解如何在ROS2框架中新建自定义的接口类型和创建服务端节点和客户端节点了！


\subsection{ROS2动作}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id14}}\label{\detokenize{chapter_rl_sys/ros_code_ex:id15}}
\sphinxAtStartPar
在上一章节中我们了解了ROS2框架内的服务端\sphinxhyphen{}客户端模式。这样一来，我们只剩下动作（action）这一种模式了。
在这一小节中，我们将通过一个简单的逐个累加一个数列的每项元素来求和的动作来演示如何使用这种模式。


\subsubsection{自定义的动作接口}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id16}}
\sphinxAtStartPar
在正式开始编写动作相关的节点代码之前，我们需要先定义好动作的信息接口。

\sphinxAtStartPar
我们可以继续使用之前建好的\sphinxcode{\sphinxupquote{my\_interfaces}}库。
让我们在\sphinxcode{\sphinxupquote{my\_interfaces/action}}中新建一个\sphinxcode{\sphinxupquote{MySum.action}}文件，并添加以下内容。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Request}
\PYG{n}{int32}\PYG{p}{[}\PYG{p}{]} \PYG{n+nb}{list}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\PYG{c+c1}{\PYGZsh{} Result}
\PYG{n}{int32} \PYG{n+nb}{sum}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\PYG{c+c1}{\PYGZsh{} Feedback}
\PYG{n}{int32} \PYG{n}{sum\PYGZus{}so\PYGZus{}far}
\end{sphinxVerbatim}

\sphinxAtStartPar
可以看到，整个信息接口十分简单。动作的请求信息只有一项类型为整数数列的项\sphinxcode{\sphinxupquote{list}}，动作的最终结果信息只有一项类型为整数的项\sphinxcode{\sphinxupquote{sum`}}，而中间反馈信息则只有一项类型同为整数的项`sum\_so\_far`，用以计算到目前位置累加的和。

\sphinxAtStartPar
接下来，让我们在\sphinxcode{\sphinxupquote{CMakeLists.txt}}中添加这个新的信息接口。具体来说只用将\sphinxcode{\sphinxupquote{"action/MySum.action"}}添加到\sphinxcode{\sphinxupquote{rosidl\_generate\_interfaces()}}方法内的\sphinxcode{\sphinxupquote{"srv/ConcatTwoStr.srv"}}之后即可。

\sphinxAtStartPar
最后别忘了编译所做的更改：在工作区根目录中运行\sphinxcode{\sphinxupquote{colcon build \sphinxhyphen{}\sphinxhyphen{}packages\sphinxhyphen{}select my\_interface}}。


\subsubsection{ROS2动作服务器}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id17}}
\sphinxAtStartPar
让我们在\sphinxcode{\sphinxupquote{hello\_world\_node.py}}所在的文件夹内新建一个名为\sphinxcode{\sphinxupquote{my\_sum\_action\_server.py}}的文件，并添加以下内容：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{rclpy}
\PYG{k+kn}{from} \PYG{n+nn}{rclpy}\PYG{n+nn}{.}\PYG{n+nn}{action} \PYG{k+kn}{import} \PYG{n}{ActionServer}
\PYG{k+kn}{from} \PYG{n+nn}{rclpy}\PYG{n+nn}{.}\PYG{n+nn}{node} \PYG{k+kn}{import} \PYG{n}{Node}

\PYG{k+kn}{from} \PYG{n+nn}{my\PYGZus{}interfaces}\PYG{n+nn}{.}\PYG{n+nn}{action} \PYG{k+kn}{import} \PYG{n}{MySum}


\PYG{k}{class} \PYG{n+nc}{MySumActionServer}\PYG{p}{(}\PYG{n}{Node}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{my\PYGZus{}sum\PYGZus{}action\PYGZus{}server}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}action\PYGZus{}server} \PYG{o}{=} \PYG{n}{ActionServer}\PYG{p}{(}
            \PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{MySum}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{my\PYGZus{}sum}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{execute\PYGZus{}callback}
        \PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{execute\PYGZus{}callback}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{goal\PYGZus{}handle}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Executing goal...}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{feedback\PYGZus{}msg} \PYG{o}{=} \PYG{n}{MySum}\PYG{o}{.}\PYG{n}{Feedback}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{feedback\PYGZus{}msg}\PYG{o}{.}\PYG{n}{sum\PYGZus{}so\PYGZus{}far} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{k}{for} \PYG{n}{elm} \PYG{o+ow}{in} \PYG{n}{goal\PYGZus{}handle}\PYG{o}{.}\PYG{n}{request}\PYG{o}{.}\PYG{n}{list}\PYG{p}{:}
            \PYG{n}{feedback\PYGZus{}msg}\PYG{o}{.}\PYG{n}{sum\PYGZus{}so\PYGZus{}far} \PYG{o}{+}\PYG{o}{=} \PYG{n}{elm}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Feedback: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{feedback\PYGZus{}msg}\PYG{o}{.}\PYG{n}{sum\PYGZus{}so\PYGZus{}far}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
            \PYG{n}{goal\PYGZus{}handle}\PYG{o}{.}\PYG{n}{publish\PYGZus{}feedback}\PYG{p}{(}\PYG{n}{feedback\PYGZus{}msg}\PYG{p}{)}
        \PYG{n}{goal\PYGZus{}handle}\PYG{o}{.}\PYG{n}{succeed}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{result} \PYG{o}{=} \PYG{n}{MySum}\PYG{o}{.}\PYG{n}{Result}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{result}\PYG{o}{.}\PYG{n}{sum} \PYG{o}{=} \PYG{n}{feedback\PYGZus{}msg}\PYG{o}{.}\PYG{n}{sum\PYGZus{}so\PYGZus{}far}
        \PYG{k}{return} \PYG{n}{result}


\PYG{k}{def} \PYG{n+nf}{main}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{init}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{n}{args}\PYG{p}{)}

    \PYG{n}{my\PYGZus{}sum\PYGZus{}action\PYGZus{}server} \PYG{o}{=} \PYG{n}{MySumActionServer}\PYG{p}{(}\PYG{p}{)}

    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{spin}\PYG{p}{(}\PYG{n}{my\PYGZus{}sum\PYGZus{}action\PYGZus{}server}\PYG{p}{)}


\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{n}{main}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
对于这个动作服务器节点类，类似的，我们还是在其初始化方法中新建一个动作服务器对象，并指定了之前定义的\sphinxcode{\sphinxupquote{MySum}}作为信息接口类型，\sphinxcode{\sphinxupquote{my\_sum}}是动作名字，\sphinxcode{\sphinxupquote{self.execute\_callback}}方法则作为动作执行的回调函数。

\sphinxAtStartPar
紧接着，我们在\sphinxcode{\sphinxupquote{self.execute\_callback()}}方法中定义了当接收到了一个新目标是应做什么处理。在这里，我们可以把一个目标当作之前定义的\sphinxcode{\sphinxupquote{MySum}}信息接口里的\sphinxcode{\sphinxupquote{request}}部分来处理，因为这里的目标就是包含了动作请求的目的的相关信息的结构体，即\sphinxcode{\sphinxupquote{request}}部分所定义的部分。

\sphinxAtStartPar
当我们接收到一个目标后，我们先从\sphinxcode{\sphinxupquote{MySum}}创建一个反馈消息对象\sphinxcode{\sphinxupquote{feedback\_msg}}，并将其\sphinxcode{\sphinxupquote{sum\_so\_far}}项用作一个累加器。然后我们遍历目标请求中的\sphinxcode{\sphinxupquote{list}}项里面的数据，并这些数据逐项进行累加。每当我们累加一项后，我们都会通过\sphinxcode{\sphinxupquote{goal\_handle.publish\_feedback()}}方法发送一次反馈消息。最后，当全部计算完成后，我们通过\sphinxcode{\sphinxupquote{goal\_handle.succeed()}}来标记此次动作已经成功完成，并且通过\sphinxcode{\sphinxupquote{MySum}}新建一个结果对象，填充结果值并返回。

\sphinxAtStartPar
在\sphinxcode{\sphinxupquote{main()}}函数中，我们只需要新建一个动作服务器节点类的新实例，并调用\sphinxcode{\sphinxupquote{rclpy.spin()}}将其加入事件循环即可。

\sphinxAtStartPar
最后别忘了将\sphinxcode{\sphinxupquote{main()}}也添加成为一个entry
point。我们只需在\sphinxcode{\sphinxupquote{setup.py}}中适当位置添加下面行即可。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{my\PYGZus{}sum\PYGZus{}action\PYGZus{}server = my\PYGZus{}hello\PYGZus{}world.my\PYGZus{}sum\PYGZus{}action\PYGZus{}server:main}\PYG{l+s+s1}{\PYGZsq{}}
\end{sphinxVerbatim}


\subsubsection{ROS2动作客户端}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id18}}
\sphinxAtStartPar
让我们在\sphinxcode{\sphinxupquote{hello\_world\_node.py}}所在的文件夹内新建一个名为\sphinxcode{\sphinxupquote{my\_sum\_action\_client.py}}的文件，并添加以下内容：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{k+kn}{import} \PYG{n+nn}{rclpy}
\PYG{k+kn}{from} \PYG{n+nn}{rclpy}\PYG{n+nn}{.}\PYG{n+nn}{action} \PYG{k+kn}{import} \PYG{n}{ActionClient}
\PYG{k+kn}{from} \PYG{n+nn}{rclpy}\PYG{n+nn}{.}\PYG{n+nn}{node} \PYG{k+kn}{import} \PYG{n}{Node}

\PYG{k+kn}{from} \PYG{n+nn}{my\PYGZus{}interfaces}\PYG{n+nn}{.}\PYG{n+nn}{action} \PYG{k+kn}{import} \PYG{n}{MySum}


\PYG{k}{class} \PYG{n+nc}{MySumActionClient}\PYG{p}{(}\PYG{n}{Node}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{my\PYGZus{}sum\PYGZus{}action\PYGZus{}client}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}action\PYGZus{}client} \PYG{o}{=} \PYG{n}{ActionClient}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{MySum}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{my\PYGZus{}sum}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{send\PYGZus{}goal}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n+nb}{list}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{goal\PYGZus{}msg} \PYG{o}{=} \PYG{n}{MySum}\PYG{o}{.}\PYG{n}{Goal}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{goal\PYGZus{}msg}\PYG{o}{.}\PYG{n}{list} \PYG{o}{=} \PYG{n+nb}{list}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}action\PYGZus{}client}\PYG{o}{.}\PYG{n}{wait\PYGZus{}for\PYGZus{}server}\PYG{p}{(}\PYG{p}{)}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}send\PYGZus{}goal\PYGZus{}future} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}action\PYGZus{}client}\PYG{o}{.}\PYG{n}{send\PYGZus{}goal\PYGZus{}async}\PYG{p}{(}
            \PYG{n}{goal\PYGZus{}msg}\PYG{p}{,} \PYG{n}{feedback\PYGZus{}callback}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{feedback\PYGZus{}callback}
        \PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}send\PYGZus{}goal\PYGZus{}future}\PYG{o}{.}\PYG{n}{add\PYGZus{}done\PYGZus{}callback}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{goal\PYGZus{}response\PYGZus{}callback}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{goal\PYGZus{}response\PYGZus{}callback}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{future}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{goal\PYGZus{}handle} \PYG{o}{=} \PYG{n}{future}\PYG{o}{.}\PYG{n}{result}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{goal\PYGZus{}handle}\PYG{o}{.}\PYG{n}{accepted}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Goal rejected...}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
            \PYG{k}{return}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Goal accepted.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}get\PYGZus{}result\PYGZus{}future} \PYG{o}{=} \PYG{n}{goal\PYGZus{}handle}\PYG{o}{.}\PYG{n}{get\PYGZus{}result\PYGZus{}async}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}get\PYGZus{}result\PYGZus{}future}\PYG{o}{.}\PYG{n}{add\PYGZus{}done\PYGZus{}callback}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}result\PYGZus{}callback}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}result\PYGZus{}callback}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{future}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{result} \PYG{o}{=} \PYG{n}{future}\PYG{o}{.}\PYG{n}{result}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{result}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Result: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{result}\PYG{o}{.}\PYG{n}{sum}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{shutdown}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{feedback\PYGZus{}callback}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{feedback\PYGZus{}msg}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{feedback} \PYG{o}{=} \PYG{n}{feedback\PYGZus{}msg}\PYG{o}{.}\PYG{n}{feedback}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Received feedback: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{feedback}\PYG{o}{.}\PYG{n}{sum\PYGZus{}so\PYGZus{}far}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}


\PYG{k}{def} \PYG{n+nf}{main}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{init}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{n}{args}\PYG{p}{)}

    \PYG{n}{action\PYGZus{}client} \PYG{o}{=} \PYG{n}{MySumActionClient}\PYG{p}{(}\PYG{p}{)}

    \PYG{n}{action\PYGZus{}client}\PYG{o}{.}\PYG{n}{send\PYGZus{}goal}\PYG{p}{(}\PYG{p}{[}\PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{elm}\PYG{p}{)} \PYG{k}{for} \PYG{n}{elm} \PYG{o+ow}{in} \PYG{n}{sys}\PYG{o}{.}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{spin}\PYG{p}{(}\PYG{n}{action\PYGZus{}client}\PYG{p}{)}


\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{n}{main}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
我们可以看到，这个动作客户端节点类比上面的服务器节点类要稍许复杂些，这是因为我们要适当的处理发送请求，接受反馈和处理结果这三件事。

\sphinxAtStartPar
首先，还是类似的，我们在这个动作客户端节点类的初始化方法中新建一个动作客户端对象，并指定\sphinxcode{\sphinxupquote{MySum}}作为消息接口类型和\sphinxcode{\sphinxupquote{my\_sum}}作为动作名称。

\sphinxAtStartPar
然后，我们申明\sphinxcode{\sphinxupquote{self.send\_goal()}}方法来负责生成并发送一个目标/请求。具体来说，我们先从\sphinxcode{\sphinxupquote{MySum}}新建一个目标对象并将接收到的\sphinxcode{\sphinxupquote{list}}参数赋值到目标对象的\sphinxcode{\sphinxupquote{list}}属性上去。紧接着，让我们等待动作服务器准备就绪。当动作服务器准备就绪后，让我们异步发送目标并指定\sphinxcode{\sphinxupquote{self.feedback\_callback}}作为反馈信息回调函数。最后，我们设定\sphinxcode{\sphinxupquote{self.goal\_response\_callback}}作为发送目标信息这个异步操作的回调函数。

\sphinxAtStartPar
在\sphinxcode{\sphinxupquote{self.goal\_response\_callback()}}这个异步发送目标信息的回调函数中，我们先检查目标请求是否被接受了，并日志记录相关结果。如果目标请求被接受了的话，我们就通过\sphinxcode{\sphinxupquote{goal\_handle.get\_result\_async()}}来得到处理结果这个异步操作的\sphinxcode{\sphinxupquote{future}}对象，并通过这个\sphinxcode{\sphinxupquote{future}}对象将\sphinxcode{\sphinxupquote{self.get\_result\_callback}}设定为最终结果的回调函数。

\sphinxAtStartPar
在\sphinxcode{\sphinxupquote{self.get\_result\_callback()}}这个最终结果的回调函数中，我们就简单的获取累加结果并记录进日志。最后我们调用\sphinxcode{\sphinxupquote{rclpy.shutdown()}}来结束当前节点。

\sphinxAtStartPar
相对的，在\sphinxcode{\sphinxupquote{self.feedback\_callback()}}这个反馈消息的回调函数中。我们仅仅简单的获取反馈信息的内容并记录进日志。值得注意的是，反馈消息的回调函数可能被执行多次，所以最好不要在其中写入太多的处理逻辑，而是尽量让其轻量化。

\sphinxAtStartPar
最后，在\sphinxcode{\sphinxupquote{main()}}方法中，我们创建一个动作客户端节点类的实例，将命令行的参数转化为需要被求和的目标数列，最后调用动作客户端节点类实例的\sphinxcode{\sphinxupquote{send\_goal()}}方法并传入目标求和数列来发起求和请求。

\sphinxAtStartPar
同样的，别忘了将\sphinxcode{\sphinxupquote{main()}}也添加成为一个entry
point。我们只需在\sphinxcode{\sphinxupquote{setup.py}}中适当位置添加下面行即可。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{my\PYGZus{}sum\PYGZus{}action\PYGZus{}client = my\PYGZus{}hello\PYGZus{}world.my\PYGZus{}sum\PYGZus{}action\PYGZus{}client:main}\PYG{l+s+s1}{\PYGZsq{}}
\end{sphinxVerbatim}


\subsubsection{确认正常运行}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id19}}
\sphinxAtStartPar
我们现在编写好了我们的动作服务器和动作客户端，让我们在工作区根目录下重新编译一边\sphinxcode{\sphinxupquote{my\_hello\_world}}库。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} openmlsys\PYGZhy{}ros2
colcon build \PYGZhy{}\PYGZhy{}packages\PYGZhy{}select my\PYGZus{}hello\PYGZus{}world \PYGZhy{}\PYGZhy{}symlink\PYGZhy{}install
\end{sphinxVerbatim}

\sphinxAtStartPar
然后让我们在两个新的终端窗口中分别运行以下命令。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} in terminal 1}
ros2 run my\PYGZus{}hello\PYGZus{}world my\PYGZus{}sum\PYGZus{}action\PYGZus{}client \PYG{l+m}{1} \PYG{l+m}{2} \PYG{l+m}{3}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} in terminal 2}
ros2 run my\PYGZus{}hello\PYGZus{}world my\PYGZus{}sum\PYGZus{}action\PYGZus{}server
\end{sphinxVerbatim}

\sphinxAtStartPar
如果一切正常的话，我们应该看到类似以下的信息。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} in terminal 1}
\PYG{o}{[}INFO\PYG{o}{]} \PYG{o}{[}\PYG{l+m}{1653561740}.000499500\PYG{o}{]} \PYG{o}{[}my\PYGZus{}sum\PYGZus{}action\PYGZus{}client\PYG{o}{]}: Goal accepted.
\PYG{o}{[}INFO\PYG{o}{]} \PYG{o}{[}\PYG{l+m}{1653561740}.001171900\PYG{o}{]} \PYG{o}{[}my\PYGZus{}sum\PYGZus{}action\PYGZus{}client\PYG{o}{]}: Received feedback: \PYG{l+m}{1}
\PYG{o}{[}INFO\PYG{o}{]} \PYG{o}{[}\PYG{l+m}{1653561740}.001644000\PYG{o}{]} \PYG{o}{[}my\PYGZus{}sum\PYGZus{}action\PYGZus{}client\PYG{o}{]}: Received feedback: \PYG{l+m}{3}
\PYG{o}{[}INFO\PYG{o}{]} \PYG{o}{[}\PYG{l+m}{1653561740}.002327500\PYG{o}{]} \PYG{o}{[}my\PYGZus{}sum\PYGZus{}action\PYGZus{}client\PYG{o}{]}: Received feedback: \PYG{l+m}{6}
\PYG{o}{[}INFO\PYG{o}{]} \PYG{o}{[}\PYG{l+m}{1653561740}.002761600\PYG{o}{]} \PYG{o}{[}my\PYGZus{}sum\PYGZus{}action\PYGZus{}client\PYG{o}{]}: Result: \PYG{l+m}{6}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} in terminal 2}
\PYG{o}{[}INFO\PYG{o}{]} \PYG{o}{[}\PYG{l+m}{1653561739}.988907200\PYG{o}{]} \PYG{o}{[}my\PYGZus{}sum\PYGZus{}action\PYGZus{}server\PYG{o}{]}: Executing goal...
\PYG{o}{[}INFO\PYG{o}{]} \PYG{o}{[}\PYG{l+m}{1653561739}.989213900\PYG{o}{]} \PYG{o}{[}my\PYGZus{}sum\PYGZus{}action\PYGZus{}server\PYG{o}{]}: Feedback: \PYG{l+m}{1}
\PYG{o}{[}INFO\PYG{o}{]} \PYG{o}{[}\PYG{l+m}{1653561739}.989549000\PYG{o}{]} \PYG{o}{[}my\PYGZus{}sum\PYGZus{}action\PYGZus{}server\PYG{o}{]}: Feedback: \PYG{l+m}{3}
\PYG{o}{[}INFO\PYG{o}{]} \PYG{o}{[}\PYG{l+m}{1653561739}.989855400\PYG{o}{]} \PYG{o}{[}my\PYGZus{}sum\PYGZus{}action\PYGZus{}server\PYG{o}{]}: Feedback: \PYG{l+m}{6}
\end{sphinxVerbatim}

\sphinxAtStartPar
恭喜！您现在已经了解如何在ROS2框架中新建自定义的接口类型和创建动作服务端节点和动作客户端节点了！


\subsection{小结}
\label{\detokenize{chapter_rl_sys/ros_code_ex:id20}}
\sphinxAtStartPar
在本章节中，我们了解了怎样安装ROS2和在Python虚拟环境中进行ROS2项目的开发。然后我们通过一些案例来更加深入的了解了ROS2的一些核心概念，即节点，主题，参数，服务，和动作。


\section{感知系统}
\label{\detokenize{chapter_rl_sys/perception:id1}}\label{\detokenize{chapter_rl_sys/perception::doc}}
\sphinxAtStartPar
感知系统不仅可以包括视觉，还可以包含触觉、声音等。在未知环境中，机器人想实现自主移动和导航必须知道自己在哪（例如通过相机重定位
\sphinxcite{chapter_rl_sys/summary:ding2019camnet}），周围什么情况（例如通过3D物体检测
\sphinxcite{chapter_rl_sys/summary:yi2020segvoxelnet}或语义分割），这些要依靠感知系统来实现
\sphinxcite{chapter_rl_sys/summary:xu2019depth}。
一提到感知系统，不得不提的就是即时定位与建图（Simultaneous Localization
and
Mapping，SLAM)系统。SLAM大致过程包括地标提取、数据关联、状态估计、状态更新以及地标更新等。视觉里程计Visual
Odometry是SLAM中的重要部分，它估计两个时刻机器人的相对运动（Ego\sphinxhyphen{}motion）。ORB\sphinxhyphen{}SLAM系列是视觉SLAM中有代表性的工作， \hyperref[\detokenize{chapter_rl_sys/perception:orbslam3}]{图\ref{\detokenize{chapter_rl_sys/perception:orbslam3}}}
展示了最新的ORB\sphinxhyphen{}SLAM3的主要系统组件。香港科技大学开源的基于单目视觉与惯导融合的SLAM技术VINS\sphinxhyphen{}Mono也很值得关注。多传感器融合、优化数据关联与回环检测、与前端异构处理器集成、提升鲁棒性和重定位精度都是SLAM技术接下来的发展方向。

\sphinxAtStartPar
最近，随着机器学习的兴起，基于学习的SLAM框架也被提了出来。TartanVO是第一个基于学习的视觉里程计（VO）模型，该模型可以推广到多个数据集和现实世界场景，并优于传统基于几何的方法。
UnDeepVO是一个无监督深度学习方案，能够通过使用深度神经网络估计单目相机的
6\sphinxhyphen{}DoF 位姿及其视图深度。DROID\sphinxhyphen{}SLAM是用于单目、立体和 RGB\sphinxhyphen{}D
相机的深度视觉 SLAM，它通过Bundle
Adjustment层对相机位姿和像素深度的反复迭代更新，具有很强的鲁棒性，故障大大减少，尽管对单目视频进行了训练，但它可以利用立体声或
RGB\sphinxhyphen{}D 视频在测试时提高性能。其中，Bundle Adjustment
(BA)与机器学习的结合被广泛研究。CMU提出通过主动神经 SLAM
的模块化系统帮助智能机器人在未知环境中的高效探索。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{orbslam3}.png}
\caption{ORB\sphinxhyphen{}SLAM3主要系统组件 \sphinxcite{chapter_rl_sys/summary:campos2021orb}}\label{\detokenize{chapter_rl_sys/perception:id6}}\label{\detokenize{chapter_rl_sys/perception:orbslam3}}\end{figure}


\section{感知系统案例}
\label{\detokenize{chapter_rl_sys/perception_code_ex:id1}}\label{\detokenize{chapter_rl_sys/perception_code_ex::doc}}
\sphinxAtStartPar
在之前\sphinxhref{./ros\_code\_ex.md}{机器人操作系统（ROS）的入门案例}这一章节中，我们学习了怎样创建一个ROS2项目以及怎样使用ROS2框架下的节点，服务，动作等。然后，我们又在上一章节中初步了解了机器人的感知系统。
在这一章节中，我们将通过一个简单的案例来演示怎样结合ROS2和深度学习框架PyTorch来完成一个我们设想的感知系统中的一个基本功能。


\subsection{案例背景}
\label{\detokenize{chapter_rl_sys/perception_code_ex:id2}}
\sphinxAtStartPar
假设我们想要帮某果园设计一款全自动摘菠萝机器人。
这个机器人可能需要有一个智能移动底盘来负责在果园中移动，若干传感器（包括一个RGB摄像头）来检测菠萝，以及一个机械臂来负责摘取动作。
在这个机器人需要完成的一长列各种功能中，它的感知系统必然需要能检测摄像头传感器的画面中央是否有一个菠萝。
检测到了菠萝才会进入到摘取的环节。

\sphinxAtStartPar
这个检测在图像中央是否有菠萝存在的功能，就是我们机器人的感知系统中基础但必要的一个基本功能。
幸运的是，随着现代卷积神经网络的发展，我们可以利用已存在的深度学习框架，如PyTorch，来快速的完成这个功能。
而且一个简单的使用ImageNet进行预训练的AlexNet就以及足够了。

\sphinxAtStartPar
在之前的案例中，我们都是使用的ROS2框架下的程序库。在这个例子中，我们将开始了解如何在ROS2中使用框架外的Python库。

\sphinxAtStartPar
和之前的案例类似，本章节的案例所使用的代码可以在本书相关的\sphinxhref{https://github.com/openmlsys/openmlsys-ros2}{ROS2案例代码库}%
\begin{footnote}[94]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/openmlsys/openmlsys-ros2}
%
\end{footnote}中的\sphinxcode{\sphinxupquote{src/object\_detector}}文件夹内找到。


\subsection{项目搭建}
\label{\detokenize{chapter_rl_sys/perception_code_ex:id3}}
\sphinxAtStartPar
让我们沿用之前已经搭建好的ROS2项目框架。
我们只需在其中增加一个ROS2的Python库来实现我们想要的功能即可。
因此，让我们回到\sphinxcode{\sphinxupquote{src}}目录下并创建此Python库。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} openmlsys\PYGZhy{}ros2/src
ros2 pkg create \PYGZhy{}\PYGZhy{}build\PYGZhy{}type ament\PYGZus{}python \PYGZhy{}\PYGZhy{}node\PYGZhy{}name object\PYGZus{}detector\PYGZus{}node object\PYGZus{}detector \PYGZhy{}\PYGZhy{}dependencies rclpy std\PYGZus{}msgs sensor\PYGZus{}msgs cv\PYGZus{}bridge opencv\PYGZhy{}python torch torchvision torchaudio
\end{sphinxVerbatim}

\sphinxAtStartPar
在创建好Python库后，别忘了将\sphinxcode{\sphinxupquote{package.xml}}和\sphinxcode{\sphinxupquote{setup.py}}中的\sphinxcode{\sphinxupquote{version}}，\sphinxcode{\sphinxupquote{maintainer}}，\sphinxcode{\sphinxupquote{maintainer\_email}}，\sphinxcode{\sphinxupquote{description}}和\sphinxcode{\sphinxupquote{license}}项都更新好。

\sphinxAtStartPar
紧接着，我们需要安装ROS2框架下的\sphinxcode{\sphinxupquote{image\_publisher}}库。
这个库能帮助我们将一张图片模拟成像摄像头视频一样的图片流。
在开发真是机器人时，我们可能可以在实机上检测我们的程序，但是对于这个案例，我们只能使用这个\sphinxcode{\sphinxupquote{image\_publisher}}库和若干选择好的图片来测试我们的程序。
实际上，就算是开发实际机器人的功能，也最好在实际测试之前使用图片来做这个功能的单元测试。

\sphinxAtStartPar
我们只需通过ubuntu的\sphinxcode{\sphinxupquote{apt}}来安装这个\sphinxcode{\sphinxupquote{image\_publisher}}库，因为作为一个常用的ROS2框架下的程序库，它已经被打包好以便通过ubuntu的包管理器来安装。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
sudo apt install ros\PYGZhy{}foxy\PYGZhy{}image\PYGZhy{}publisher
\end{sphinxVerbatim}

\sphinxAtStartPar
关于\sphinxcode{\sphinxupquote{image\_publisher}}这个库更多的信息和使用方法，可以查看\sphinxhref{http://wiki.ros.org/image\_publisher}{它的文档}%
\begin{footnote}[95]\sphinxAtStartFootnote
\sphinxnolinkurl{http://wiki.ros.org/image\_publisher}
%
\end{footnote}。这是个针对早期ROS1版本的文档，但是因为这个库之后没有任何变化，文档中所有的功能都和我们所使用的ROS2版本中的一样。

\sphinxAtStartPar
接下来，让我们在ROS2项目的Python虚拟环境中安装\sphinxcode{\sphinxupquote{opencv\sphinxhyphen{}python}}，\sphinxcode{\sphinxupquote{torch}}，\sphinxcode{\sphinxupquote{torchvision}}和\sphinxcode{\sphinxupquote{torchaudio}}。
例如使用\sphinxcode{\sphinxupquote{pipenv}}的用户可能会执行\sphinxcode{\sphinxupquote{pipenv install opencv\sphinxhyphen{}python torch torchvision torchaudio}}这条命令。

\sphinxAtStartPar
最后，让我们把下面这两张菠萝和苹果的图片保存在\sphinxcode{\sphinxupquote{openmlsys\sphinxhyphen{}ros2/data}}下。
我们将使用这两张图片来检测我们的程序可以检测到菠萝并且不会把菠萝当成苹果。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=256\sphinxpxdimen]{{ros-pineapple}.jpg}
\caption{菠萝图片}\label{\detokenize{chapter_rl_sys/perception_code_ex:id7}}\label{\detokenize{chapter_rl_sys/perception_code_ex:ros2-pineapple}}\end{figure}

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=256\sphinxpxdimen]{{ros-apple}.jpg}
\caption{苹果图片}\label{\detokenize{chapter_rl_sys/perception_code_ex:id8}}\label{\detokenize{chapter_rl_sys/perception_code_ex:ros2-apple}}\end{figure}


\subsection{添加代码}
\label{\detokenize{chapter_rl_sys/perception_code_ex:id4}}
\sphinxAtStartPar
之前创建Python库的命令应该已经帮我们创建好了\sphinxcode{\sphinxupquote{src/object\_detector/object\_detector/object\_detector\_node.py}}这个文件。现在让我们用以下内容来替换掉此文件中已有的内容。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{rclpy}
\PYG{k+kn}{from} \PYG{n+nn}{rclpy}\PYG{n+nn}{.}\PYG{n+nn}{node} \PYG{k+kn}{import} \PYG{n}{Node}

\PYG{k+kn}{from} \PYG{n+nn}{std\PYGZus{}msgs}\PYG{n+nn}{.}\PYG{n+nn}{msg} \PYG{k+kn}{import} \PYG{n}{Bool}
\PYG{k+kn}{from} \PYG{n+nn}{sensor\PYGZus{}msgs}\PYG{n+nn}{.}\PYG{n+nn}{msg} \PYG{k+kn}{import} \PYG{n}{Image}
\PYG{k+kn}{import} \PYG{n+nn}{cv2}
\PYG{k+kn}{from} \PYG{n+nn}{cv\PYGZus{}bridge} \PYG{k+kn}{import} \PYG{n}{CvBridge}

\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{import} \PYG{n+nn}{torchvision}\PYG{n+nn}{.}\PYG{n+nn}{models} \PYG{k}{as} \PYG{n+nn}{models}
\PYG{k+kn}{from} \PYG{n+nn}{torchvision} \PYG{k+kn}{import} \PYG{n}{transforms}


\PYG{k}{class} \PYG{n+nc}{ObjectDetectorNode}\PYG{p}{(}\PYG{n}{Node}\PYG{p}{)}\PYG{p}{:}

    \PYG{n}{PINEAPPLE\PYGZus{}CLASS\PYGZus{}ID} \PYG{o}{=} \PYG{l+m+mi}{953}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{object\PYGZus{}detector\PYGZus{}node}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{detection\PYGZus{}publisher} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{create\PYGZus{}publisher}\PYG{p}{(}\PYG{n}{Bool}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{object\PYGZus{}detected}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{camera\PYGZus{}subscriber} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{create\PYGZus{}subscription}\PYG{p}{(}
            \PYG{n}{Image}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{camera\PYGZus{}topic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{camera\PYGZus{}callback}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,}
        \PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alex\PYGZus{}net} \PYG{o}{=} \PYG{n}{models}\PYG{o}{.}\PYG{n}{alexnet}\PYG{p}{(}\PYG{n}{pretrained}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alex\PYGZus{}net}\PYG{o}{.}\PYG{n}{eval}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{preprocess} \PYG{o}{=} \PYG{n}{transforms}\PYG{o}{.}\PYG{n}{Compose}\PYG{p}{(}\PYG{p}{[}
            \PYG{n}{transforms}\PYG{o}{.}\PYG{n}{ToPILImage}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
            \PYG{n}{transforms}\PYG{o}{.}\PYG{n}{Resize}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{)}\PYG{p}{,}
            \PYG{n}{transforms}\PYG{o}{.}\PYG{n}{CenterCrop}\PYG{p}{(}\PYG{l+m+mi}{224}\PYG{p}{)}\PYG{p}{,}
            \PYG{n}{transforms}\PYG{o}{.}\PYG{n}{ToTensor}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
            \PYG{n}{transforms}\PYG{o}{.}\PYG{n}{Normalize}\PYG{p}{(}\PYG{n}{mean}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.485}\PYG{p}{,} \PYG{l+m+mf}{0.456}\PYG{p}{,} \PYG{l+m+mf}{0.406}\PYG{p}{]}\PYG{p}{,} \PYG{n}{std}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.229}\PYG{p}{,} \PYG{l+m+mf}{0.224}\PYG{p}{,} \PYG{l+m+mf}{0.225}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
        \PYG{p}{]}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cv\PYGZus{}bridge} \PYG{o}{=} \PYG{n}{CvBridge}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{declare\PYGZus{}parameter}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{detection\PYGZus{}class\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{PINEAPPLE\PYGZus{}CLASS\PYGZus{}ID}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Detector node is ready.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{camera\PYGZus{}callback}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{msg}\PYG{p}{:} \PYG{n}{Image}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Received an image, ready to detect!}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{detection\PYGZus{}class\PYGZus{}id} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}parameter}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{detection\PYGZus{}class\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{get\PYGZus{}parameter\PYGZus{}value}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{integer\PYGZus{}value}
        \PYG{n}{img} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cv\PYGZus{}bridge}\PYG{o}{.}\PYG{n}{imgmsg\PYGZus{}to\PYGZus{}cv2}\PYG{p}{(}\PYG{n}{msg}\PYG{p}{)}
        \PYG{n}{input\PYGZus{}batch} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{preprocess}\PYG{p}{(}\PYG{n}{img}\PYG{p}{)}\PYG{o}{.}\PYG{n}{unsqueeze}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{img\PYGZus{}output} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alex\PYGZus{}net}\PYG{p}{(}\PYG{n}{input\PYGZus{}batch}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{detection} \PYG{o}{=} \PYG{n}{Bool}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{detection}\PYG{o}{.}\PYG{n}{data} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{img\PYGZus{}output}\PYG{p}{)}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)} \PYG{o}{==} \PYG{n}{detection\PYGZus{}class\PYGZus{}id}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{detection\PYGZus{}publisher}\PYG{o}{.}\PYG{n}{publish}\PYG{p}{(}\PYG{n}{detection}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Detected: }\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{detection}\PYG{o}{.}\PYG{n}{data}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{, target class id: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{detection\PYGZus{}class\PYGZus{}id}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}


\PYG{k}{def} \PYG{n+nf}{main}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{init}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{n}{args}\PYG{p}{)}
    \PYG{n}{object\PYGZus{}detector\PYGZus{}node} \PYG{o}{=} \PYG{n}{ObjectDetectorNode}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{spin}\PYG{p}{(}\PYG{n}{object\PYGZus{}detector\PYGZus{}node}\PYG{p}{)}
    \PYG{n}{object\PYGZus{}detector\PYGZus{}node}\PYG{o}{.}\PYG{n}{destroy\PYGZus{}node}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{shutdown}\PYG{p}{(}\PYG{p}{)}


\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{n}{main}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
可能有些细心的读者已经发现了，这段代码和我们之前创建ROS2节点的代码非常像。
实际上这段代码就是创建了一个新的节点类来完成我们的功能。

\sphinxAtStartPar
这个节点类的实例将被赋予名字\sphinxcode{\sphinxupquote{object\_detector\_node}}，同时它将订阅\sphinxcode{\sphinxupquote{camera\_topic}}这个主题并发布\sphinxcode{\sphinxupquote{Bool}}类型的信息至\sphinxcode{\sphinxupquote{object\_detected}}主题。
其中，\sphinxcode{\sphinxupquote{camera\_topic}}主题的内容是机器人的摄像头传感器所接收到的视频流，而我们将用\sphinxcode{\sphinxupquote{image\_publisher}}库和之前的图片来模拟这个视频流。
而\sphinxcode{\sphinxupquote{object\_detected}}主题则将包含我们的检测结果以供机器人逻辑链的后续节点使用。
如果我们检测到菠萝，则我们将发布\sphinxcode{\sphinxupquote{True}}信息，否则就发布\sphinxcode{\sphinxupquote{False}}信息。

\sphinxAtStartPar
下面，让我们关注这个新节点类中的一些新细节。

\sphinxAtStartPar
首先，我们引入了\sphinxcode{\sphinxupquote{cv\_bridge.CvBridge}}这个类。
这个类是ROS2框架内的一个功能类，主要帮我们把图片在\sphinxcode{\sphinxupquote{opencv}}/\sphinxcode{\sphinxupquote{numpy}}格式和ROS2自己的\sphinxcode{\sphinxupquote{sensor\_msgs.msg.Image}}信息格式之间进行转换。
在我们新的节点类中我们可以看到它的具体用法（即\sphinxcode{\sphinxupquote{self.cv\_bridge = CvBridge()}}和\sphinxcode{\sphinxupquote{img = self.cv\_bridge.imgmsg\_to\_cv2(msg)}}）。

\sphinxAtStartPar
然后，在新的节点类\sphinxcode{\sphinxupquote{ObjectDetectorNode}}中，我们使用了\sphinxcode{\sphinxupquote{PINEAPPLE\_CLASS\_ID}}这个类成员变量来保存我们想要识别的物体在ImageNet中的类别ID（class
id）。这里\sphinxcode{\sphinxupquote{953}}是菠萝在ImageNet中的具体类别ID。

\sphinxAtStartPar
再之后，我们通过PyTorch实例化了一个预训练好的AlexNet，并将其设置到\sphinxcode{\sphinxupquote{eval}}状态。
同时，我们声明了\sphinxcode{\sphinxupquote{detection\_class\_id}}这个参数，以方便再运行时修改需要识别物体的类别ID（虽然这并不常用）。

\sphinxAtStartPar
最后，在\sphinxcode{\sphinxupquote{camera\_topic}}主题的回调函数\sphinxcode{\sphinxupquote{camera\_callback}}中，我们将收到的\sphinxcode{\sphinxupquote{Image}}类型信息传换成\sphinxcode{\sphinxupquote{numpy}}格式，然后调用AlexNet来进行物体识别，最后将识别结果以\sphinxcode{\sphinxupquote{Bool}}的形式发布到\sphinxcode{\sphinxupquote{object\_detected}}主题上去，并进行日志记录。

\sphinxAtStartPar
至此，一个使用PyTorch和AlexNet来识别摄像头中是否有菠萝的节点类就完成了。


\subsection{运行及检测}
\label{\detokenize{chapter_rl_sys/perception_code_ex:id5}}
\sphinxAtStartPar
下面，让我们尝试运行我们新写好的节点类并用菠萝和苹果的图片来检测这个节点类是否运行正常。

\sphinxAtStartPar
首先，让我们编译这个新写的Python库。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} openmlsys\PYGZhy{}ros2
colcon build \PYGZhy{}\PYGZhy{}symlink\PYGZhy{}install
\end{sphinxVerbatim}

\sphinxAtStartPar
在成功编译之后，我们可以新开一个终端窗口并执行下面的命令来运行一个节点类实例。
记住，你可能需要先运行\sphinxcode{\sphinxupquote{source install/local\_setup.zsh}}来引入我们自己的ROS2项目。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ros2 run object\PYGZus{}detector object\PYGZus{}detector\PYGZus{}node \PYGZhy{}\PYGZhy{}ros\PYGZhy{}args \PYGZhy{}r camera\PYGZus{}topic:\PYG{o}{=}image\PYGZus{}raw
\end{sphinxVerbatim}

\sphinxAtStartPar
如果你遇到\sphinxcode{\sphinxupquote{ModuleNotFoundError: No module named 'cv2'}}这之类的问题，则代表ROS2命令没有成功识别你Python虚拟环境中的程序库。这时候你可以尝试进入你所用的虚拟环境后执行下面这个命令。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nv}{PYTHONPATH}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{k}{\PYGZdl{}(}dirname \PYG{k}{\PYGZdl{}(}which python\PYG{k}{)}\PYG{k}{)}\PYG{l+s+s2}{/../lib/python3.8/site\PYGZhy{}packages:}\PYG{n+nv}{\PYGZdl{}PYTHONPATH}\PYG{l+s+s2}{\PYGZdq{}} ros2 run object\PYGZus{}detector object\PYGZus{}detector\PYGZus{}node \PYGZhy{}\PYGZhy{}ros\PYGZhy{}args \PYGZhy{}r camera\PYGZus{}topic:\PYG{o}{=}image\PYGZus{}raw
\end{sphinxVerbatim}

\sphinxAtStartPar
这个命令前面\sphinxcode{\sphinxupquote{PYTHONPATH="\$(dirname \$(which python))/../lib/python3.8/site\sphinxhyphen{}packages:\$PYTHONPATH"}}这一串的作用是将你目前Python环境的库添加到\sphinxcode{\sphinxupquote{PYTHONPATH}}，这样ROS2命令的Python就可以找到你目前Python环境（即ROS2项目所对应的Python虚拟环境）中的Python库了。

\sphinxAtStartPar
当这个ROS2命令成功运行时，你应该能看到这行信息：\sphinxcode{\sphinxupquote{{[}INFO{]} {[}1655172977.491378700{]} {[}object\_detector\_node{]}: Detector node is ready.}}。

\sphinxAtStartPar
另外，在这个ROS2命令中，我们使用了\sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}ros\sphinxhyphen{}args \sphinxhyphen{}r camera\_topic:=image\_raw}}这一系列参数。
这些参数是用来告诉ROS2将我们新节点类所使用的\sphinxcode{\sphinxupquote{camera\_topic}}主题重映射（remap）到\sphinxcode{\sphinxupquote{image\_raw}}这个主题上。
这样一来，我们新节点类所有使用\sphinxcode{\sphinxupquote{camera\_topic}}主题的场合实际上都是在使用\sphinxcode{\sphinxupquote{image\_raw}}这个主题。
使用主题名字重映射的好处是在于解耦合。
对于每个新的ROS2程序库或者每个新的节点类，我们都可以自由的命名我们要使用的主题的名字，然后当它需要和其它组件组合起来发挥作用时，只需要使用重映射将两个不同组件所使用的不同主题名字连接起来，就可以达到数据在两个组件之间正常流通的效果。
这实际上是ROS2框架的一个很实用的特性。

\sphinxAtStartPar
如果你想更深入的了解重映射相关的细节，可以阅读\sphinxhref{https://design.ros2.org/articles/static\_remapping.html}{这篇官方介绍}%
\begin{footnote}[96]\sphinxAtStartFootnote
\sphinxnolinkurl{https://design.ros2.org/articles/static\_remapping.html}
%
\end{footnote}。

\sphinxAtStartPar
在我们成功运行新节点后，让我们在一个新终端窗口中运行下面这行命令来测试它是否能检测到菠萝。同样的，你可能需要先运行\sphinxcode{\sphinxupquote{source install/local\_setup.zsh}}来引入我们自己的ROS2项目。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nv}{PYTHONPATH}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{k}{\PYGZdl{}(}dirname \PYG{k}{\PYGZdl{}(}which python\PYG{k}{)}\PYG{k}{)}\PYG{l+s+s2}{/../lib/python3.8/site\PYGZhy{}packages:}\PYG{n+nv}{\PYGZdl{}PYTHONPATH}\PYG{l+s+s2}{\PYGZdq{}} ros2 run image\PYGZus{}publisher image\PYGZus{}publisher\PYGZus{}node data/ros\PYGZhy{}pineapple.jpg \PYGZhy{}\PYGZhy{}ros\PYGZhy{}args \PYGZhy{}p publish\PYGZus{}rate:\PYG{o}{=}\PYG{l+m}{1}.0
\end{sphinxVerbatim}

\sphinxAtStartPar
上面这行命令将会使用\sphinxcode{\sphinxupquote{image\_publisher}}库和它的节点来以1Hz的频率将之前准备好的菠萝图片发布到\sphinxcode{\sphinxupquote{image\_raw}}这个主题上去。
当这个\sphinxcode{\sphinxupquote{image\_publisher\_node}}节点成功运行后，我们应当能在\sphinxcode{\sphinxupquote{object\_detector\_node}}节点运行的终端窗口中看到类似\sphinxcode{\sphinxupquote{{[}INFO{]} {[}1655174212.930385900{]} {[}object\_detector\_node{]}: Detected: "True", target class id: 953}}这样的信息来证明我们的节点类能够检测到菠萝。

\sphinxAtStartPar
接着让我们在\sphinxcode{\sphinxupquote{image\_publisher\_node}}节点的窗口中使用\sphinxcode{\sphinxupquote{Ctrl+C}}结束掉节点，然后使用下面这行命令来发布准备好的苹果图片。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nv}{PYTHONPATH}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{k}{\PYGZdl{}(}dirname \PYG{k}{\PYGZdl{}(}which python\PYG{k}{)}\PYG{k}{)}\PYG{l+s+s2}{/../lib/python3.8/site\PYGZhy{}packages:}\PYG{n+nv}{\PYGZdl{}PYTHONPATH}\PYG{l+s+s2}{\PYGZdq{}} ros2 run image\PYGZus{}publisher image\PYGZus{}publisher\PYGZus{}node data/ros\PYGZhy{}apple.jpg \PYGZhy{}\PYGZhy{}ros\PYGZhy{}args \PYGZhy{}p publish\PYGZus{}rate:\PYG{o}{=}\PYG{l+m}{1}.0
\end{sphinxVerbatim}

\sphinxAtStartPar
现在，在\sphinxcode{\sphinxupquote{object\_detector\_node}}节点运行的终端窗口中我们应该看到的是类似\sphinxcode{\sphinxupquote{{[}INFO{]} {[}1655171989.912783400{]} {[}object\_detector\_node{]}: Detected: "False", target class id: 953}}这样的信息来证明我们的节点类不会把苹果识别成菠萝。


\subsection{小结}
\label{\detokenize{chapter_rl_sys/perception_code_ex:id6}}
\sphinxAtStartPar
恭喜，你已经成功了解如何在ROS2项目中使用ROS2框架外的Python库了！
如果你使用Python虚拟环境，你可能需要额外的设置\sphinxcode{\sphinxupquote{PYTHONPATH}}环境变量。
另外，主题名字重映射（Name Remapping）是一个很有用的ROS2特性。
你在以后的项目中很可能会经常用到它。


\section{规划系统}
\label{\detokenize{chapter_rl_sys/planning:id1}}\label{\detokenize{chapter_rl_sys/planning::doc}}
\sphinxAtStartPar
规划不仅包含运动路径规划，还包含高级任务规划
\sphinxcite{chapter_rl_sys/summary:id5}。其中，运动规划是机器人技术的核心问题之一，应用范围从导航到复杂环境中的操作。它具有悠久的研究历史，方法需要有概率完整性和最优性的保证。然而，当经典运动规划在处理现实世界的机器人问题（在高维空间中）时，挑战仍然存在。研究人员在继续开发新算法来克服与这些方法相关的限制，包括优化计算和内存负载、更好的规划表示和处理维度灾难等。

\sphinxAtStartPar
相比之下，机器学习的最新进展为机器人专家研究运动规划问题开辟了新视角：经典运动规划器的瓶颈可以以数据驱动的方式解决；基于深度学习的规划器可以避免几何输入的局限性，例如使用视觉或语义输入进行规划等。最近的工作有：基于深度神经网络的四足机器人快速运动规划框架，通过贝叶斯学习进行运动规划，通过运动规划器指导的视觉运动策略学习。ML4KP是一个用于有效运动动力学运动规划的C++库，该库可以轻松地将机器学习方法集成到规划过程中。
自动驾驶领域和行人和车辆轨迹预测方面也涌现出使用机器学习解决运动规划的工作，比如斯坦福大学提出Trajectron++。强化学习在规划系统上也有重要应用
\sphinxcite{chapter_rl_sys/summary:sun2021adversarial}，比如基于MetaDrive模拟器
\sphinxcite{chapter_rl_sys/summary:li2021metadrive}，最近有一些关于多智能体强化学习，多智能体车流模拟、驾驶行为分析
\sphinxcite{chapter_rl_sys/summary:peng2021learning}，考虑安全性因素的强化学习
\sphinxcite{chapter_rl_sys/summary:peng2021safe}，以及拓展到由真人专家在旁边监督，出现危险的时候接管的专家参与的强化学习工作（Online
Imitation Learning、Offline RL）
\sphinxcite{chapter_rl_sys/summary:li2021efficient}，样本效率极高，是单纯强化学习算法的50倍。为了更好地说明强化学习是如何应用在自动驾驶中的， \hyperref[\detokenize{chapter_rl_sys/planning:rl-ad}]{图\ref{\detokenize{chapter_rl_sys/planning:rl-ad}}}展示了一个基于深度强化学习的自动驾驶POMDP模型。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{rl_ad}.png}
\caption{基于深度强化学习的自动驾驶POMDP模型 \sphinxcite{chapter_rl_sys/summary:aradi2020survey}}\label{\detokenize{chapter_rl_sys/planning:id9}}\label{\detokenize{chapter_rl_sys/planning:rl-ad}}\end{figure}


\section{规划系统案例}
\label{\detokenize{chapter_rl_sys/planning_code_ex:id1}}\label{\detokenize{chapter_rl_sys/planning_code_ex::doc}}
\sphinxAtStartPar
在上一章节中，我们初步了解了机器人的规划系统。
这一章节中，我们将通过一个简单的案例来演示怎样结合ROS2和机器学习框架scikit\sphinxhyphen{}learn来完成一个我们设想的规划系统中的一个基本功能。
我们将使用和\sphinxhref{./perception\_code\_ex.md}{感知系统案例}这一章节类似的方法和结构来讲解本章节。


\subsection{案例背景}
\label{\detokenize{chapter_rl_sys/planning_code_ex:id2}}
\sphinxAtStartPar
假设我们想要帮某花园设计一款打理鸢尾花的园丁机器人。
很“碰巧”的是，这个小花园里面正好只有经典的\sphinxhref{https://scikit-learn.org/stable/auto\_examples/datasets/plot\_iris\_dataset.html}{鸢尾花数据集}%
\begin{footnote}[97]\sphinxAtStartFootnote
\sphinxnolinkurl{https://scikit-learn.org/stable/auto\_examples/datasets/plot\_iris\_dataset.html}
%
\end{footnote}中的那三种鸢尾花，而且已经有人帮我们完成了一个“魔术般的”ROS2感知组件来自动的检测目标鸢尾花的花萼长和宽以及花瓣长和宽（Sepal
Length, Sepal Width, Petal Length and Petal
Width：鸢尾花数据集所需要的4个输入维度）。
同时因为机器人的性能限制，我们不能使用比较复杂的模型（例如神经网络）。
这种情况下，我们可以尝试使用经典的机器学习模型，例如决策树，来接受感知组件的结果并识别鸢尾花的类别，然后用一个映射表（mapping
table）来查找出我们应该为机器人规划怎样的行为去执行。
当季节或情况改变时，花园的技术团队可以更新映射表来更改机器人的规划系统逻辑。

\sphinxAtStartPar
当然，上面的案例背景和解决方案都是为了生成一个简单的案例而设计的“非现实”的例子。
大家在现实项目中遇到的案例应该会复杂的多。
不过，我们任然希望这样一个简单的案例可以为大家带来些许价值。

\sphinxAtStartPar
让我们回到我们刚刚介绍的解决方案中。
在之前的感知系统的案例中，我们选择使用ROS2节点类来处理感知任务。
这是因为机器人会不断的接收到传感器的信号，而我们希望尽可能多的处理收到的信号。
而对于我们这一章节的案例来说，因为我们不一定需要不间断的进行新的规划，同时每一次规划我们都期待有一个结果，所以使用ROS2服务可能会是一个更好的选择。

\sphinxAtStartPar
和之前的案例类似，本章节的案例所使用的代码可以在本书相关的\sphinxhref{https://github.com/openmlsys/openmlsys-ros2}{ROS2案例代码库}%
\begin{footnote}[98]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/openmlsys/openmlsys-ros2}
%
\end{footnote}中的\sphinxcode{\sphinxupquote{src/action\_decider}}文件夹内找到。


\subsection{项目搭建}
\label{\detokenize{chapter_rl_sys/planning_code_ex:id3}}
\sphinxAtStartPar
让我们继续沿用之前已经搭建好的ROS2项目框架。
和感知系统案例类似，我们只需在其中增加一个ROS2的Python库来实现我们想要的功能即可。
因此，让我们回到\sphinxcode{\sphinxupquote{src}}目录下并创建此Python库。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} openmlsys\PYGZhy{}ros2/src
ros2 pkg create \PYGZhy{}\PYGZhy{}build\PYGZhy{}type ament\PYGZus{}python \PYGZhy{}\PYGZhy{}node\PYGZhy{}name action\PYGZus{}decider\PYGZus{}node action\PYGZus{}decider \PYGZhy{}\PYGZhy{}dependencies rclpy std\PYGZus{}msgs scikit\PYGZhy{}learn my\PYGZus{}interfaces
\end{sphinxVerbatim}

\sphinxAtStartPar
我们将\sphinxcode{\sphinxupquote{my\_interfaces}}添加为依赖项是因为我们需要为新的ROS2服务创建对应的消息类型接口。

\sphinxAtStartPar
在创建好Python库后，别忘了将\sphinxcode{\sphinxupquote{package.xml}}和\sphinxcode{\sphinxupquote{setup.py}}中的\sphinxcode{\sphinxupquote{version}}，\sphinxcode{\sphinxupquote{maintainer}}，\sphinxcode{\sphinxupquote{maintainer\_email}}，\sphinxcode{\sphinxupquote{description}}和\sphinxcode{\sphinxupquote{license}}项都更新好。

\sphinxAtStartPar
接下来，让我们在ROS2项目的Python虚拟环境中安装\sphinxcode{\sphinxupquote{scikit\sphinxhyphen{}learn}}。
例如使用\sphinxcode{\sphinxupquote{pipenv}}的用户可能会执行\sphinxcode{\sphinxupquote{pipenv install scikit\sphinxhyphen{}learn}}这条命令。


\subsection{添加消息类型接口}
\label{\detokenize{chapter_rl_sys/planning_code_ex:id4}}
\sphinxAtStartPar
我们将要编写的新ROS2服务需要有它自己的服务消息接口。
让我们借用已有的\sphinxcode{\sphinxupquote{my\_interfaces}}库来放置这个新接口。

\sphinxAtStartPar
首先，让我们在\sphinxcode{\sphinxupquote{openmlsys\sphinxhyphen{}ros2/src/my\_interfaces/srv}}中新建一个名为\sphinxcode{\sphinxupquote{IrisData.srv}}的文件并用下面的内容填充它。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
float32 sepal\PYGZus{}length
float32 sepal\PYGZus{}width
float32 petal\PYGZus{}length
float32 petal\PYGZus{}width
\PYGZhy{}\PYGZhy{}\PYGZhy{}
string action
\end{sphinxVerbatim}

\sphinxAtStartPar
我们可以看到，新的ROS2服务将会接受4个浮点值作为输入。
这4个浮点值分别为鸢尾花的花萼的长和宽还有花瓣的长和宽。
当规划完成后，服务会返回一个字符串。
这个字符串将会是机器人需要执行的动作的名称。

\sphinxAtStartPar
我们还需要在\sphinxcode{\sphinxupquote{my\_interfaces}}库的\sphinxcode{\sphinxupquote{CMakeLists.txt}}文件中的相应位置（\sphinxcode{\sphinxupquote{rosidl\_generate\_interfaces}}函数的参数部分）添加一行新的内容：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdq{}srv/IrisData.srv\PYGZdq{}
\end{sphinxVerbatim}

\sphinxAtStartPar
最后，别忘了在ROS2项目的根目录下执行\sphinxcode{\sphinxupquote{colcon build \sphinxhyphen{}\sphinxhyphen{}packages\sphinxhyphen{}select my\_interfaces}}来重新编译\sphinxcode{\sphinxupquote{my\_interfaces}}这个库。


\subsection{添加代码}
\label{\detokenize{chapter_rl_sys/planning_code_ex:id5}}
\sphinxAtStartPar
之前创建Python库的命令应该已经帮我们创建好了\sphinxcode{\sphinxupquote{src/action\_decider/action\_decider/action\_decider\_node.py}}这个文件。现在让我们用以下内容来替换掉此文件中已有的内容。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{pickle}

\PYG{k+kn}{import} \PYG{n+nn}{rclpy}
\PYG{k+kn}{from} \PYG{n+nn}{rclpy}\PYG{n+nn}{.}\PYG{n+nn}{node} \PYG{k+kn}{import} \PYG{n}{Node}

\PYG{k+kn}{from} \PYG{n+nn}{std\PYGZus{}msgs}\PYG{n+nn}{.}\PYG{n+nn}{msg} \PYG{k+kn}{import} \PYG{n}{String}
\PYG{k+kn}{from} \PYG{n+nn}{my\PYGZus{}interfaces}\PYG{n+nn}{.}\PYG{n+nn}{srv} \PYG{k+kn}{import} \PYG{n}{IrisData}

\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{load\PYGZus{}iris}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn} \PYG{k+kn}{import} \PYG{n}{tree}


\PYG{k}{def} \PYG{n+nf}{main}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{init}\PYG{p}{(}\PYG{n}{args}\PYG{o}{=}\PYG{n}{args}\PYG{p}{)}
    \PYG{n}{action\PYGZus{}decider\PYGZus{}service} \PYG{o}{=} \PYG{n}{ActionDeciderService}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{spin}\PYG{p}{(}\PYG{n}{action\PYGZus{}decider\PYGZus{}service}\PYG{p}{)}
    \PYG{n}{action\PYGZus{}decider\PYGZus{}service}\PYG{o}{.}\PYG{n}{destroy\PYGZus{}node}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{rclpy}\PYG{o}{.}\PYG{n}{shutdown}\PYG{p}{(}\PYG{p}{)}


\PYG{k}{class} \PYG{n+nc}{ActionDeciderService}\PYG{p}{(}\PYG{n}{Node}\PYG{p}{)}\PYG{p}{:}

    \PYG{n}{IRIS\PYGZus{}CLASSES} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{setosa}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{versicolor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{virginica}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

    \PYG{n}{IRIS\PYGZus{}ACTION\PYGZus{}MAP} \PYG{o}{=} \PYG{p}{\PYGZob{}}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{setosa}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fertilise}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{versicolor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{idle}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{virginica}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{prune}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{DEFAULT\PYGZus{}MODEL\PYGZus{}PATH} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{dirname}\PYG{p}{(}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}file\PYGZus{}\PYGZus{}}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{/../../../data/iris\PYGZus{}model.pickle}\PYG{l+s+s1}{\PYGZsq{}}

    \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}iris\PYGZus{}classifier}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{model\PYGZus{}path}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{isfile}\PYG{p}{(}\PYG{n}{model\PYGZus{}path}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{n}{model\PYGZus{}path}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{model\PYGZus{}file}\PYG{p}{:}
                \PYG{k}{return} \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{model\PYGZus{}file}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Cannot find trained model at }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{model\PYGZus{}path}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{, will train a new model.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n}{iris} \PYG{o}{=} \PYG{n}{load\PYGZus{}iris}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{X}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{iris}\PYG{o}{.}\PYG{n}{data}\PYG{p}{,} \PYG{n}{iris}\PYG{o}{.}\PYG{n}{target}
        \PYG{n}{clf} \PYG{o}{=} \PYG{n}{tree}\PYG{o}{.}\PYG{n}{DecisionTreeClassifier}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{clf} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
        \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{n}{model\PYGZus{}path}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{model\PYGZus{}file}\PYG{p}{:}
            \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{clf}\PYG{p}{,} \PYG{n}{model\PYGZus{}file}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{clf}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris\PYGZus{}action\PYGZus{}decider\PYGZus{}service}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{srv} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{create\PYGZus{}service}\PYG{p}{(}\PYG{n}{IrisData}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris\PYGZus{}action\PYGZus{}decider}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{decide\PYGZus{}iris\PYGZus{}action\PYGZus{}callback}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{iris\PYGZus{}classifier} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}iris\PYGZus{}classifier}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{DEFAULT\PYGZus{}MODEL\PYGZus{}PATH}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Iris action decider service is ready.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{decide\PYGZus{}iris\PYGZus{}action\PYGZus{}callback}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{request}\PYG{p}{,} \PYG{n}{response}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{iris\PYGZus{}data} \PYG{o}{=} \PYG{p}{[}\PYG{n}{request}\PYG{o}{.}\PYG{n}{sepal\PYGZus{}length}\PYG{p}{,} \PYG{n}{request}\PYG{o}{.}\PYG{n}{sepal\PYGZus{}width}\PYG{p}{,} \PYG{n}{request}\PYG{o}{.}\PYG{n}{petal\PYGZus{}length}\PYG{p}{,} \PYG{n}{request}\PYG{o}{.}\PYG{n}{petal\PYGZus{}width}\PYG{p}{]}
        \PYG{n}{iris\PYGZus{}class\PYGZus{}idx} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{iris\PYGZus{}classifier}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{p}{[}\PYG{n}{iris\PYGZus{}data}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{iris\PYGZus{}class} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{IRIS\PYGZus{}CLASSES}\PYG{p}{[}\PYG{n}{iris\PYGZus{}class\PYGZus{}idx}\PYG{p}{]}
        \PYG{n}{response}\PYG{o}{.}\PYG{n}{action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{IRIS\PYGZus{}ACTION\PYGZus{}MAP}\PYG{p}{[}\PYG{n}{iris\PYGZus{}class}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}logger}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}
            \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Incoming request}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{sepal\PYGZus{}length: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{request}\PYG{o}{.}\PYG{n}{sepal\PYGZus{}length}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{sepal\PYGZus{}width: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{request}\PYG{o}{.}\PYG{n}{sepal\PYGZus{}width}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}
            \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{petal\PYGZus{}length: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{request}\PYG{o}{.}\PYG{n}{petal\PYGZus{}length}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{petal\PYGZus{}width: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{request}\PYG{o}{.}\PYG{n}{petal\PYGZus{}width}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}
            \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{iris class: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{iris\PYGZus{}class}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}
            \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{decided action: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{response}\PYG{o}{.}\PYG{n}{action}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}
        \PYG{p}{)}

        \PYG{k}{return} \PYG{n}{response}


\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{n}{main}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
细心的读者可能已经发现了，这段代码和我们之前创建的使用ROS2服务的服务端节点类的代码非常像。
实际上这段代码就是使用了同样的服务端节点类框架和一个新的服务来完成我们想要的功能。

\sphinxAtStartPar
这个服务端节点类的实例将被赋予名字\sphinxcode{\sphinxupquote{iris\_action\_decider\_service}}，它将提供一个名为\sphinxcode{\sphinxupquote{iris\_action\_decider}}的服务并且这个服务期待\sphinxcode{\sphinxupquote{IrisData}}格式的服务请求（即我们之前定义的消息类型接口的请求部分）。
当服务计算完成后，它将把结果返回给请求发起方。
这个结果是规划好的行为的名字并被封装到\sphinxcode{\sphinxupquote{IrisData}}格式的服务结果中去（即我们之前定义的消息类型接口的结果部分）。

\sphinxAtStartPar
下面，让我们关注这个新节点类中的一些新细节。

\sphinxAtStartPar
首先，我们在新的服务端节点类\sphinxcode{\sphinxupquote{ActionDeciderService}}中声明了三个类成员变量\sphinxcode{\sphinxupquote{IRIS\_CLASSES}}，\sphinxcode{\sphinxupquote{IRIS\_ACTION\_MAP}}和\sphinxcode{\sphinxupquote{DEFAULT\_MODEL\_PATH}}。
它们分别表示鸢尾花的类别标签，鸢尾花类别至机器人行动名称的映射表，和默认存放训练好的决策树模型的路径。

\sphinxAtStartPar
当我们的服务端节点类初始化时，它将调用\sphinxcode{\sphinxupquote{get\_iris\_classifier()}}来读取训练好的决策树模型。
如果模型文件缺失，则会重新训练一个模型并保存。
这里我们把训练模型的代码放到了同一个节点内。
实际上，对于大型项目或大型模型，我们可以把模型训练和模型使用分开到不同的组件中去，并且它们可能在不同的时机运行。

\sphinxAtStartPar
当服务的回调函数\sphinxcode{\sphinxupquote{decide\_iris\_action\_callback()}}被调用时，服务将会使用训练好的模型和接收到的鸢尾花信息来预测鸢尾花的类别，然后通过查找映射表来决定机器人需要执行的动作。最后服务返回结果并进行日志记录。

\sphinxAtStartPar
至此，一个使用scikit\sphinxhyphen{}learn和决策树的简易“玩具级”规划组件就完成了。


\subsection{运行及检测}
\label{\detokenize{chapter_rl_sys/planning_code_ex:id6}}
\sphinxAtStartPar
下面，让我们尝试运行新写好的服务端节点类并检测它是否能正常运行。

\sphinxAtStartPar
首先，让我们编译这个新写的Python库。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} openmlsys\PYGZhy{}ros2
colcon build \PYGZhy{}\PYGZhy{}symlink\PYGZhy{}install
\end{sphinxVerbatim}

\sphinxAtStartPar
在成功编译之后，我们可以新开一个终端窗口并执行下面的命令来运行一个节点类实例。
记住，你可能需要先运行\sphinxcode{\sphinxupquote{source install/local\_setup.zsh}}来引入我们自己的ROS2项目。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ros2 run action\PYGZus{}decider action\PYGZus{}decider\PYGZus{}node
\end{sphinxVerbatim}

\sphinxAtStartPar
如果你使用了Python虚拟环境，则可以尝试下面这条命令，而不是上面那条。背后具体的原因已在之前的案例章节叙述过。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nv}{PYTHONPATH}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{k}{\PYGZdl{}(}dirname \PYG{k}{\PYGZdl{}(}which python\PYG{k}{)}\PYG{k}{)}\PYG{l+s+s2}{/../lib/python3.8/site\PYGZhy{}packages:}\PYG{n+nv}{\PYGZdl{}PYTHONPATH}\PYG{l+s+s2}{\PYGZdq{}} ros2 run action\PYGZus{}decider action\PYGZus{}decider\PYGZus{}node
\end{sphinxVerbatim}

\sphinxAtStartPar
当这个ROS2命令成功运行时，你应该能看到这行信息：\sphinxcode{\sphinxupquote{{[}INFO{]} {[}1655253519.693893500{]} {[}iris\_action\_decider\_service{]}: Iris action decider service is ready.}}。

\sphinxAtStartPar
在我们成功运行新的服务端节点后，让我们在一个新终端窗口中运行下面这行命令来测试新的服务是否能正常运行。同样的，你可能需要先运行\sphinxcode{\sphinxupquote{source install/local\_setup.zsh}}来引入我们自己的ROS2项目。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ros2 service call /iris\PYGZus{}action\PYGZus{}decider my\PYGZus{}interfaces/srv/IrisData \PYG{l+s+s2}{\PYGZdq{}\PYGZob{}sepal\PYGZus{}length: 1.0, sepal\PYGZus{}width: 2.0, petal\PYGZus{}length: 3.0, petal\PYGZus{}width: 4.0\PYGZcb{}\PYGZdq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
这里，我们用的\sphinxcode{\sphinxupquote{ros2 service call}}命令是专门用来通过命令行调用一个ROS2服务的命令。其中服务请求的数据应该是字符串化的YAML格式数据。这个命令更多的信息可以通过\sphinxcode{\sphinxupquote{ros2 service call \sphinxhyphen{}h}}来查阅。

\sphinxAtStartPar
一切顺利的话，执行完命令后不久，你应该就能在新窗口中很快看到类似这样的信息了：\sphinxcode{\sphinxupquote{response: my\_interfaces.srv.IrisData\_Response(action='prune')}}。


\subsection{小结}
\label{\detokenize{chapter_rl_sys/planning_code_ex:id7}}
\sphinxAtStartPar
恭喜，你已经成功了解如何在ROS2项目中使用scikit\sphinxhyphen{}learn这样库并训练一个模型了！


\section{控制系统}
\label{\detokenize{chapter_rl_sys/control:id1}}\label{\detokenize{chapter_rl_sys/control::doc}}
\sphinxAtStartPar
虽然控制理论已牢牢植根于基于模型（Model\sphinxhyphen{}based）的设计传统，但丰富的数据和机器学习给控制理论带来了新的机遇。控制理论和机器学习的交叉点涵盖了广泛的研究方向，包括但不限于动态系统的学习、在线学习和控制、深度学习的控制理论观点、强化学习以及在各种现实世界系统中的应用。
从机器学习的角度来看，未来的主要挑战之一是超越模式识别并解决数据驱动控制和动态过程优化方面的问题。

\sphinxAtStartPar
理论方面，线性二次控制（Linear\sphinxhyphen{}Quadratic
Control）是经典的控制方法，最近有关于图神经网络在分布式线性二次控制的研究。作者称将线性二次问题转换为自监督学习问题，能够找到基于图神经网络（Graph
Neural
Networks，GNN）的最佳分布式控制器，他们还推导出了所得闭环系统稳定的充分条件。随着基于数据和学习的机器人控制方法不断得到重视，研究人员必须了解何时以及如何在现实世界中最好地利用这些方法，因为安全是至关重要的，有的研究通过学习不确定的动力学来安全地提高性能，鼓励安全或稳健的强化学习方法，以及可以正式认证所学控制策略的安全性的方法。
\hyperref[\detokenize{chapter_rl_sys/control:safe-learning-control}]{图\ref{\detokenize{chapter_rl_sys/control:safe-learning-control}}}展示了安全学习控制（Safe Learning
Control）系统的框架图，用数据驱动的方法来学习控制策略，兼顾安全性。Lyapunov
函数是评估非线性动力系统稳定性的有效工具，最近有人提出Neural
Lyapunov来将安全性纳入考虑。

\sphinxAtStartPar
应用方面，有基于神经网络的自动驾驶汽车模型预测控制，也有研究将最优控制和学习相结合并应用在陌生环境中的视觉导航，该研究将基于模型的控制与基于学习的感知相结合来解决。基于学习的感知模块产生一系列航路点通过无碰撞路径引导机器人到达目标。基于模型的规划器使用这些航路点来生成平滑且动态可行的轨迹，该轨迹使用反馈控制在物理系统上执行。在模拟的现实世界杂乱环境和实际地面车辆上的实验表明，与纯粹基于几何映射或基于端到端学习的替代方案相比，这种新的系统可以在新环境中更可靠、更有效地到达目标位置。强化学习和模仿学习与控制论有密切联系：LEOC整合了强化学习和经典控制理论的原则方法。有人将基于模型的离线强化学习算法扩展到高维视觉观察空间并在真实机器人上执行基于图像的抽屉关闭任务方面表现出色。控制部分通过神经网络优化可以更加平滑、节能、安全，如何将
神经网络和传统控制理论结合，特别是和运动学算法相结合，将会是一个有趣的方向。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{safe_learning_control}.png}
\caption{安全学习控制系统，数据被用来更新控制策略或或安全滤波器
\sphinxcite{chapter_rl_sys/summary:brunke2021safe}}\label{\detokenize{chapter_rl_sys/control:id3}}\label{\detokenize{chapter_rl_sys/control:safe-learning-control}}\end{figure}


\section{控制系统案例}
\label{\detokenize{chapter_rl_sys/control_code_ex:id1}}\label{\detokenize{chapter_rl_sys/control_code_ex::doc}}
\sphinxAtStartPar
在上一章节中，我们初步了解了机器人的控制系统，同时也知道了机器学习在机器人控制系统这个领域有着很多有趣和有前景的研究方向。
只不过由于控制系统的复杂性和这些研究的前瞻性，它们不太适合用来作为简单的案例。

\sphinxAtStartPar
与此同时，ROS作为一个成熟的机器人框架，它已经包含了很多成熟稳定的经典控制组件。
这些控制组件和其他的成熟的功能模块一起组成了更大规模的功能模组，来完成更复杂的任务。

\sphinxAtStartPar
在这些更大规模的功能模组中，\sphinxstylestrong{Nav2}和\sphinxstylestrong{MoveIt2}可能是最常用的两个。

\sphinxAtStartPar
从名字上就可以看出来，这两个功能模组各自都是它们ROS1版本的继承者。
Nav2是ROS Navigation
Stack在ROS2中的继承者，专注于移动机器人的导航相关的功能，例如定位，路径规划等，并致力于用安全的方式将机器人从一点移动到另一点。
MoveIt2是ROS
MoveIt在ROS2中的继承者，致力于打造一个容易使用的机器人操纵平台。带机械臂的机器人都基本离不开它。

\sphinxAtStartPar
这两个模组都成熟，可靠，和容易使用。使用ROS框架开发机器人是基本上都会直接使用它们或者在它们已有功能的基础上做适合自己的自定义修改，以避免重复造轮子。

\sphinxAtStartPar
因此，在本章节中，我们将以Nav2为案例，来带领大家初步了解怎样使用一个大型的ROS2功能模组。

\sphinxAtStartPar
本章节的内容很大程度参考了Nav2的\sphinxhref{https://navigation.ros.org/}{英文官方文档}%
\begin{footnote}[99]\sphinxAtStartFootnote
\sphinxnolinkurl{https://navigation.ros.org/}
%
\end{footnote}，尤其是“Getting
Started”这一章。对自己英文有信心的读者可以尝试阅读官方文档以了解更多细节。

\sphinxAtStartPar
本章没有额外的代码案例。


\subsection{安装}
\label{\detokenize{chapter_rl_sys/control_code_ex:id2}}
\sphinxAtStartPar
首先，让我们通过Ubuntu的库管理器来安装Nav2相关的程序库。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
sudo apt install ros\PYGZhy{}foxy\PYGZhy{}navigation2 ros\PYGZhy{}foxy\PYGZhy{}nav2\PYGZhy{}bringup
\end{sphinxVerbatim}

\sphinxAtStartPar
其中\sphinxcode{\sphinxupquote{ros\sphinxhyphen{}foxy\sphinxhyphen{}navigation2}}是Nav2的核心程序库，而\sphinxcode{\sphinxupquote{ros\sphinxhyphen{}foxy\sphinxhyphen{}nav2\sphinxhyphen{}bringup}}则是Nav2的一个启动案例。
这个案例十分灵活，很多时候我们可以将其稍加修改后放到自己的项目中使用。

\sphinxAtStartPar
接下来让我们安装\sphinxcode{\sphinxupquote{turtlebot3}}相关的一系列程序库。
turtlebot系列是一个很成功的入门级移动机器人系列。
而这一系列程序库则提供了和turtlebot3机器人相关的组件，其中包含了在模拟环境中使用虚拟turtlebot3机器人的相关功能组件。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
sudo apt install \PYG{l+s+s2}{\PYGZdq{}ros\PYGZhy{}foxy\PYGZhy{}turtlebot3*\PYGZdq{}}
\end{sphinxVerbatim}


\subsection{运行}
\label{\detokenize{chapter_rl_sys/control_code_ex:id3}}
\sphinxAtStartPar
在安装好上面的那些程序库后，我们就可以尝试使用Nav2了。

\sphinxAtStartPar
首先，让我们新开一个终端窗口，并执行以下命令。这些命令分别导入了ROS2框架，并设定好了我们要使用哪个Turtlebot3模型和在哪儿搜索虚拟世界（Gazebo）需要的模型。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{source} /opt/ros/foxy/setup.bash
\PYG{n+nb}{export} \PYG{n+nv}{TURTLEBOT3\PYGZus{}MODEL}\PYG{o}{=}waffle
\PYG{n+nb}{export} \PYG{n+nv}{GAZEBO\PYGZus{}MODEL\PYGZus{}PATH}\PYG{o}{=}\PYG{n+nv}{\PYGZdl{}GAZEBO\PYGZus{}MODEL\PYGZus{}PATH}:/opt/ros/foxy/share/turtlebot3\PYGZus{}gazebo/models
\end{sphinxVerbatim}

\sphinxAtStartPar
现在，我们一切就绪，可以下面这行命令来运行一个Nav2的演示程序。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ros2 launch nav2\PYGZus{}bringup tb3\PYGZus{}simulation\PYGZus{}launch.py
\end{sphinxVerbatim}

\sphinxAtStartPar
其中\sphinxcode{\sphinxupquote{ros2 launch}}命令是用来执行一个launch文件，而后者则是将很多需要启动的ROS2组件集合到一起来按计划启动的一个说明文件。
一个机器人项目经常需要启动很多个不同的组件来配合完成任务。
而如果每个组件都要新开一个窗口执行命令的话，整个机器人的启动将会变得十分繁琐。
launch文件和\sphinxcode{\sphinxupquote{ros2 launch}}命令就是来解决这个问题的。
我们可以把整个ROS2项目想象成一个交响乐团，其中每个组件分别代表一个乐器。
而launch文件就像是乐团的指挥，负责调配每个乐器应该在什么时候启动。
总而言之，这是ROS2中一个非常使用的特性。

\sphinxAtStartPar
关于\sphinxcode{\sphinxupquote{ros2 launch}}命令和launch文件的更多细节，感兴趣的读者可以查阅\sphinxhref{https://docs.ros.org/en/foxy/Tutorials/Launch/Creating-Launch-Files.html}{官方英文文档}%
\begin{footnote}[100]\sphinxAtStartFootnote
\sphinxnolinkurl{https://docs.ros.org/en/foxy/Tutorials/Launch/Creating-Launch-Files.html}
%
\end{footnote}。

\sphinxAtStartPar
成功运行上述命令之后，我们应该会看到两个新开的GUI窗口，分别对应\sphinxcode{\sphinxupquote{RViz}}和\sphinxcode{\sphinxupquote{Gazebo}}程序。
其中\sphinxcode{\sphinxupquote{RViz}}是ROS2框架的可视化接口，我们稍后将通过它来控制我们的虚拟机器人。
而\sphinxcode{\sphinxupquote{Gazebo}}则是一个用过创建和运行虚拟世界的软件。
它独立于ROS2框架，但两者又互相紧密合作。

\sphinxAtStartPar
在\sphinxcode{\sphinxupquote{Gazebo}}窗口中（如下图所示），我们应该能够看到一个三维的类六边形虚拟世界。
这个世界中还有一个虚拟的Turtlebot3机器人。
这个机器人发射出很多蓝色的射线。
这些射线代表了机器人的激光雷达的读数射线。
而激光雷达的读数则被Nav2用来在环境中定位机器人。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{ros2-gazebo-1}.JPG}
\caption{Gazebo 截图 1}\label{\detokenize{chapter_rl_sys/control_code_ex:id4}}\end{figure}

\sphinxAtStartPar
在\sphinxcode{\sphinxupquote{RViz}}窗口中（如下图所示），我们应该能够看到虚拟世界的一个二维地图。
地图上的白色部分是机器人可以到达的部分，而黑色则是检测到的障碍物或墙。
如果你在左侧看到有红色的\sphinxcode{\sphinxupquote{Global Status: Error}}错误的话，你的机器人并没有在RViz（即ROS2框架）中正确的定位。
请在工具栏选择\sphinxcode{\sphinxupquote{2D Pose Estimate}}并在RViz地图上机器人应该在的位置（以Gazebo中机器人的位置为准）更新好机器人的姿态。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{ros2-rviz-1}.JPG}
\caption{RViz 截图 1}\label{\detokenize{chapter_rl_sys/control_code_ex:id5}}\end{figure}

\sphinxAtStartPar
更新好机器人的姿态后，RViz应该和下图比较相似。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{ros2-rviz-2}.JPG}
\caption{RViz 截图 2}\label{\detokenize{chapter_rl_sys/control_code_ex:id6}}\end{figure}

\sphinxAtStartPar
这样一来，我们的机器人就准备好在虚拟事件中移动了。

\sphinxAtStartPar
请在RViz的工具栏中选择\sphinxcode{\sphinxupquote{Navigation2 Goal}}按钮，并在地图上选择你想要Turtlebot3机器人最终所到达的位置和姿态。
一旦选好了，你将会看到机器人开始向目标位置移动并最终到达目标。

\sphinxAtStartPar
RViz还提供了很多其它的Nav2功能的按钮，你可以通过Nav2和ROS2的官方英文文档来了解更多使用方法。

\sphinxAtStartPar
恭喜，你现在初步了解了怎样使用ROS2框架内的大型功能模组！


\subsubsection{章节附录：在WSL中使用Nav2}
\label{\detokenize{chapter_rl_sys/control_code_ex:wslnav2}}
\sphinxAtStartPar
有些读者可能是通过Windows下的WSL（Windows Subsystem for
Linux）来运行ROS2的。
如果是这种情况，这一章节中的图形界面程序，如RViz和Gazebo，可能会造成问题。
这是因为WSL默认并不能打开图形界面程序。

\sphinxAtStartPar
幸运的是，我们可以更改设置来达到在WSL中运行图形界面程序这一点。
\sphinxhref{https://github.com/rhaschke/lecture/wiki/WSL-install}{这篇笔记}%
\begin{footnote}[101]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/rhaschke/lecture/wiki/WSL-install}
%
\end{footnote}介绍了其作者是如何在WSL中运行ROS2和图形界面的。其中第二点尤为值得注意。
而\sphinxhref{https://github.com/cascadium/wsl-windows-toolbar-launcher\#firewall-rules}{这篇笔记}%
\begin{footnote}[102]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/cascadium/wsl-windows-toolbar-launcher\#firewall-rules}
%
\end{footnote}则更为细致的介绍了在一般情况下怎样在WSL中运行图形界面程序。

\sphinxAtStartPar
这两篇笔记应该可以给读者足够的信息来解决上述所说的和RViz还有Gazebo相关的问题。唯一的缺点就是这两篇笔记都是英文的，对读者的英语水平有一定要求。


\section{在机器人项目中安全的应用机器学习}
\label{\detokenize{chapter_rl_sys/robot_safety:id1}}\label{\detokenize{chapter_rl_sys/robot_safety::doc}}
\sphinxAtStartPar
机器人和机器学习都是有广阔前景和令人兴奋的前沿领域，而当它们结合在一起后，会变得更加迷人，并且有远大于1+1>2的效果。
因此，当我们在机器人项目中应用机器学习时，我们很容易过于兴奋，尝试着用机器学习去做很多之前只能幻想的成果。
然而，在机器人中应用机器学习和直接使用机器学习有着很多不同。
其中很重要的一点不同就是，一般的机器学习系统更多的是在虚拟世界中造成直接影响，而机器人中的机器学习系统很容易通过机器人对物理世界造成直接影响。
因此，\sphinxstylestrong{当我们在机器人项目中应用机器学习时，我们必须时刻关注系统的安全性}，保证无论是在产品开发时还是在产品上市后的使用期，开发者和用户的安全性都能得到可靠的保证。
而且不仅商业项目要考虑安全性，开发个人项目是也需要确保安全性。
没有人想因为安全性上的疏忽而对自己或朋友/同事造成无法挽回的遗憾。

\sphinxAtStartPar
以上这些并不是危言耸听，让我们设想以下这些情况。

\sphinxAtStartPar
假设你正在为你们公司开发一个物流仓库内使用的移动货运机器人，它被设计为和工人在同一工作环境内运行，以便在需要时及时帮工人搬运货物至目的地。
这个机器人有一个视觉的行人识别系统，以便识别前方是否有人。
当机器人在前进的过程中遇到障碍物的话，这个行人识别系统会参与决定机器人的行为。
如果有人的话，机器人会选择绕大弯来避开行进道路上的行人障碍物；而如果没人的话，机器人可以绕小弯来避障。
可是，如果某次这个行人识别系统检测失误，系统没有检测到前方的障碍物是一个正在梯子上整理货物的工人，所以选择小弯避障。
而当机器人靠近时，工人才突然发现有个机器人正在靠近他，并因此受到惊吓跌落至机器人行进的正前方。如果我们考虑到物流仓库的货运机器人自重加载重一般至少是几百公斤，我们就知道万一真的因此发生碰撞，后果是不堪设想。
如果真的发生这种情况，这个机器人产品的商业前景会毁于一旦，公司和负责人也会被追究相应责任（甚至法律意义上的责任）。更重要的是，对受害者所造成的伤害和自己心里的内疚会对双方的一生都造成严重的影响。

\sphinxAtStartPar
不仅是商业项目，假设你正在开发一个小型娱乐机械臂来尝试帮你完成桌面上的一些小任务，例如移动茶杯或打开关闭开关。
你的这个机械臂也依赖于一个物体识别系统来识别任务目标。
某次在移动茶杯时，机械臂没有识别到规划路线中有一个接线板，因此茶杯不小心摔倒并且水泼到接线板里引起短路。
幸运的话可能只需要换一个接线板，而不幸的时候甚至可能会引起火灾或电击。
我相信，没有人会想遇到这类突发事件。

\sphinxAtStartPar
因此，无论是在怎样的机器人项目中应用机器学习，我们都必须时刻关注和确保系统的安全性。


\subsection{确保安全性的办法：谨慎的风险评估和独立的安全系统}
\label{\detokenize{chapter_rl_sys/robot_safety:id2}}

\subsubsection{谨慎的风险评估}
\label{\detokenize{chapter_rl_sys/robot_safety:id3}}
\sphinxAtStartPar
为了能够确保机器人和机器学习系统的安全性，我们首先要知道可能有哪些危险。
我们可以通过风险评估（risk assessment）来做到这一点。

\sphinxAtStartPar
怎样完成一份风险评估网上已经有很多文章了，我们在这里就不过多的介绍。
我们想要强调的是，对于发现的风险，我们需要尽可能的给出一个避免风险的方案（risk
mitigation）。
更重要的时，我们需要确保这些方案的具体执行，而不仅仅是流于表面的给出方案就完事。
一份没有执行的方案等于没有方案。


\subsubsection{独立的安全系统}
\label{\detokenize{chapter_rl_sys/robot_safety:id4}}
\sphinxAtStartPar
在了解了可能有哪些风险之后，我们可以通过设计一个独立的安全系统来规避掉风险中和机器人系统相关的那一部分。

\sphinxAtStartPar
具体来讲，这个安全系统应该独立于机器学习系统，并且处于机器人架构的底层和拥有足够或最高等级的优先级。
实际上，这个安全系统不应该只针对机器学习系统，而是应该针对整个机器人的方方面面。
或者换句话来说，当开发机器人项目时，必须要有一个足够安全且独立的安全系统。
而针对于机器学习系统的安全性只是这个独立安全系统“足够安全”的部分体现罢了。

\sphinxAtStartPar
还是以之前的那个物流仓库移动货运机器人为例。
如果机器人的轮子是有独立安全回路并且断电自动刹车的轮子，而机器人又有一个严格符合安全标准且也有安全回路的激光雷达来检测障碍物，同时这个激光雷达的安全回路直接连接至轮子的安全回路。
这样一来，不管机器人是否检测到前方有人或突然有一个人闯入机器人行进路线，激光雷达都会检测到有异物，直接通过独立的安全回路将轮子断电并刹车，以确保不会发生碰撞。
这样一个配置完全独立于任何控制逻辑，从而不受任何上层系统的影响。
\sphinxstylestrong{而对于开发者来说，当我们有了一个可靠独立的安全系统，我们也可以放心的去使用最新的突破性技术，而不用担心新技术是否会造成不可预期的后果。}


\subsection{机器学习系统的伦理问题}
\label{\detokenize{chapter_rl_sys/robot_safety:id5}}
\sphinxAtStartPar
除了上述讨论到的最根本的安全性问题，机器学习系统的伦理问题也会对机器人的使用造成影响。

\sphinxAtStartPar
例如训练数据集中人种类型不平衡这一类经典的伦理问题。
让我们还是以之前的那个物流仓库移动货运机器人为例。
如果我们的训练数据集只有亚洲人的图片，那么当我们想要开拓海外市场时，我们的海外用户很有可能会发现我们的机器人并不能很好的识别他们的工人。
虽然独立的安全系统可以避免事故的发生，但是急停在工人面前肯定不是一个很好的用户体验。
我们机器人的海外销量也会受到影响。

\sphinxAtStartPar
机器学习系统的伦理问题是目前比较火热的一个讨论领域。作为行业相关人员，我们需要了解这个方向上的最新进展。一方面是在系统设计的初期就把这些问题考虑进去，另一方面也是希望我们的成果能够给更多人带来幸福，而不是带去困扰。


\subsection{小结}
\label{\detokenize{chapter_rl_sys/robot_safety:id6}}
\sphinxAtStartPar
在这一章节中，我们稍微讨论了下怎样在机器人项目中安全的应用机器学习。我们确认了执行一份谨慎的风险检测和设计一个独立的安全系统是一个不错的办法。最后我们也稍微探讨了下机器学习系统的伦理问题。希望大家都能安全的在机器人项目中运用最新的机器学习技术！


\section{小结}
\label{\detokenize{chapter_rl_sys/summary:id1}}\label{\detokenize{chapter_rl_sys/summary::doc}}
\sphinxAtStartPar
在这一章，我们简单介绍了机器人系统的基本概念，包括通用机器人操作系统、感知系统、规划系统和控制系统等，给读者对机器人问题的基本认识。当前，机器人许多实际问题都有可能通过算法的进一步发展得到解决。另一方面，由于机器人问题设置的特殊性，也使得相应系统与相关硬件的耦合程度更高、更复杂：如何更好地平衡各种传感器负载？如何在计算资源有限的情况下最大化计算效率（实时性）？等等，都需要对计算机系统的设计和使用有更好的理解。


\section{参考文献}
\label{\detokenize{chapter_rl_sys/summary:id2}}
\sphinxAtStartPar



\chapter{附录：机器学习介绍}
\label{\detokenize{appendix_machine_learning_introduction/index:id1}}\label{\detokenize{appendix_machine_learning_introduction/index::doc}}
\sphinxAtStartPar
本书假设读者有一定的机器学习算法基础，因此本章只会简略地介绍一下机器学习，其中的梯度下降方法对本书机器学习系统来说尤为重要，是必须掌握的内容。


\section{神经网络}
\label{\detokenize{appendix_machine_learning_introduction/neural_network:id1}}\label{\detokenize{appendix_machine_learning_introduction/neural_network::doc}}

\subsection{感知器}
\label{\detokenize{appendix_machine_learning_introduction/neural_network:id2}}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{single_neuron2}.png}
\caption{有三个输入和单一输出的神经元}\label{\detokenize{appendix_machine_learning_introduction/neural_network:id15}}\label{\detokenize{appendix_machine_learning_introduction/neural_network:single-neuron}}\end{figure}

\sphinxAtStartPar
\hyperref[\detokenize{appendix_machine_learning_introduction/neural_network:single-neuron}]{图\ref{\detokenize{appendix_machine_learning_introduction/neural_network:single-neuron}}}是一个神经元的例子，输入数据\(x\)根据连线上的权重\(w\)做加权求和得到输出\(z\)，我们把这样的模型叫作\sphinxstylestrong{感知器}（Perceptron）。
因为输入和输出之间只有一层神经连接，这个模型也叫做单层感知器。
\hyperref[\detokenize{appendix_machine_learning_introduction/neural_network:single-neuron}]{图\ref{\detokenize{appendix_machine_learning_introduction/neural_network:single-neuron}}}的模型计算可以写为：\(z = w_{1}x_{1}+ w_{2}x_{2} + w_{3}x_{3}\)。

\sphinxAtStartPar
当输入数据用列向量\({x}=[x_1,x_2,x_3]^T\)表示，模型权重用行向量\({w}=[w_1,w_2,w_3]\)表示，那么输出的标量\(z\)可以写为：
\begin{equation}\label{equation:appendix_machine_learning_introduction/neural_network:appendix_machine_learning_introduction/neural_network:0}
\begin{split}z =
\begin{bmatrix}
w_1,w_2,w_3\\
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix}
={w}{x}\end{split}
\end{equation}
\sphinxAtStartPar
我们可以利用输出标量\(z\)为输入的加权组合来实现特定任务。
比如，可以对“好苹果”和“坏苹果”进行分类，输入的\(x_1,x_2,x_3\)分别代表三种不同的特征：1）红色的程度，2）有没有洞，3）大小。如果苹果的大小对这个判断没有影响，那么对应的权重就为零。
这个神经网络的训练，其实就是选择合适的权重，来实现我们的任务。比如我们可以选择合适的权重，使得当\(z\)小于等于\(0\)时代表“坏苹果”，而当\(z\)大于\(0\)时则是“好苹果”。
则最终的分类输出标签\(y\)如下，为\(1\)时代表好，\(0\)代表坏。这个神经元的输入和输出之间只有一层，所以可以成为单层神经网络。
\begin{equation}\label{equation:appendix_machine_learning_introduction/neural_network:appendix_machine_learning_introduction/neural_network:1}
\begin{split}y =
\begin{cases}
1 &  z>0 \\
0 & z \leq 0 \\
\end{cases}\end{split}
\end{equation}

\subsection{决策边界vs.偏置}
\label{\detokenize{appendix_machine_learning_introduction/neural_network:vs}}
\sphinxAtStartPar
通过选择合适的权重以\(z\)大于或小于\(0\)来对输入数据做分类的话，可以在数据空间上获得一个\sphinxstylestrong{决策边界}
（Decision Boundary）。如
\hyperref[\detokenize{appendix_machine_learning_introduction/neural_network:single-neuron-decision-boundary2}]{图\ref{\detokenize{appendix_machine_learning_introduction/neural_network:single-neuron-decision-boundary2}}}所示，以神经元输出\(z=0\)作为输出标签\(y\)的决策边界，
没有偏置时决策边界必然经过坐标原点，如果数据样本点不以原点来分开，会导致分类错误。
为了解决这个问题，可以在神经元上加入一个\sphinxstylestrong{偏置}（Bias）。
\hyperref[\detokenize{appendix_machine_learning_introduction/neural_network:single-neuron-bias2}]{图\ref{\detokenize{appendix_machine_learning_introduction/neural_network:single-neuron-bias2}}}
是一个有偏置\(b\)的神经元模型，可以用
\eqref{equation:appendix_machine_learning_introduction/neural_network:singleneuron_bias}表达：
\begin{equation}\label{equation:appendix_machine_learning_introduction/neural_network:singleneuron_bias}
\begin{split}z = w_{1}x_{1}+ w_{2}x_{2}+ w_{3}x_{3} + b\end{split}
\end{equation}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{single_neuron_decision_boundary2}.png}
\caption{两个输入（左）和三个输入（右）时的决策边界。不同形状的点代表不同类别的数据，需要找到\(z=0\)作为决策边界来把不同数据点分开。两个输入时决策边界是一直线，三个输入时决策边界是一个平面，高维度输入时决策边界称为\sphinxstylestrong{超平面}（Hyperplane）。
左：
\(z=w_{1}x_{1}+w_{2}x_{2}+b\)。右：\(z=w_{1}x_{1}+w_{2}x_{2}+w_{3}x_{3}+b\)。没有偏置时，决策边界必然经过原点，所以不能分开不同类别的数据样本。}\label{\detokenize{appendix_machine_learning_introduction/neural_network:id16}}\label{\detokenize{appendix_machine_learning_introduction/neural_network:single-neuron-decision-boundary2}}\end{figure}

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{single_neuron_bias2}.png}
\caption{一个有偏置的单层神经网络}\label{\detokenize{appendix_machine_learning_introduction/neural_network:id17}}\label{\detokenize{appendix_machine_learning_introduction/neural_network:single-neuron-bias2}}\end{figure}

\sphinxAtStartPar
有了偏置以后，决策边界（直线、平面或超平面）可以不经过坐标原点，因此能更好地分类样本。
准确来说，决策边界把这些样本数据分成两个不同的类别，这个边界是
\(\{x_1, x_2, x_3 | w_{1}x_{1}+ w_{2}x_{2}+ w_{3}x_{3} + b = 0\}\)。


\subsection{逻辑回归}
\label{\detokenize{appendix_machine_learning_introduction/neural_network:id3}}
\sphinxAtStartPar
上述神经元的输入和输出是线性关系，为了提供非线性的数据表达能力，可以在神经元输出上加上\sphinxstylestrong{激活函数}（Activation
Function），最常见的激活函数有Sigmoid、Tanh、ReLU和Softmax等。
比如，上述神经元以\(z=0\)为分界来做分类任务，那么我们可不可以让神经元输出一个概率呢？比如输出\(0~1\)，\(1\)代表输入数据\(100\%\)为某一类。
为了让神经元输出\(0~1\)，可以在\(z\)上加一个逻辑函数\sphinxstylestrong{Sigmoid}，
如
\eqref{equation:appendix_machine_learning_introduction/neural_network:sigmoid}所示，Sigmoid把数值限制在0和1之中，通过一个简单的临界值（如：0.5）来决定最终输出的标签是否属于某个类别。这个方法叫做\sphinxstylestrong{逻辑回归}（Logistic
Regression）。
\begin{equation}\label{equation:appendix_machine_learning_introduction/neural_network:sigmoid}
\begin{split}a = f({z}) = \frac{1}{1+{\rm e}^{-{z}}}\end{split}
\end{equation}

\subsection{多个神经元}
\label{\detokenize{appendix_machine_learning_introduction/neural_network:id4}}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{two_neurons2}.png}
\caption{多个神经元}\label{\detokenize{appendix_machine_learning_introduction/neural_network:id18}}\label{\detokenize{appendix_machine_learning_introduction/neural_network:two-neurons2}}\end{figure}

\sphinxAtStartPar
上述网络只有一个输出，若多个神经元在一起就可以有多个输出。
\hyperref[\detokenize{appendix_machine_learning_introduction/neural_network:two-neurons2}]{图\ref{\detokenize{appendix_machine_learning_introduction/neural_network:two-neurons2}}}是有两个输出的网络，每个输出都和所有输入相连，所以也被称\sphinxstylestrong{全连接层}（Fully\sphinxhyphen{}Connected(FC)
Layer）， 可由下述式子 \eqref{equation:appendix_machine_learning_introduction/neural_network:fc_cal}表示X。
\begin{equation}\label{equation:appendix_machine_learning_introduction/neural_network:fc_cal}
\begin{split}z_{1} &= w_{11}x_{1} + w_{12}x_{2} + w_{13}x_{3} + b_1 \notag \\ z_{2} &= w_{21}x_{1} + w_{22}x_{2} + w_{23}x_{3} + b_2\end{split}
\end{equation}
\sphinxAtStartPar
如下式子表示了矩阵方法的实现：
\begin{equation}\label{equation:appendix_machine_learning_introduction/neural_network:appendix_machine_learning_introduction/neural_network:2}
\begin{split}{z} =
\begin{bmatrix}
z_1 \\
z_2
\end{bmatrix}
=
\begin{bmatrix}
w_{11} & w_{12} & w_{13}\\
w_{21} & w_{22} & w_{23}\\
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix}
+
\begin{bmatrix}
b_1 \\ b_2
\end{bmatrix}
= {W}{x} + {b}\end{split}
\end{equation}
\sphinxAtStartPar
多输出的网络可以实现多分类问题，比如有10个数值输出，每个数值分别代表一类物品的概率，每个输出在\(0\)到\(1\)之间，10个输出之和为\(1\)。
可用 \eqref{equation:appendix_machine_learning_introduction/neural_network:e_softmax}的\sphinxstylestrong{Softmax}
函数来实现，\(K\)为输出的个数：
\begin{equation}\label{equation:appendix_machine_learning_introduction/neural_network:e_softmax}
\begin{split}f({z})_{i} = \frac{{\rm e}^{z_{i}}}{\sum_{k=1}^{K}{\rm e}^{z_{k}}}\end{split}
\end{equation}

\subsection{多层感知器}
\label{\detokenize{appendix_machine_learning_introduction/neural_network:id5}}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{mlp2}.png}
\caption{多层感知器例子。\(a^l_i\)表示神经元输出\(z\)经过激活函数后的值，其中\(l\)代表层的序号（\(L\)代表输出层），\(i\)代表输出的序号}\label{\detokenize{appendix_machine_learning_introduction/neural_network:id19}}\end{figure}

\sphinxAtStartPar
\sphinxstylestrong{多层感知器}（Multi\sphinxhyphen{}Layer Perceptron，MLP）
\DUrole{bibtex}{{[}rosenblatt1958perceptron{]}}通过叠加多层全连接层来提升网络的表达能力。相比单层网络，多层感知器有很多中间层的输出并不暴露给最终输出，这些层被称为\sphinxstylestrong{隐含层}（Hidden
Layers）。这个例子中的网络可以通过下方的串联式矩阵运算实现，其中\(W^l\)和\(b^l\)代表不同层的权重矩阵和偏置，\(l\)代表层号，\(L\)代表输出层。
\begin{equation}\label{equation:appendix_machine_learning_introduction/neural_network:appendix_machine_learning_introduction/neural_network:3}
\begin{split}{z} = f({W^L}f({W^3}f({W^2}f({W^1}{x} + {b^1}) + {b^2}) + {b^3}) + {b^L})\end{split}
\end{equation}
\sphinxAtStartPar
在深度学习时代，网络模型基本都是多层的神经网络层连接起来的，输入数据经过多层的特征提取，可以学到不同抽象层级的\sphinxstylestrong{特征向量}（Feature
Vector）。下面我们介绍一下其他常用的神经网络层。


\subsection{卷积网络}
\label{\detokenize{appendix_machine_learning_introduction/neural_network:id7}}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{conv_computation_v4}.png}
\caption{卷积运算例子。
输入一个三通道的数据，其大小为\(4 \times 4 \times 3\)（高
\(\times\) 宽 \(\times\)
通道数），为了对各个通道做卷积，卷积核也必须有三个通道，一个卷积核的大小为\(3 \times 3 \times 3 \times 1\)（高
\(\times\)
宽\(\times\)输入通道数\(\times\)输出通道数（卷积核的个数））。有多少个卷积核就有多少个输出的\sphinxstylestrong{特征图}（Feature
Map），在这个例子中因为只有一个卷积核，所以输出的通道数为1，高宽为2。与此同时，我们把这种高维度的输入数据称为\sphinxstylestrong{张量}（Tensor），比如RGB图像、视频、前一层卷积层的输出等等。}\label{\detokenize{appendix_machine_learning_introduction/neural_network:id20}}\label{\detokenize{appendix_machine_learning_introduction/neural_network:conv-computation-v4}}\end{figure}

\sphinxAtStartPar
\sphinxstylestrong{卷积神经网络} （Convolutional Neural Network，CNN）
\DUrole{bibtex}{{[}lecun1989backpropagation{]}}由多层\sphinxstylestrong{卷积层}（Convolutional
Layer）组成，常用于计算机视觉任务
\DUrole{bibtex}{{[}krizhevsky2012imagenet{]}}\DUrole{bibtex}{{[}he2016deep{]}}。
\hyperref[\detokenize{appendix_machine_learning_introduction/neural_network:conv-computation-v4}]{图\ref{\detokenize{appendix_machine_learning_introduction/neural_network:conv-computation-v4}}}描述了一个卷积运算的例子。
根据卷积的特点，我们可以知道两个事实：1）一个卷积核的通道数，等于输入的通道数；2）输出的通道数，等于卷积核的数量。

\sphinxAtStartPar
\hyperref[\detokenize{appendix_machine_learning_introduction/neural_network:conv-computation-v4}]{图\ref{\detokenize{appendix_machine_learning_introduction/neural_network:conv-computation-v4}}}例子中，卷积核每次滑动一个数值的范围来进行卷积操作，我们称它的\sphinxstylestrong{步长}（Stride）为1。此外，如果希望输入的边缘数值也能被考虑在内的话，则需要对边缘做\sphinxstylestrong{填零}（Zero
Padding）操作。
\hyperref[\detokenize{appendix_machine_learning_introduction/neural_network:conv-computation-v4}]{图\ref{\detokenize{appendix_machine_learning_introduction/neural_network:conv-computation-v4}}}例子中，如果输入的每个通道上下左右都填充一圈零，那么输出的大小则为\(4\times 4\times 1\)。填零的圈数取决于卷积核的大小，卷积核越大则填零圈数越大。

\sphinxAtStartPar
为了对输入的图像数据做特征提取，卷积核数量往往比输入数据的通道数据要多，这样的话输出数据的数值会很多，计算量变大。然而图像数据中相邻像素的特征往往相似，所以我们可以对相邻的输出特征进行聚合操作。\sphinxstylestrong{池化层}就是为了实现这个目的，我们通常有两种池化方法最大值池化（Max
Pooling）和平均值池化（Mean Pooling）。如
\hyperref[\detokenize{appendix_machine_learning_introduction/neural_network:pooling-v3}]{图\ref{\detokenize{appendix_machine_learning_introduction/neural_network:pooling-v3}}}所示，假设池化的卷积核高宽为\(2\times2\)，输入\(4\times4\)的数据，步长为2（步长为1时，则输出等于输入），则输出为\(2\times2\)。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{pooling_v3}.png}
\caption{\(2 \times 2\)
最大值池化和平均值池化的例子，它们的步长为2，输入大小是\(4 \times 4\)}\label{\detokenize{appendix_machine_learning_introduction/neural_network:id21}}\label{\detokenize{appendix_machine_learning_introduction/neural_network:pooling-v3}}\end{figure}

\sphinxAtStartPar
卷积层和全连接层都是很常用的，但是卷积层在输入是高维度的图像时，需要的参数量远远小于全连接层。卷积层的运算和全连接层是类似的，前者基于高维度张量运算，后者基于二维矩阵运算。


\subsection{时序模型}
\label{\detokenize{appendix_machine_learning_introduction/neural_network:id11}}
\sphinxAtStartPar
现实生活中除了图像还有大量时间序列数据，例如视频、股票价格等等。\sphinxstylestrong{循环神经网络}（Recurrent
Neural Networks，RNN）
\DUrole{bibtex}{{[}rumelhart1986learning{]}}是一种处理序列数据的深度学习模型结构。序列数据是一串连续的数据\(\{x_1, x_2, \dots, x_n\}\)，比如每个\(x\)代表一个句子中的单词。

\sphinxAtStartPar
为了可以接收一连串的输入序列，如
\hyperref[\detokenize{appendix_machine_learning_introduction/neural_network:rnn-simple-cell2}]{图\ref{\detokenize{appendix_machine_learning_introduction/neural_network:rnn-simple-cell2}}}所示，朴素循环神经网络使用了循环单元（Cell）作为计算单元，用隐状态（Hidden
State）来存储过去输入的信息。具体来说，对输入模型的每个数据\(x\)，根据公式 \eqref{equation:appendix_machine_learning_introduction/neural_network:aligned}，循环单元会反复计算新的隐状态，用于记录当前和过去输入的信息。而新的隐状态会被用到下一单元的计算中。
\begin{equation}\label{equation:appendix_machine_learning_introduction/neural_network:aligned}
\begin{split}{h}_t = {W}[{x}_t; {h}_{t-1}] + {b}\end{split}
\end{equation}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{rnn_simple_cell2}.png}
\caption{朴素循环神经网络。
在每一步的计算中，循环单元通过过去时刻的隐状态\({h}_{t-1}\)和当前的输入\({x}_t\)，求得当前的隐状态\({h}_t\)。}\label{\detokenize{appendix_machine_learning_introduction/neural_network:id22}}\label{\detokenize{appendix_machine_learning_introduction/neural_network:rnn-simple-cell2}}\end{figure}

\sphinxAtStartPar
然而这种简单的朴素循环神经网络有严重的信息遗忘问题。比如说我们的输入是“我是中国人，我的母语是\_\sphinxstylestrong{“，隐状态记住了”中国人”的信息，使得网络最后可以预测出”中文”一词；但是如果句子很长的时候，隐状态可能记不住太久之前的信息了，比如说”我是中国人，我去英国读书，后来在法国工作，我的母语是}\_”，这时候在最后的隐状态中关于“中国人”的信息可能会被因为多次的更新而遗忘了。
为了解决这个问题，后面有人提出了各种各样的改进方法，其中最有名的是长短期记忆（Long
Short\sphinxhyphen{}Term Memory，LSTM）
\DUrole{bibtex}{{[}Hochreiter1997lstm{]}}。关于时序的模型还有很多很多，比如近年来出现的Transformer
\DUrole{bibtex}{{[}vaswani2017attention{]}}等等。


\section{梯度下降与反向传播}
\label{\detokenize{appendix_machine_learning_introduction/gradient_descent:id1}}\label{\detokenize{appendix_machine_learning_introduction/gradient_descent::doc}}
\sphinxAtStartPar
上面大体上介绍了经典神经网络的内容，那么现在有一个问题，这些网络中的参数是如何确定的呢？如果要解决的问题是一个小感知器就能解决的话，参数可以人为地去确定。但是如果是一个深度网络的话，参数的确定需要自动化，也就是所谓的网络训练，而这个过程需要我们设定一个\sphinxstylestrong{损失函数}（Loss
Function）来作为训练优化的一个方向。
常见的损失函数有：1）用来衡量向量之间距离的均方误差(Mean Squared
Error，MSE)
\(\mathcal{L} = \frac{1}{N}\|{y}-\hat{{y}}\|^{2}_{2} = \frac{1}{N}\sum_{i=1}^N(y_{i}-\hat{y}_{i})^{2}\)
和 平均绝对误差(Mean Absolute Error，MAE)
\(\mathcal{L} = \frac{1}{N}\sum_{i=1}^{N}|y_{i}-\hat{y}_{i}|\)
，其中\(N\)代表数据样本的数量，用以求平均用，而\(y\)代表真实标签（Ground
Truth）、\(\hat{y}\)代表网络输出的预测标签。
2）分类任务可以用的交叉熵损失（Cross Entropy）
\(\mathcal{L} = - \frac{1}{N} \sum_{i=1}^N \bigg(y_{i}\log\hat{y}_{i} + (1 - y_{i})\log(1 - \hat{y}_{i})\bigg)\)来作为损失数，当且仅当输出标签和预测标签一样的时候损失值才为零。

\sphinxAtStartPar
有了损失值之后，我们就可以利用大量真实标签的数据和优化方法来更新模型参数了，其中最常用的方法是\sphinxstylestrong{梯度下降}（Gradient
Descent）。如 \hyperref[\detokenize{appendix_machine_learning_introduction/gradient_descent:gradient-descent2}]{图\ref{\detokenize{appendix_machine_learning_introduction/gradient_descent:gradient-descent2}}}所示，
开始的时候，模型的参数\({w}\)是随机选取的，然后求出损失值对参数的偏导数\(\frac{\partial \mathcal{L}}{\partial {w}}\)，通过反复迭代
\({w}:={w}-\alpha\frac{\partial \mathcal{L}}{\partial {w}}\)完成优化。这个优化的过程其实就可以降低损失值以达到任务目标，其中\(\alpha\)是控制优化幅度的\sphinxstylestrong{学习率}（Learning
Rate）。
在实践中，梯度下降最终得到的最小值很大可能是一个局部最小值，而不是全局最小值。不过由于深度神经网络能提供一个很强的数据表达能力，所以局部最小值可以很接近全局最小值，损失值可以足够小。

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{gradient_descent2}.png}
\caption{梯度下降介绍。（左图）只有一个可以训练的参数\(w\)；（右图）有两个可以训练的参数\({w}=[w_1,w_2]\)。在不断更新迭代参数后，损失值\(\mathcal{L}\)会逐渐地减小。但是由于存在很多局部最优解，我们往往不能更新到全局最优解。}\label{\detokenize{appendix_machine_learning_introduction/gradient_descent:id7}}\label{\detokenize{appendix_machine_learning_introduction/gradient_descent:gradient-descent2}}\end{figure}

\sphinxAtStartPar
那么接下来，在深度神经网络中如何实现梯度下降呢，这需要计算出网络中每层参数的偏导数\(\frac{\partial \mathcal{L}}{\partial {w}}\)，我们可以用\sphinxstylestrong{反向传播}（Back\sphinxhyphen{}Propagation）
\DUrole{bibtex}{{[}rumelhart1986learning{]}}\DUrole{bibtex}{{[}lecun2015deep{]}}来实现。 接下来，
我们引入一个中间量\({\delta}=\frac{\partial \mathcal{L}}{\partial {z}}\)来表示损失函数\(\mathcal{L}\)
对于神经网络输出\({z}\)（未经过激活函数，不是\(a\)）的偏导数，
并最终得到\(\frac{\partial \mathcal{L}}{\partial {w}}\)。

\sphinxAtStartPar
我们下面用一个例子来介绍反向传播算法，
我们设层序号为\(l=1, 2, \ldots L\)（输出层（最后一层）序号为\(L\)）。
对于每个网络层，我们有输出\({z}^l\)，中间值\({\delta}^l=\frac{\partial \mathcal{L}}{\partial {z}^l}\)和一个激活值输出\({a}^l=f({z}^l)\)
（其中\(f\)为激活函数）。
我们假设模型是使用Sigmoid激活函数的多层感知器，损失函数是均方误差（MSE）。也就是说，我们设定：
\begin{itemize}
\item {} 
\sphinxAtStartPar
网络结构\({z}^{l}={W}^{l}{a}^{l-1}+{b}^{l}\)

\item {} 
\sphinxAtStartPar
激活函数\({a}^l=f({z}^l)=\frac{1}{1+{\rm e}^{-{z}^l}}\)

\item {} 
\sphinxAtStartPar
损失函数\(\mathcal{L}=\frac{1}{2}\|{y}-{a}^{L}\|^2_2\)

\end{itemize}

\sphinxAtStartPar
我们可以直接算出激活输出对于原输出的偏导数：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\frac{\partial {a}^l}{\partial {z}^l}=f'({z}^l)=f({z}^l)(1-f({z}^l))={a}^l(1-{a}^l)\)

\end{itemize}

\sphinxAtStartPar
和损失函数对于激活输出的偏导数：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\frac{\partial \mathcal{L}}{\partial {a}^{L}}=({a}^{L}-{y})\)

\end{itemize}

\sphinxAtStartPar
有了这些后，为了进一步得到损失函数对于每一个参数的偏导数，可以使用\sphinxstylestrong{链式法则}（Chain
Rule），细节如下：

\sphinxAtStartPar
首先，从输出层（\(l=L\)，最后一层）开始向后方传播误差，根据链式法则，我们先计算输出层的中间量：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\({\delta}^{L} =\frac{\partial \mathcal{L}}{\partial {z}^{L}} =\frac{\partial \mathcal{L}}{\partial {a}^{L}}\frac{\partial {a}^L}{\partial {z}^{L}}=({a}^L-{y})\odot({a}^L(1-{a}^L))\)

\end{itemize}

\sphinxAtStartPar
除了输出层（\(l=L\)）的中间值\({\delta}^{L}\)，其他层（\(l=1, 2, \ldots , L-1\)）的中间值\({\delta}^{l}\)如何计算呢？
\begin{itemize}
\item {} 
\sphinxAtStartPar
已知模型结构\({z}^{l+1}={W}^{l+1}{a}^{l}+{b}^{l+1}\)，我们可以直接得到\(\frac{\partial {z}^{l+1}}{\partial {a}^{l}}={W}^{l+1}\)；而且我们已知\(\frac{\partial {a}^l}{\partial {z}^l}={a}^l(1-{a}^l)\)

\item {} 
\sphinxAtStartPar
那么根据链式法则，我们可以得到
\({\delta}^{l} =\frac{\partial \mathcal{L}}{\partial {z}^{l}} =\frac{\partial \mathcal{L}}{\partial {z}^{l+1}}\frac{\partial {z}^{l+1}}{\partial {a}^{l}}\frac{\partial {a}^{l}}{\partial {z}^{l}} =({W}^{l+1})^\top{\delta}^{l+1}\odot({a}^l(1-{a}^l))\)

\end{itemize}

\sphinxAtStartPar
根据上面的计算有所有层的中间值\({\delta}^l, l=1, 2, \ldots , L\)后，我们就可以在此基础上求出损失函数对于每层参数的偏导数：\(\frac{\partial \mathcal{L}}{\partial {W}^l}\)和\(\frac{\partial \mathcal{L}}{\partial {b}^l}\)，以此来根据梯度下降的方法来更新每一层的参数。
\begin{itemize}
\item {} 
\sphinxAtStartPar
已知模型结构\({z}^l={W}^l{a}^{l-1}+{b}^l\)，我们可以求出
\(\frac{\partial {z}^{l}}{\partial {W}^l}={a}^{l-1}\) 和
\(\frac{\partial {z}^{l}}{\partial {b}^l}=1\)

\item {} 
\sphinxAtStartPar
那么根据链式法则，我们可以得到\(\frac{\partial \mathcal{L}}{\partial {W}^l}=\frac{\partial \mathcal{L}}{\partial {z}^l}\frac{\partial {z}^l}{\partial {W}^l}={\delta}^l({a}^{l-1})^\top\)
,
\(\frac{\partial \mathcal{L}}{\partial {b}^l}=\frac{\partial \mathcal{L}}{\partial {z}^l}\frac{\partial {z}^l}{\partial {b}^l}={\delta}^l\)

\end{itemize}

\sphinxAtStartPar
求得所有偏导数\(\frac{\partial \mathcal{L}}{\partial {W}^l}\) 和
\(\frac{\partial \mathcal{L}}{\partial {b}^l}\)后，我们就可以用梯度下降更新所有参数\({W}^l\)
和 \({b}^l\)：
\begin{itemize}
\item {} 
\sphinxAtStartPar
\({W}^l:={W}^l-\alpha\frac{\partial \mathcal{L}}{\partial {W}^l}\),
\({b}^l:={b}^l-\alpha\frac{\partial \mathcal{L}}{\partial {b}^l}\)

\end{itemize}

\sphinxAtStartPar
但是还有一个问题需要解决，那就是梯度下降的时候每更新一次参数，都需要计算一次当前参数下的损失值。然而，当训练数据集很大时（\(N\)很大），若每次更新都用整个训练集来计算损失值的话，计算量会非常巨大。
为了减少计算量，我们使用\sphinxstylestrong{随机梯度下降}（Stochastic Gradient
Descent，SGD）来计算损失值。具体来说，我们计算损失值不用全部训练数据，而是从训练集中随机选取一些数据样本来计算损失值，比如选取16、32、64或者128个数据样本，样本的数量被称为\sphinxstylestrong{批大小}（Batch
Size）。
此外，学习率的设定也非常重要。如果学习率太大，可能无法接近最小值的山谷，如果太小，训练又太慢。
自适应学习率，例如Adam \DUrole{bibtex}{{[}KingmaAdam2014{]}}、RMSProp \DUrole{bibtex}{{[}tieleman2012rmsprop{]}}和
Adagrad \DUrole{bibtex}{{[}duchi2011adagrad{]}}等，在训练的过程中通过自动的方法来修改学习率，实现训练的快速收敛，到达最小值点。


\section{经典机器学习方法}
\label{\detokenize{appendix_machine_learning_introduction/classic_machine_learning:id1}}\label{\detokenize{appendix_machine_learning_introduction/classic_machine_learning::doc}}
\sphinxAtStartPar
大量经典机器学习算法，如 支持向量机(Support Vector Machine，SVM),
K最近邻(K\sphinxhyphen{}Nearest Neighbor, KNN)分类算法 和K均值聚类算法(K\sphinxhyphen{}Means
Clustering Algorithm)等，
虽然它们有的有网络参数，有的没有网络参数，有的是监督学习算法，有的是无监督学习算法，
训练过程也不一样，但是从系统的角度，它们都是以矩阵运算为基础的。下面，我们来简要介绍一下这些算法。


\subsection{支持向量机}
\label{\detokenize{appendix_machine_learning_introduction/classic_machine_learning:id2}}
\sphinxAtStartPar
\sphinxstylestrong{支持向量机}(Support Vector
Machine，SVM)，是一种经典的机器学习分类算法，其核心思想在于最大化决策边界到数据点的距离。在这里，我们以线性可分数据为例；对于非线性可分的数据，运用\sphinxstylestrong{核方法}(Kernel
Method)即可类似处理。

\sphinxAtStartPar
如果训练数据是线性可分的，SVM的目标则是最大化\sphinxstylestrong{间隔}(Margin)。首先，我们先来定义最大化间隔的分类器，如下：
\begin{equation}\label{equation:appendix_machine_learning_introduction/classic_machine_learning:appendix_machine_learning_introduction/classic_machine_learning:0}
\begin{split}\min_{{w},b} ~~~\frac{1}{2} ||{w}||^2\end{split}
\end{equation}\begin{equation}\label{equation:appendix_machine_learning_introduction/classic_machine_learning:appendix_machine_learning_introduction/classic_machine_learning:1}
\begin{split}s.t. ~~~y_i ({w}^T {x_i} + b) \geq 1, ~~~\forall 1 \leq i \leq n\end{split}
\end{equation}
\sphinxAtStartPar
其拉格朗日乘子为
\begin{equation}\label{equation:appendix_machine_learning_introduction/classic_machine_learning:appendix_machine_learning_introduction/classic_machine_learning:2}
\begin{split}L({w},b,{\lambda}) = \frac{1}{2} ||{w}||^2 + \sum_{i=1}^n \lambda_i (1-y_i({w}^T {x_i} + b))\end{split}
\end{equation}
\sphinxAtStartPar
由于\(\frac{1}{2} ||{w}||^2\)是凸的，并且\(\lambda_i (1-y_i({w}^T {x_i} + b))\)是线性的（也是凸的），所以优化问题的解为
\begin{equation}\label{equation:appendix_machine_learning_introduction/classic_machine_learning:appendix_machine_learning_introduction/classic_machine_learning:3}
\begin{split}\max_{\lambda>0} \min_{{w},b} L({w},b, {\lambda})\end{split}
\end{equation}
\sphinxAtStartPar
求\(L\)关于\({w},b\)的导数有
\begin{equation}\label{equation:appendix_machine_learning_introduction/classic_machine_learning:appendix_machine_learning_introduction/classic_machine_learning:4}
\begin{split}\nabla_{{w}} L= {w} - \sum_{i=1}^n \lambda_i y_i {x_i}\end{split}
\end{equation}\begin{equation}\label{equation:appendix_machine_learning_introduction/classic_machine_learning:appendix_machine_learning_introduction/classic_machine_learning:5}
\begin{split}\nabla_b L = - \sum_{i=1}^n \lambda_i y_i\end{split}
\end{equation}
\sphinxAtStartPar
令\(L\)关于\({w},b\)的导数均为0得到，\({w}^* = \sum_{i=1}^n \lambda_i y_i {x_i}\)以及\(\sum_{i=1}^n \lambda_i y_i = 0\)。
由于当\(\lambda\)固定的时候，\(b\)的值对目标函数无贡献，所以可以令\(b^* = 0\)。
这时，由对偶性理论和KTT条件，我们得到：
\begin{equation}\label{equation:appendix_machine_learning_introduction/classic_machine_learning:appendix_machine_learning_introduction/classic_machine_learning:6}
\begin{split}y_i ({w}^{*T} {x_i} + b^*) > 1 \Rightarrow \lambda_i^* = 0\end{split}
\end{equation}\begin{equation}\label{equation:appendix_machine_learning_introduction/classic_machine_learning:appendix_machine_learning_introduction/classic_machine_learning:7}
\begin{split}\lambda_i^* > 0  \Rightarrow y_i ({w}^{*T} {x_i} + b^*) = 1\end{split}
\end{equation}\begin{equation}\label{equation:appendix_machine_learning_introduction/classic_machine_learning:appendix_machine_learning_introduction/classic_machine_learning:8}
\begin{split}{w}^* = \sum_{i=1}^n \lambda_i^* y_i {x_i}\end{split}
\end{equation}
\sphinxAtStartPar
如果\(y_i ({w}^{*T} {x_i} + b^*) = 1\)，那么\({x_i}\)就是离超平面\(({w}^*,b^*)\)最近的点之一，否则就不是。因此，\({w}^*\)就是离超平面\(({w}^*,b^*)\)最近的点\({x_i}\)的线性组合。

\sphinxAtStartPar
如此，通过SVM算法，我们实现了数据的分类，并且能够最大化了决策边界到最近点的距离。
我们定义满足\(y_i ({w}^{*T} {x_i} + b^*) = 1\)的\({x_i}\)为\sphinxstylestrong{支持向量}(Support
Vectors)，同时把分类器\(\hat{y}=sgn({w}^{*T} {x_i} + b^*)\)称为支持向量机。


\subsection{K最近邻算法}
\label{\detokenize{appendix_machine_learning_introduction/classic_machine_learning:k}}
\sphinxAtStartPar
\sphinxstylestrong{K最近邻算法}(K\sphinxhyphen{}Nearest
Neighbor，KNN)也是一种传统的机器学习算法，可用于分类、回归等基本的机器学习任务。和上面介绍的SVM算法不同，K最近邻算法的核心思想并不是用一个决策边界把属于不同类的数据分开，而是依靠每个数据点周围几个距离最近的数据的性质，来预测数据点本身的性质。

\sphinxAtStartPar
KNN用于分类时，为了预测某个样本点的类别，会进行一次投票。投票的对象为离这个观测样本点最近的K个样本点，每个要投票的样本点可能会被赋予不同的权重，而投票的“内容”则是样本点的类别。处理投票结果的时候，采用的是少数服从多数的决策方法(Majority
Vote)。也就是说，若一个样本点最近的K个样本点中大多数属于某个类别，那么该样本点也属于这个类别。

\sphinxAtStartPar
KNN算法的具体描述如下：（1）计算待分类点到各已知类别点的距离；（2）将这些点按照距离排序，并按照距离挑选出最近的K个点；（3）按照每个点的权重进行“统票”，票面内容为点所处的类别；（4）返回得票最高的类别，并作为待分类点的预测类别。

\sphinxAtStartPar
KNN算法有几个需要注意的关键问题，包括超参数K的选择，距离的度量方式，还有分类决策规则。对于超参数K，不宜过大，否则会导致很大的近似误差，反之亦不宜过小，否则会导致很大的估计误差。距离的度量，则可以选择曼哈顿距离、欧式距离和闵可夫斯基距离等等。为了降低K值对于预测结果产生的误差和影响，我们通常可以对分类决策规则做一定的规定，比如在投票决策时让距离小的点有更大的权重，距离较大的点权重较小。在编程实现KNN算法的时候，权重等参数都会以矩阵的形式进行运算，以提高运算效率。


\subsection{K均值聚类算法}
\label{\detokenize{appendix_machine_learning_introduction/classic_machine_learning:id3}}
\sphinxAtStartPar
\sphinxstylestrong{K均值聚类算法}(K\sphinxhyphen{}Means Clustering
Algorithm)是机器学习中一种常见的无监督聚类算法。在这里，我们首先定义聚类问题：给定数据点\({x_1},\cdots, {x_n} \in \mathbb{R}^d\)和\(K\in \mathbb{N}\)，需要划分为\(K\)个簇\({C_1}, \cdots, {C_K} \in \mathbb{R}^d\)以及每个数据点所对应的分类中心点\({ C_{(1)}}, \cdots, {C_{(n)}}\)，以最小化距离和\(\sum_i ||{x_i} - {C_{(i)}}||^2\)。

\sphinxAtStartPar
K均值聚类算法是一种解决聚类问题的算法，算法过程如下：
\begin{itemize}
\item {} 
\sphinxAtStartPar
随机选择\({C_1}, \cdots, {C_K}\)

\item {} 
\sphinxAtStartPar
把\({x_i}\)所对应的分类置为距离其最近的聚类中心点的分类

\item {} 
\sphinxAtStartPar
计算并赋值\({C_K} = \frac{\sum_{{C_{(i)}}={C_K}} {x_i}}{\sum_{{C_{(i)}}={C_K}} 1}\)

\item {} 
\sphinxAtStartPar
重复以上步骤直到算法收敛

\end{itemize}

\sphinxAtStartPar
可以证明，K均值聚类算法会使得距离和\(\sum_i ||{x_i} - {C_{(i)}}||^2\)不断地单调减小，并且最终能够收敛。不过，算法可能收敛到局部最小值。

\sphinxAtStartPar
本章结束语：

\sphinxAtStartPar
在系统角度，机器学习的算法无论是什么算法，涉及到高维数据任务的现都是矩阵运算实现的。


\section{参考文献}
\label{\detokenize{appendix_machine_learning_introduction/classic_machine_learning:id4}}
\sphinxAtStartPar


\begin{sphinxthebibliography}{Ragan\sphinxhyphen{}Ke}
\bibitem[Bastoul, 2004]{chapter_accelerator/summary:bastoul2004code}
\sphinxAtStartPar
Bastoul, C. (2004). Code generation in the polyhedral model is easier than you think. \sphinxstyleemphasis{Proceedings. 13th International Conference on Parallel Architecture and Compilation Techniques, 2004. PACT 2004.} (pp. 7–16).
\bibitem[Chen et al., 2018]{chapter_accelerator/summary:chen2018tvm}
\sphinxAtStartPar
Chen, T., Moreau, T., Jiang, Z., Shen, H., Yan, E. Q., Wang, L., … Krishnamurthy, A. (2018). Tvm: end\sphinxhyphen{}to\sphinxhyphen{}end optimization stack for deep learning. \sphinxstyleemphasis{arXiv preprint arXiv:1802.04799}, \sphinxstyleemphasis{11}, 20.
\bibitem[Lattner et al., 2020]{chapter_accelerator/summary:lattner2020mlir}
\sphinxAtStartPar
Lattner, C., Amini, M., Bondhugula, U., Cohen, A., Davis, A., Pienaar, J., … Zinenko, O. (2020). Mlir: a compiler infrastructure for the end of moore’s law. \sphinxstyleemphasis{arXiv preprint arXiv:2002.11054}.
\bibitem[Liao et al., 2021]{chapter_accelerator/summary:ascend}
\sphinxAtStartPar
Liao, H., Tu, J., Xia, J., Liu, H., Zhou, X., Yuan, H., \& Hu, Y. (2021). Ascend: a scalable and unified architecture for ubiquitous deep neural network computing : industry track paper. \sphinxstyleemphasis{2021 IEEE International Symposium on High\sphinxhyphen{}Performance Computer Architecture (HPCA)} (pp. 789–801). \sphinxhref{https://doi.org/10.1109/HPCA51647.2021.00071}{doi:10.1109/HPCA51647.2021.00071}%
\begin{footnote}[38]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1109/HPCA51647.2021.00071}
%
\end{footnote}
\bibitem[NVIDIA, 2017]{chapter_accelerator/summary:nvidia}
\sphinxAtStartPar
NVIDIA (2017). \sphinxstyleemphasis{NVIDIA Tesla V100 GPU Architecture: The World’s Most Advanced Datacenter GPU}. http://www.nvidia.com/object/volta\sphinxhyphen{}architecture\sphinxhyphen{}whitepaper.html.
\bibitem[Ragan\sphinxhyphen{}Kelley et al., 2013]{chapter_accelerator/summary:ragan2013halide}
\sphinxAtStartPar
Ragan\sphinxhyphen{}Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F., \& Amarasinghe, S. (2013). Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. \sphinxstyleemphasis{Acm Sigplan Notices}, \sphinxstyleemphasis{48}(6), 519–530.
\bibitem[Raihan et al., 2018]{chapter_accelerator/summary:modeling}
\sphinxAtStartPar
Raihan, M. A., Goli, N., \& Aamodt, T. (2018). Modeling deep learning accelerator enabled gpus. \sphinxstyleemphasis{arXiv e\sphinxhyphen{}prints arXiv:1811.08309}.
\bibitem[Vasilache et al., 2022]{chapter_accelerator/summary:vasilache2022composable}
\sphinxAtStartPar
Vasilache, N., Zinenko, O., Bik, A. J., Ravishankar, M., Raoux, T., Belyaev, A., … others. (2022). Composable and modular code generation in mlir: a structured and retargetable approach to tensor compiler construction. \sphinxstyleemphasis{arXiv preprint arXiv:2202.03293}.
\bibitem[Verdoolaege, 2010]{chapter_accelerator/summary:verdoolaege2010isl}
\sphinxAtStartPar
Verdoolaege, S. (2010). Isl: an integer set library for the polyhedral model. \sphinxstyleemphasis{International Congress on Mathematical Software} (pp. 299–302).
\bibitem[Zhao et al., 2021]{chapter_accelerator/summary:zhao2021akg}
\sphinxAtStartPar
Zhao, J., Li, B., Nie, W., Geng, Z., Zhang, R., Gao, X., … others. (2021). Akg: automatic kernel generation for neural processing units using polyhedral transformations. \sphinxstyleemphasis{Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation} (pp. 1233–1248).
\bibitem[Zheng et al., 2020]{chapter_accelerator/summary:zheng2020ansor}
\sphinxAtStartPar
Zheng, L., Jia, C., Sun, M., Wu, Z., Yu, C. H., Haj\sphinxhyphen{}Ali, A., … others. (2020). Ansor: generating \$\textbackslash{}\$High\sphinxhyphen{}Performance\$\textbackslash{}\$ tensor programs for deep learning. \sphinxstyleemphasis{14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)} (pp. 863–879).
\bibitem[Cheng et al., 2016]{chapter_recommender_system/summary:id4}
\sphinxAtStartPar
Cheng, H.\sphinxhyphen{}T., Koc, L., Harmsen, J., Shaked, T., Chandra, T., Aradhye, H., … Shah, H. (2016). Wide \& deep learning for recommender systems. \sphinxstyleemphasis{Proceedings of the 1st Workshop on Deep Learning for Recommender Systems} (pp. 7–10). New York, NY, USA: Association for Computing Machinery. URL: \sphinxurl{https://doi.org/10.1145/2988450.2988454}, \sphinxhref{https://doi.org/10.1145/2988450.2988454}{doi:10.1145/2988450.2988454}%
\begin{footnote}[65]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1145/2988450.2988454}
%
\end{footnote}
\bibitem[Ginart et al., 2021]{chapter_recommender_system/summary:ginart2021mixed}
\sphinxAtStartPar
Ginart, A., Naumov, M., Mudigere, D., Yang, J., \& Zou, J. (2021). \sphinxstyleemphasis{Mixed Dimension Embeddings with Application to Memory\sphinxhyphen{}Efficient Recommendation Systems}.
\bibitem[Gong et al., 2020]{chapter_recommender_system/summary:gong2020edgerec}
\sphinxAtStartPar
Gong, Y., Jiang, Z., Feng, Y., Hu, B., Zhao, K., Liu, Q., \& Ou, W. (2020). Edgerec: recommender system on edge in mobile taobao. \sphinxstyleemphasis{Proceedings of the 29th ACM International Conference on Information \& Knowledge Management} (pp. 2477–2484).
\bibitem[Guo et al., 2017]{chapter_recommender_system/summary:ijcai2017-239}
\sphinxAtStartPar
Guo, H., TANG, R., Ye, Y., Li, Z., \& He, X. (2017). Deepfm: a factorization\sphinxhyphen{}machine based neural network for ctr prediction. \sphinxstyleemphasis{Proceedings of the Twenty\sphinxhyphen{}Sixth International Joint Conference on Artificial Intelligence, IJCAI\sphinxhyphen{}17} (pp. 1725–1731). URL: \sphinxurl{https://doi.org/10.24963/ijcai.2017/239}, \sphinxhref{https://doi.org/10.24963/ijcai.2017/239}{doi:10.24963/ijcai.2017/239}%
\begin{footnote}[66]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.24963/ijcai.2017/239}
%
\end{footnote}
\bibitem[He et al., 2020]{chapter_recommender_system/summary:neurips2020-a1d4c20b}
\sphinxAtStartPar
He, C., Annavaram, M., \& Avestimehr, S. (2020). Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., \& Lin, H. (Eds.). Group knowledge transfer: federated learning of large cnns at the edge. \sphinxstyleemphasis{Advances in Neural Information Processing Systems} (pp. 14068–14080). Curran Associates, Inc. URL: \sphinxurl{https://proceedings.neurips.cc/paper/2020/file/a1d4c20b182ad7137ab3606f0e3fc8a4-Paper.pdf}
\bibitem[Jiang et al., 2021]{chapter_recommender_system/summary:mlsys2021-ec895663}
\sphinxAtStartPar
Jiang, W., He, Z., Zhang, S., Preuß er, T. B., Zeng, K., Feng, L., … Alonso, G. (2021). Smola, A., Dimakis, A., \& Stoica, I. (Eds.). Microrec: efficient recommendation inference by hardware and data structure solutions. \sphinxstyleemphasis{Proceedings of Machine Learning and Systems} (pp. 845–859). URL: \sphinxurl{https://proceedings.mlsys.org/paper/2021/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf}
\bibitem[Naumov et al., 2019]{chapter_recommender_system/summary:naumov2019deep}
\sphinxAtStartPar
Naumov, M., Mudigere, D., Shi, H.\sphinxhyphen{}J. M., Huang, J., Sundaraman, N., Park, J., … others. (2019). Deep learning recommendation model for personalization and recommendation systems. \sphinxstyleemphasis{arXiv preprint arXiv:1906.00091}.
\bibitem[NVIDIA, 2022]{chapter_recommender_system/summary:merlin}
\sphinxAtStartPar
NVIDIA (2022). \sphinxstyleemphasis{NVIDIA Merlin}. Accessed on 2022\sphinxhyphen{}03\sphinxhyphen{}24.
\bibitem[Shi et al., 2020]{chapter_recommender_system/summary:id5}
\sphinxAtStartPar
Shi, H.\sphinxhyphen{}J. M., Mudigere, D., Naumov, M., \& Yang, J. (2020). Compositional embeddings using complementary partitions for memory\sphinxhyphen{}efficient recommendation systems. \sphinxstyleemphasis{Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining} (pp. 165–175). New York, NY, USA: Association for Computing Machinery. URL: \sphinxurl{https://doi.org/10.1145/3394486.3403059}, \sphinxhref{https://doi.org/10.1145/3394486.3403059}{doi:10.1145/3394486.3403059}%
\begin{footnote}[67]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1145/3394486.3403059}
%
\end{footnote}
\bibitem[Wang et al., 2017]{chapter_recommender_system/summary:id6}
\sphinxAtStartPar
Wang, R., Fu, B., Fu, G., \& Wang, M. (2017). Deep \& cross network for ad click predictions. \sphinxstyleemphasis{Proceedings of the ADKDD’17}. New York, NY, USA: Association for Computing Machinery. URL: \sphinxurl{https://doi.org/10.1145/3124749.3124754}, \sphinxhref{https://doi.org/10.1145/3124749.3124754}{doi:10.1145/3124749.3124754}%
\begin{footnote}[68]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1145/3124749.3124754}
%
\end{footnote}
\bibitem[Xie et al., 2020]{chapter_recommender_system/summary:id7}
\sphinxAtStartPar
Xie, M., Ren, K., Lu, Y., Yang, G., Xu, Q., Wu, B., … Shu, J. (2020). Kraken: memory\sphinxhyphen{}efficient continual learning for large\sphinxhyphen{}scale real\sphinxhyphen{}time recommendations. \sphinxstyleemphasis{SC20: International Conference for High Performance Computing, Networking, Storage and Analysis} (pp. 1–17). \sphinxhref{https://doi.org/10.1109/SC41405.2020.00025}{doi:10.1109/SC41405.2020.00025}%
\begin{footnote}[69]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1109/SC41405.2020.00025}
%
\end{footnote}
\bibitem[Yin et al., 2021]{chapter_recommender_system/summary:mlsys2021-979d472a}
\sphinxAtStartPar
Yin, C., Acun, B., Wu, C.\sphinxhyphen{}J., \& Liu, X. (2021). Smola, A., Dimakis, A., \& Stoica, I. (Eds.). Tt\sphinxhyphen{}rec: tensor train compression for deep learning recommendation models. \sphinxstyleemphasis{Proceedings of Machine Learning and Systems} (pp. 448–462). URL: \sphinxurl{https://proceedings.mlsys.org/paper/2021/file/979d472a84804b9f647bc185a877a8b5-Paper.pdf}
\bibitem[Zhao et al., 2020]{chapter_recommender_system/summary:mlsys2020-f7e6c855}
\sphinxAtStartPar
Zhao, W., Xie, D., Jia, R., Qian, Y., Ding, R., Sun, M., \& Li, P. (2020). Dhillon, I., Papailiopoulos, D., \& Sze, V. (Eds.). Distributed hierarchical gpu parameter server for massive scale deep learning ads systems. \sphinxstyleemphasis{Proceedings of Machine Learning and Systems} (pp. 412–428). URL: \sphinxurl{https://proceedings.mlsys.org/paper/2020/file/f7e6c85504ce6e82442c770f7c8606f0-Paper.pdf}
\bibitem[Berner et al., 2019]{chapter_reinforcement_learning/summary:berner2019dota}
\sphinxAtStartPar
Berner, C., Brockman, G., Chan, B., Cheung, V., Dębiak, P., Dennison, C., … others. (2019). Dota 2 with large scale deep reinforcement learning. \sphinxstyleemphasis{arXiv preprint arXiv:1912.06680}.
\bibitem[Cassirer et al., 2021]{chapter_reinforcement_learning/summary:cassirer2021reverb}
\sphinxAtStartPar
Cassirer, A., Barth\sphinxhyphen{}Maron, G., Brevdo, E., Ramos, S., Boyd, T., Sottiaux, T., \& Kroiss, M. (2021). Reverb: a framework for experience replay. \sphinxstyleemphasis{arXiv preprint arXiv:2102.04736}.
\bibitem[Chu et al., 2011]{chapter_reinforcement_learning/summary:id3}
\sphinxAtStartPar
Chu, W., Zinkevich, M., Li, L., Thomas, A., \& Tseng, B. (2011). Unbiased online active learning in data streams. \sphinxstyleemphasis{Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining} (pp. 195–203). New York, NY, USA: Association for Computing Machinery. URL: \sphinxurl{https://doi.org/10.1145/2020408.2020444}, \sphinxhref{https://doi.org/10.1145/2020408.2020444}{doi:10.1145/2020408.2020444}%
\begin{footnote}[80]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1145/2020408.2020444}
%
\end{footnote}
\bibitem[Ding et al., 2020]{chapter_reinforcement_learning/summary:ding2020efficient}
\sphinxAtStartPar
Ding, Z., Yu, T., Huang, Y., Zhang, H., Li, G., Guo, Q., … Dong, H. (2020). Efficient reinforcement learning development with rlzoo. \sphinxstyleemphasis{arXiv preprint arXiv:2009.08644}.
\bibitem[Espeholt et al., 2019]{chapter_reinforcement_learning/summary:espeholt2019seed}
\sphinxAtStartPar
Espeholt, L., Marinier, R., Stanczyk, P., Wang, K., \& Michalski, M. (2019). Seed rl: scalable and efficient deep\sphinxhyphen{}rl with accelerated central inference. \sphinxstyleemphasis{arXiv preprint arXiv:1910.06591}.
\bibitem[Espeholt et al., 2018]{chapter_reinforcement_learning/summary:espeholt2018impala}
\sphinxAtStartPar
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., … others. (2018). Impala: scalable distributed deep\sphinxhyphen{}rl with importance weighted actor\sphinxhyphen{}learner architectures. \sphinxstyleemphasis{arXiv preprint arXiv:1802.01561}.
\bibitem[Foerster et al., 2018]{chapter_reinforcement_learning/summary:foerster2018counterfactual}
\sphinxAtStartPar
Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., \& Whiteson, S. (2018). Counterfactual multi\sphinxhyphen{}agent policy gradients. \sphinxstyleemphasis{Proceedings of the AAAI conference on artificial intelligence}.
\bibitem[Ginart et al., 2021]{chapter_reinforcement_learning/summary:ginart2021mixed}
\sphinxAtStartPar
Ginart, A., Naumov, M., Mudigere, D., Yang, J., \& Zou, J. (2021). \sphinxstyleemphasis{Mixed Dimension Embeddings with Application to Memory\sphinxhyphen{}Efficient Recommendation Systems}.
\bibitem[Gong et al., 2020]{chapter_reinforcement_learning/summary:gong2020edgerec}
\sphinxAtStartPar
Gong, Y., Jiang, Z., Feng, Y., Hu, B., Zhao, K., Liu, Q., \& Ou, W. (2020). Edgerec: recommender system on edge in mobile taobao. \sphinxstyleemphasis{Proceedings of the 29th ACM International Conference on Information \& Knowledge Management} (pp. 2477–2484).
\bibitem[Han et al., 2020]{chapter_reinforcement_learning/summary:han2020tstarbot}
\sphinxAtStartPar
Han, L., Xiong, J., Sun, P., Sun, X., Fang, M., Guo, Q., … others. (2020). Tstarbot\sphinxhyphen{}x: an open\sphinxhyphen{}sourced and comprehensive study for efficient league training in starcraft ii full game. \sphinxstyleemphasis{arXiv preprint arXiv:2011.13729}.
\bibitem[He et al., 2020]{chapter_reinforcement_learning/summary:neurips2020-a1d4c20b}
\sphinxAtStartPar
He, C., Annavaram, M., \& Avestimehr, S. (2020). Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., \& Lin, H. (Eds.). Group knowledge transfer: federated learning of large cnns at the edge. \sphinxstyleemphasis{Advances in Neural Information Processing Systems} (pp. 14068–14080). Curran Associates, Inc. URL: \sphinxurl{https://proceedings.neurips.cc/paper/2020/file/a1d4c20b182ad7137ab3606f0e3fc8a4-Paper.pdf}
\bibitem[He et al., 2014]{chapter_reinforcement_learning/summary:id4}
\sphinxAtStartPar
He, X., Pan, J., Jin, O., Xu, T., Liu, B., Xu, T., … Candela, J. Q. (2014). Practical lessons from predicting clicks on ads at facebook. \sphinxstyleemphasis{Proceedings of the Eighth International Workshop on Data Mining for Online Advertising} (pp. 1–9). New York, NY, USA: Association for Computing Machinery. URL: \sphinxurl{https://doi.org/10.1145/2648584.2648589}, \sphinxhref{https://doi.org/10.1145/2648584.2648589}{doi:10.1145/2648584.2648589}%
\begin{footnote}[81]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1145/2648584.2648589}
%
\end{footnote}
\bibitem[Hoffman et al., 2020]{chapter_reinforcement_learning/summary:hoffman2020acme}
\sphinxAtStartPar
Hoffman, M., Shahriari, B., Aslanides, J., Barth\sphinxhyphen{}Maron, G., Behbahani, F., Norman, T., … others. (2020). Acme: a research framework for distributed reinforcement learning. \sphinxstyleemphasis{arXiv preprint arXiv:2006.00979}.
\bibitem[Horgan et al., 2018]{chapter_reinforcement_learning/summary:horgan2018distributed}
\sphinxAtStartPar
Horgan, D., Quan, J., Budden, D., Barth\sphinxhyphen{}Maron, G., Hessel, M., van Hasselt, H., \& Silver, D. (2018). \sphinxstyleemphasis{Distributed Prioritized Experience Replay}.
\bibitem[Jiang et al., 2021]{chapter_reinforcement_learning/summary:mlsys2021-ec895663}
\sphinxAtStartPar
Jiang, W., He, Z., Zhang, S., Preuß er, T. B., Zeng, K., Feng, L., … Alonso, G. (2021). Smola, A., Dimakis, A., \& Stoica, I. (Eds.). Microrec: efficient recommendation inference by hardware and data structure solutions. \sphinxstyleemphasis{Proceedings of Machine Learning and Systems} (pp. 845–859). URL: \sphinxurl{https://proceedings.mlsys.org/paper/2021/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf}
\bibitem[Lanctot et al., 2017]{chapter_reinforcement_learning/summary:lanctot2017unified}
\sphinxAtStartPar
Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., Pérolat, J., … Graepel, T. (2017). A unified game\sphinxhyphen{}theoretic approach to multiagent reinforcement learning. \sphinxstyleemphasis{Advances in neural information processing systems}, \sphinxstyleemphasis{30}.
\bibitem[Liang et al., 2017]{chapter_reinforcement_learning/summary:liang2017ray}
\sphinxAtStartPar
Liang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Gonzalez, J., … Stoica, I. (2017). Ray rllib: a composable and scalable reinforcement learning library. \sphinxstyleemphasis{arXiv preprint arXiv:1712.09381}, p. 85.
\bibitem[Makoviychuk et al., 2021]{chapter_reinforcement_learning/summary:makoviychuk2021isaac}
\sphinxAtStartPar
Makoviychuk, V., Wawrzyniak, L., Guo, Y., Lu, M., Storey, K., Macklin, M., … others. (2021). Isaac gym: high performance gpu\sphinxhyphen{}based physics simulation for robot learning. \sphinxstyleemphasis{arXiv preprint arXiv:2108.10470}.
\bibitem[Mnih et al., 2016]{chapter_reinforcement_learning/summary:mnih2016asynchronous}
\sphinxAtStartPar
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., … Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. \sphinxstyleemphasis{International Conference on Machine Learning (ICML)} (pp. 1928–1937).
\bibitem[Mnih et al., 2013]{chapter_reinforcement_learning/summary:mnih2013playing}
\sphinxAtStartPar
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., \& Riedmiller, M. (2013). Playing atari with deep reinforcement learning. \sphinxstyleemphasis{arXiv preprint arXiv:1312.5602}.
\bibitem[Moritz et al., 2018]{chapter_reinforcement_learning/summary:moritz2018ray}
\sphinxAtStartPar
Moritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R., Liang, E., … others. (2018). Ray: a distributed framework for emerging \$\textbackslash{}\$AI\$\textbackslash{}\$ applications. \sphinxstyleemphasis{13th \$\textbackslash{}\$USENIX\$\textbackslash{}\$ Symposium on Operating Systems Design and Implementation (\$\textbackslash{}\$OSDI\$\textbackslash{}\$ 18)} (pp. 561–577).
\bibitem[Mudigere et al., 2021]{chapter_reinforcement_learning/summary:zionex}
\sphinxAtStartPar
Mudigere, D., Hao, Y., Huang, J., Jia, Z., Tulloch, A., Sridharan, S., … others. (2021). Software\sphinxhyphen{}hardware co\sphinxhyphen{}design for fast and scalable training of deep learning recommendation models. \sphinxstyleemphasis{arXiv preprint arXiv:2104.05158}.
\bibitem[NVIDIA, 2017]{chapter_reinforcement_learning/summary:nvidia}
\sphinxAtStartPar
NVIDIA (2017). \sphinxstyleemphasis{NVIDIA Tesla V100 GPU Architecture: The World’s Most Advanced Datacenter GPU}. http://www.nvidia.com/object/volta\sphinxhyphen{}architecture\sphinxhyphen{}whitepaper.html.
\bibitem[Rashid et al., 2018]{chapter_reinforcement_learning/summary:rashid2018qmix}
\sphinxAtStartPar
Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., \& Whiteson, S. (2018). Qmix: monotonic value function factorisation for deep multi\sphinxhyphen{}agent reinforcement learning. \sphinxstyleemphasis{International Conference on Machine Learning} (pp. 4295–4304).
\bibitem[Shi et al., 2020]{chapter_reinforcement_learning/summary:id5}
\sphinxAtStartPar
Shi, H.\sphinxhyphen{}J. M., Mudigere, D., Naumov, M., \& Yang, J. (2020). Compositional embeddings using complementary partitions for memory\sphinxhyphen{}efficient recommendation systems. \sphinxstyleemphasis{Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining} (pp. 165–175). New York, NY, USA: Association for Computing Machinery. URL: \sphinxurl{https://doi.org/10.1145/3394486.3403059}, \sphinxhref{https://doi.org/10.1145/3394486.3403059}{doi:10.1145/3394486.3403059}%
\begin{footnote}[82]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1145/3394486.3403059}
%
\end{footnote}
\bibitem[Sunehag et al., 2017]{chapter_reinforcement_learning/summary:sunehag2017value}
\sphinxAtStartPar
Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V., Jaderberg, M., … others. (2017). Value\sphinxhyphen{}decomposition networks for cooperative multi\sphinxhyphen{}agent learning. \sphinxstyleemphasis{arXiv preprint arXiv:1706.05296}.
\bibitem[Tian et al., 2018]{chapter_reinforcement_learning/summary:id6}
\sphinxAtStartPar
Tian, H., Yu, M., \& Wang, W. (2018). Continuum: a platform for cost\sphinxhyphen{}aware, low\sphinxhyphen{}latency continual learning. \sphinxstyleemphasis{Proceedings of the ACM Symposium on Cloud Computing} (pp. 26–40). New York, NY, USA: Association for Computing Machinery. URL: \sphinxurl{https://doi.org/10.1145/3267809.3267817}, \sphinxhref{https://doi.org/10.1145/3267809.3267817}{doi:10.1145/3267809.3267817}%
\begin{footnote}[83]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1145/3267809.3267817}
%
\end{footnote}
\bibitem[Vinyals et al., 2019]{chapter_reinforcement_learning/summary:vinyals2019grandmaster}
\sphinxAtStartPar
Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., … others. (2019). Grandmaster level in starcraft ii using multi\sphinxhyphen{}agent reinforcement learning. \sphinxstyleemphasis{Nature}, \sphinxstyleemphasis{575}(7782), 350–354.
\bibitem[Wang et al., 2021]{chapter_reinforcement_learning/summary:wang2021scc}
\sphinxAtStartPar
Wang, X., Song, J., Qi, P., Peng, P., Tang, Z., Zhang, W., … others. (2021). Scc: an efficient deep reinforcement learning agent mastering the game of starcraft ii. \sphinxstyleemphasis{International Conference on Machine Learning} (pp. 10905–10915).
\bibitem[Xie et al., 2020]{chapter_reinforcement_learning/summary:id7}
\sphinxAtStartPar
Xie, M., Ren, K., Lu, Y., Yang, G., Xu, Q., Wu, B., … Shu, J. (2020). Kraken: memory\sphinxhyphen{}efficient continual learning for large\sphinxhyphen{}scale real\sphinxhyphen{}time recommendations. \sphinxstyleemphasis{SC20: International Conference for High Performance Computing, Networking, Storage and Analysis} (pp. 1–17). \sphinxhref{https://doi.org/10.1109/SC41405.2020.00025}{doi:10.1109/SC41405.2020.00025}%
\begin{footnote}[84]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1109/SC41405.2020.00025}
%
\end{footnote}
\bibitem[Yin et al., 2021]{chapter_reinforcement_learning/summary:mlsys2021-979d472a}
\sphinxAtStartPar
Yin, C., Acun, B., Wu, C.\sphinxhyphen{}J., \& Liu, X. (2021). Smola, A., Dimakis, A., \& Stoica, I. (Eds.). Tt\sphinxhyphen{}rec: tensor train compression for deep learning recommendation models. \sphinxstyleemphasis{Proceedings of Machine Learning and Systems} (pp. 448–462). URL: \sphinxurl{https://proceedings.mlsys.org/paper/2021/file/979d472a84804b9f647bc185a877a8b5-Paper.pdf}
\bibitem[Zhao et al., 2020]{chapter_reinforcement_learning/summary:mlsys2020-f7e6c855}
\sphinxAtStartPar
Zhao, W., Xie, D., Jia, R., Qian, Y., Ding, R., Sun, M., \& Li, P. (2020). Dhillon, I., Papailiopoulos, D., \& Sze, V. (Eds.). Distributed hierarchical gpu parameter server for massive scale deep learning ads systems. \sphinxstyleemphasis{Proceedings of Machine Learning and Systems} (pp. 412–428). URL: \sphinxurl{https://proceedings.mlsys.org/paper/2020/file/f7e6c85504ce6e82442c770f7c8606f0-Paper.pdf}
\bibitem[Chen et al., 2019]{chapter_explainable_AI/explainable_ai:chen2019looks}
\sphinxAtStartPar
Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., \& Su, J. K. (2019). This looks like that: deep learning for interpretable image recognition. \sphinxstyleemphasis{Advances in neural information processing systems}, \sphinxstyleemphasis{32}.
\bibitem[Erhan et al., 2009]{chapter_explainable_AI/explainable_ai:erhan2009visualizing}
\sphinxAtStartPar
Erhan, D., Bengio, Y., Courville, A., \& Vincent, P. (2009 , 01). Visualizing higher\sphinxhyphen{}layer features of a deep network. \sphinxstyleemphasis{Technical Report, Univeristé de Montréal}.
\bibitem[Frosst \& Hinton, 2017]{chapter_explainable_AI/explainable_ai:frosst2017distilling}
\sphinxAtStartPar
Frosst, N., \& Hinton, G. (2017). Distilling a neural network into a soft decision tree. \sphinxstyleemphasis{arXiv preprint arXiv:1711.09784}.
\bibitem[Li et al., 2020]{chapter_explainable_AI/explainable_ai:tkde-li}
\sphinxAtStartPar
Li, X.\sphinxhyphen{}H., Cao, C. C., Shi, Y., Bai, W., Gao, H., Qiu, L., … Chen, L. (2020). A survey of data\sphinxhyphen{}driven and knowledge\sphinxhyphen{}aware explainable ai. \sphinxstyleemphasis{IEEE Transactions on Knowledge and Data Engineering}, (), 1\sphinxhyphen{}1. \sphinxhref{https://doi.org/10.1109/TKDE.2020.2983930}{doi:10.1109/TKDE.2020.2983930}%
\begin{footnote}[88]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1109/TKDE.2020.2983930}
%
\end{footnote}
\bibitem[Nauta et al., 2021]{chapter_explainable_AI/explainable_ai:nauta2021neural}
\sphinxAtStartPar
Nauta, M., van Bree, R., \& Seifert, C. (2021). Neural prototype trees for interpretable fine\sphinxhyphen{}grained image recognition. \sphinxstyleemphasis{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition} (pp. 14933–14943).
\bibitem[Petsiuk et al., 2018]{chapter_explainable_AI/explainable_ai:petsiuk2018rise}
\sphinxAtStartPar
Petsiuk, V., Das, A., \& Saenko, K. (2018). Rise: randomized input sampling for explanation of black\sphinxhyphen{}box models. \sphinxstyleemphasis{arXiv preprint arXiv:1806.07421}.
\bibitem[Riedl, 2019]{chapter_explainable_AI/explainable_ai:riedl2019human}
\sphinxAtStartPar
Riedl, M. O. (2019). Human\sphinxhyphen{}centered artificial intelligence and machine learning. \sphinxstyleemphasis{Human Behavior and Emerging Technologies}, \sphinxstyleemphasis{1}(1), 33–36.
\bibitem[Selvaraju et al., 2017]{chapter_explainable_AI/explainable_ai:selvaraju2017grad}
\sphinxAtStartPar
Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., \& Batra, D. (2017). Grad\sphinxhyphen{}cam: visual explanations from deep networks via gradient\sphinxhyphen{}based localization. \sphinxstyleemphasis{Proceedings of the IEEE international conference on computer vision} (pp. 618–626).
\bibitem[Wang et al., 2020]{chapter_explainable_AI/explainable_ai:wang2020score}
\sphinxAtStartPar
Wang, H., Wang, Z., Du, M., Yang, F., Zhang, Z., Ding, S., … Hu, X. (2020). Score\sphinxhyphen{}cam: score\sphinxhyphen{}weighted visual explanations for convolutional neural networks. \sphinxstyleemphasis{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops} (pp. 24–25).
\bibitem[Zeiler \& Fergus, 2014]{chapter_explainable_AI/explainable_ai:zeiler2014visualizing}
\sphinxAtStartPar
Zeiler, M. D., \& Fergus, R. (2014). Visualizing and understanding convolutional networks. \sphinxstyleemphasis{European conference on computer vision} (pp. 818–833).
\bibitem[Zhang et al., 2018]{chapter_explainable_AI/explainable_ai:zhang2018interpretable}
\sphinxAtStartPar
Zhang, Q., Wu, Y. N., \& Zhu, S.\sphinxhyphen{}C. (2018). Interpretable convolutional neural networks. \sphinxstyleemphasis{Proceedings of the IEEE conference on computer vision and pattern recognition} (pp. 8827–8836).
\bibitem[Zhang et al., 2019]{chapter_explainable_AI/explainable_ai:zhang2019interpreting}
\sphinxAtStartPar
Zhang, Q., Yang, Y., Ma, H., \& Wu, Y. N. (2019). Interpreting cnns via decision trees. \sphinxstyleemphasis{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition} (pp. 6261–6270).
\bibitem[Aradi, 2020]{chapter_rl_sys/summary:aradi2020survey}
\sphinxAtStartPar
Aradi, S. (2020). Survey of deep reinforcement learning for motion planning of autonomous vehicles. \sphinxstyleemphasis{IEEE Transactions on Intelligent Transportation Systems}.
\bibitem[Brunke et al., 2021]{chapter_rl_sys/summary:brunke2021safe}
\sphinxAtStartPar
Brunke, L., Greeff, M., Hall, A. W., Yuan, Z., Zhou, S., Panerati, J., \& Schoellig, A. P. (2021). Safe learning in robotics: from learning\sphinxhyphen{}based control to safe reinforcement learning. \sphinxstyleemphasis{Annual Review of Control, Robotics, and Autonomous Systems}, \sphinxstyleemphasis{5}.
\bibitem[Campos et al., 2021]{chapter_rl_sys/summary:campos2021orb}
\sphinxAtStartPar
Campos, C., Elvira, R., Rodríguez, J. J. G., Montiel, J. M., \& Tardós, J. D. (2021). Orb\sphinxhyphen{}slam3: an accurate open\sphinxhyphen{}source library for visual, visual–inertial, and multimap slam. \sphinxstyleemphasis{IEEE Transactions on Robotics}, \sphinxstyleemphasis{37}(6), 1874–1890.
\bibitem[Ding et al., 2019]{chapter_rl_sys/summary:ding2019camnet}
\sphinxAtStartPar
Ding, M., Wang, Z., Sun, J., Shi, J., \& Luo, P. (2019). Camnet: coarse\sphinxhyphen{}to\sphinxhyphen{}fine retrieval for camera re\sphinxhyphen{}localization. \sphinxstyleemphasis{Proceedings of the IEEE/CVF International Conference on Computer Vision} (pp. 2871–2880).
\bibitem[Huang et al., 2018]{chapter_rl_sys/summary:huang2018navigationnet}
\sphinxAtStartPar
Huang, H., Shen, Y., Sun, J., \& Lu, C. (2018). Navigationnet: a large\sphinxhyphen{}scale interactive indoor navigation dataset. \sphinxstyleemphasis{arXiv preprint arXiv:1808.08374}.
\bibitem[Huang et al., 2021]{chapter_rl_sys/summary:pmlr-v155-huang21a}
\sphinxAtStartPar
Huang, J., Xie, S., Sun, J., Ma, Q., Liu, C., Lin, D., \& Zhou, B. (2021 , 16–18 Nov). Kober, J., Ramos, F., \& Tomlin, C. (Eds.). Learning a decision module by imitating driver’s control behaviors. \sphinxstyleemphasis{Proceedings of the 2020 Conference on Robot Learning} (pp. 1–10). PMLR. URL: \sphinxurl{https://proceedings.mlr.press/v155/huang21a.html}
\bibitem[Li et al., 2021a]{chapter_rl_sys/summary:li2021metadrive}
\sphinxAtStartPar
Li, Q., Peng, Z., Xue, Z., Zhang, Q., \& Zhou, B. (2021). Metadrive: composing diverse driving scenarios for generalizable reinforcement learning. \sphinxstyleemphasis{ArXiv preprint}, \sphinxstyleemphasis{abs/2109.12674}. URL: \sphinxurl{https://arxiv.org/abs/2109.12674}
\bibitem[Li et al., 2021b]{chapter_rl_sys/summary:li2021efficient}
\sphinxAtStartPar
Li, Q., Peng, Z., \& Zhou, B. (2021). Efficient learning of safe driving policy via human\sphinxhyphen{}ai copilot optimization. \sphinxstyleemphasis{International Conference on Learning Representations}.
\bibitem[Lu \& Shi, 2021]{chapter_rl_sys/summary:id3}
\sphinxAtStartPar
Lu, S., \& Shi, W. (2021). The emergence of vehicle computing. \sphinxstyleemphasis{IEEE Internet Computing}, \sphinxstyleemphasis{25}(3), 18\sphinxhyphen{}22. \sphinxhref{https://doi.org/10.1109/MIC.2021.3066076}{doi:10.1109/MIC.2021.3066076}%
\begin{footnote}[103]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1109/MIC.2021.3066076}
%
\end{footnote}
\bibitem[Maruyama et al., 2016]{chapter_rl_sys/summary:maruyama2016exploring}
\sphinxAtStartPar
Maruyama, Y., Kato, S., \& Azumi, T. (2016). Exploring the performance of ros2. \sphinxstyleemphasis{Proceedings of the 13th ACM SIGBED International Conference on Embedded Software (EMSOFT)} (pp. 1–10).
\bibitem[Pan et al., 2020]{chapter_rl_sys/summary:id4}
\sphinxAtStartPar
Pan, B., Sun, J., Leung, H. Y. T., Andonian, A., \& Zhou, B. (2020). Cross\sphinxhyphen{}view semantic segmentation for sensing surroundings. \sphinxstyleemphasis{IEEE Robotics and Automation Letters}, \sphinxstyleemphasis{5}(3), 4867\sphinxhyphen{}4873. \sphinxhref{https://doi.org/10.1109/LRA.2020.3004325}{doi:10.1109/LRA.2020.3004325}%
\begin{footnote}[104]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1109/LRA.2020.3004325}
%
\end{footnote}
\bibitem[Peng et al., 2021a]{chapter_rl_sys/summary:peng2021learning}
\sphinxAtStartPar
Peng, Z., Li, Q., Hui, K. M., Liu, C., \& Zhou, B. (2021). Learning to simulate self\sphinxhyphen{}driven particles system with coordinated policy optimization. \sphinxstyleemphasis{Advances in Neural Information Processing Systems}, \sphinxstyleemphasis{34}.
\bibitem[Peng et al., 2021b]{chapter_rl_sys/summary:peng2021safe}
\sphinxAtStartPar
Peng, Z., Li, Q., Liu, C., \& Zhou, B. (2021). Safe driving via expert guided policy optimization. \sphinxstyleemphasis{5th Annual Conference on Robot Learning}.
\bibitem[Sun et al., 2022a]{chapter_rl_sys/summary:id5}
\sphinxAtStartPar
Sun, J., Huang, D.\sphinxhyphen{}A., Lu, B., Liu, Y.\sphinxhyphen{}H., Zhou, B., \& Garg, A. (2022). Plate: visually\sphinxhyphen{}grounded planning with transformers in procedural tasks. \sphinxstyleemphasis{IEEE Robotics and Automation Letters}, \sphinxstyleemphasis{7}(2), 4924\sphinxhyphen{}4930. \sphinxhref{https://doi.org/10.1109/LRA.2022.3150855}{doi:10.1109/LRA.2022.3150855}%
\begin{footnote}[105]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1109/LRA.2022.3150855}
%
\end{footnote}
\bibitem[Sun et al., 2022b]{chapter_rl_sys/summary:sun2022selfsupervisedta}
\sphinxAtStartPar
Sun, J., Kousik, S., Fridovich\sphinxhyphen{}Keil, D., \& Schwager, M. (2022). Self\sphinxhyphen{}supervised traffic advisors: distributed, multi\sphinxhyphen{}view traffic prediction for smart cities. \sphinxstyleemphasis{arXiv preprint}.
\bibitem[Sun et al., 2021a]{chapter_rl_sys/summary:pmlr-v155-sun21a}
\sphinxAtStartPar
Sun, J., Sun, H., Han, T., \& Zhou, B. (2021 , 16–18 Nov). Kober, J., Ramos, F., \& Tomlin, C. (Eds.). Neuro\sphinxhyphen{}symbolic program search for autonomous driving decision module design. \sphinxstyleemphasis{Proceedings of the 2020 Conference on Robot Learning} (pp. 21–30). PMLR. URL: \sphinxurl{https://proceedings.mlr.press/v155/sun21a.html}
\bibitem[Sun et al., 2021b]{chapter_rl_sys/summary:sun2021adversarial}
\sphinxAtStartPar
Sun, J., Yu, L., Dong, P., Lu, B., \& Zhou, B. (2021). Adversarial inverse reinforcement learning with self\sphinxhyphen{}attention dynamics model. \sphinxstyleemphasis{IEEE Robotics and Automation Letters}, \sphinxstyleemphasis{6}(2), 1880–1886.
\bibitem[Xu et al., 2019]{chapter_rl_sys/summary:xu2019depth}
\sphinxAtStartPar
Xu, Y., Zhu, X., Shi, J., Zhang, G., Bao, H., \& Li, H. (2019). Depth completion from sparse lidar data with depth\sphinxhyphen{}normal constraints. \sphinxstyleemphasis{Proceedings of the IEEE/CVF International Conference on Computer Vision} (pp. 2811–2820).
\bibitem[Yi et al., 2020]{chapter_rl_sys/summary:yi2020segvoxelnet}
\sphinxAtStartPar
Yi, H., Shi, S., Ding, M., Sun, J., Xu, K., Zhou, H., … Wang, G. (2020). Segvoxelnet: exploring semantic context and depth\sphinxhyphen{}aware features for 3d vehicle detection from point cloud. \sphinxstyleemphasis{2020 IEEE International Conference on Robotics and Automation (ICRA)} (pp. 2274–2280).
\end{sphinxthebibliography}



\renewcommand{\indexname}{Index}
\printindex
\end{document}